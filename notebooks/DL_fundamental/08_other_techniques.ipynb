{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b27b909",
   "metadata": {},
   "source": [
    "# 08: 正則化と実践的テクニック (Regularization & Advanced Techniques)\n",
    "\n",
    "このノートブックでは、ニューラルネットワークの汎化性能を高め、学習を安定させるための2つの重要なテクニック、「Dropout」と「Gradient Clipping」について学びます。\n",
    "これまでのノートブックで学んだ技術（活性化関数、初期化、オプティマイザ、正規化層）は、主にモデルが訓練データにうまく適合し、学習を高速化・安定化させることを目的としていました。\n",
    "\n",
    "今回は、モデルが訓練データに過度に適合してしまう「過学習（Overfitting）」を防ぎ、未知のデータに対しても高い性能を発揮する（汎化する）ための代表的な手法である**Dropout**と、RNNの学習などで発生しやすい「勾配爆発」を防ぐための**Gradient Clipping**を扱います。\n",
    "\n",
    "**参考論文:**\n",
    "*   (Dropout) Srivastava, N., Hinton, G., et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\n",
    "*   (Gradient Clipping) Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks.\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  過学習（Overfitting）の概念とその問題点。\n",
    "2.  Dropoutのアルゴリズム：学習時にランダムにニューロンを非活性化する。\n",
    "3.  学習時と推論時でDropoutの振る舞いが異なる理由と、その調整（Inverted Dropout）。\n",
    "4.  Dropoutがなぜ正則化として機能するのか（アンサンブル学習の観点）。\n",
    "5.  勾配爆発（Gradient Exploding）の問題と、それを防ぐGradient Clippingの仕組み。\n",
    "\n",
    "**前提知識:**\n",
    "*   ニューラルネットワークの基本的な学習プロセス。\n",
    "*   過学習と汎化の概念。\n",
    "*   勾配降下法と勾配の役割。\n",
    "*   NumPyとMatplotlibの基本的な使い方。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212411ac",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119ef336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c789d",
   "metadata": {},
   "source": [
    "## 2. 過学習 (Overfitting) とは？\n",
    "\n",
    "過学習とは、ニューラルネットワークが**訓練データに過剰に適合してしまい、そのデータの特定のパターンやノイズまで学習してしまった結果、未知の新しいデータ（テストデータ）に対してうまく性能を発揮できなくなる現象**を指します。\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/600px-Overfitting.svg.png\" width=\"400\"></center>\n",
    "<center><small>出典: Wikimedia Commons</small></center>\n",
    "\n",
    "上の図のように、緑の線（過学習モデル）は訓練データ（青い点）を完璧に通っていますが、真の関数（黒い線）からは大きく外れてしまっています。これでは、新しいデータポイントが来たときに大きな誤差を生んでしまいます。\n",
    "\n",
    "過学習は、モデルの表現力（パラメータ数）がデータの複雑さに対して高すぎる場合に特に起こりやすくなります。モデルの複雑さを抑え、汎化性能を高めるためのテクニックを総称して**正則化**と呼びます。Dropoutはその代表的な手法の一つです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56916a",
   "metadata": {},
   "source": [
    "## 3. Dropout\n",
    "\n",
    "Dropoutは、2014年に提案された非常にシンプルかつ強力な正則化手法です。そのアイデアは、「**学習時に、各ニューロンを一定の確率 $p$ でランダムに非活性化（出力を0に）する**」というものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64685eb6",
   "metadata": {},
   "source": [
    "### 3.1 Dropoutのアルゴリズム\n",
    "\n",
    "**学習時:**\n",
    "1.  順伝播の際、ある層の各ニューロンに対して、確率 $p$ で「ドロップ（非活性化）」するかどうかを決定します。\n",
    "2.  ドロップすると選ばれたニューロンの出力は0になります。\n",
    "3.  ドロップされなかったニューロンの出力は、そのまま次の層に伝播します。\n",
    "\n",
    "**推論時:**\n",
    "1.  推論時には、ニューロンをドロップアウトしません。**全てのニューロンを使用します。**\n",
    "2.  しかし、学習時にはニューロンの一部しか使っていなかったため、そのままだと出力のスケールが学習時と異なってしまいます。\n",
    "3.  このスケールを合わせるために、各ニューロンの出力を**学習時にドロップアウトされなかった確率 $(1-p)$ でスケールダウン**(**乗算**)します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c5e0e",
   "metadata": {},
   "source": [
    "### 3.2 Inverted Dropout（近年の標準的な実装）\n",
    "\n",
    "推論時のスケール調整は、毎回計算が必要で少し面倒です。そこで、**Inverted Dropout**という実装が現在では標準となっています。\n",
    "この手法では、スケール調整を**学習時**に行います。\n",
    "\n",
    "**Inverted Dropoutのアルゴリズム:**\n",
    "*   **学習時**:\n",
    "    1.  ニューロンを確率 $p$ でドロップアウトします。\n",
    "    2.  ドロップされなかったニューロンの出力を、確率 $(1-p)$ で**割る**ことでスケールアップします。\n",
    "*   **推論時**:\n",
    "    1.  何もしません。全てのニューロンをそのまま使用します。\n",
    "\n",
    "学習時にあらかじめスケールを調整しておくことで、推論時の処理が不要になり、実装がシンプルになります。\n",
    "\n",
    "**数式 (Inverted Dropout):**\n",
    "層の出力を $a$、ドロップアウト率を $p$ とすると、\n",
    "$$\n",
    "\\text{Dropout}(a) =\n",
    "\\begin{cases}\n",
    "\\frac{a}{1-p} & \\text{with probability } 1-p \\\\\n",
    "0 & \\text{with probability } p\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e84f5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "After Dropout (training=True, p=0.5):\n",
      " [2. 0. 2. 2. 0. 2. 0. 0. 2. 0.]\n",
      "-> ニューロンがランダムに0になり、生存したニューロンの出力が 1/(1-p)=2.0 にスケールされている\n",
      "\n",
      "After Dropout (training=False):\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "-> 何も変化しない\n"
     ]
    }
   ],
   "source": [
    "def dropout(x, dropout_ratio=0.5, training=True):\n",
    "    \"\"\"\n",
    "    Inverted Dropoutの実装\n",
    "    \"\"\"\n",
    "    if not training:\n",
    "        return x\n",
    "\n",
    "    # ドロップアウトするニューロンを決めるマスクを生成\n",
    "    # dropout_ratioより大きい値はTrue(生存)、小さい値はFalse(ドロップ)\n",
    "    mask = np.random.rand(*x.shape) > dropout_ratio\n",
    "    \n",
    "    # マスクを適用し、スケールを調整\n",
    "    return x * mask / (1.0 - dropout_ratio)\n",
    "\n",
    "# 簡単な実験\n",
    "x = np.ones(10)\n",
    "dropout_ratio = 0.5\n",
    "\n",
    "print(\"Original data:\", x)\n",
    "\n",
    "# 学習時\n",
    "y_train = dropout(x, dropout_ratio, training=True)\n",
    "print(f\"\\nAfter Dropout (training=True, p={dropout_ratio}):\\n\", y_train)\n",
    "print(\"-> ニューロンがランダムに0になり、生存したニューロンの出力が 1/(1-p)=2.0 にスケールされている\")\n",
    "\n",
    "# 推論時\n",
    "y_test = dropout(x, dropout_ratio, training=False)\n",
    "print(\"\\nAfter Dropout (training=False):\\n\", y_test)\n",
    "print(\"-> 何も変化しない\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc5eff",
   "metadata": {},
   "source": [
    "### 3.3 Dropoutはなぜ機能するのか？\n",
    "\n",
    "Dropoutが強力な正則化手法として機能する理由は、主に2つの観点から説明されます。\n",
    "\n",
    "1.  **アンサンブル学習の効果**:\n",
    "    Dropoutは、学習の各イテレーションで異なるニューロンの組み合わせを持つ「痩せた（thinned）」ネットワークを学習していると見なせます。これは、膨大な数の異なるネットワークアーキテクチャを訓練し、推論時にはそれらの予測を平均化（アンサンブル）するのと似た効果をもたらします。アンサンブル学習は、単一のモデルよりも汎化性能が高いことが知られています。\n",
    "\n",
    "2.  **共適応の抑制**:\n",
    "    ニューロンは、他の特定のニューロンの存在を前提として学習を進める「共適応（co-adaptation）」を起こすことがあります。これは、あるニューロンのミスを他のニューロンが補うような、過度に複雑な協調関係を生み出し、過学習の原因となります。\n",
    "    Dropoutは、どのニューロンがドロップされるか分からない状況を作り出すことで、各ニューロンが他の特定のニューロンに依存せず、単独でも頑健な特徴を学習するように促します。これにより、共適応が抑制され、汎化性能が向上します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142deed8",
   "metadata": {},
   "source": [
    "## 4. 勾配クリッピング (Gradient Clipping)\n",
    "\n",
    "勾配クリッピングは、正則化とは少し目的が異なりますが、学習プロセスを安定させるための非常に重要な実践的テクニックです。\n",
    "特にRNN（再帰型ニューラルネットワーク）の学習では、BPTTの過程で勾配が指数関数的に増加する**勾配爆発** (**Gradient Exploding**)という問題が発生しやすく、学習が発散してしまうことがあります。\n",
    "\n",
    "勾配クリッピングは、この勾配爆発を防ぐためのシンプルな手法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e04ea3",
   "metadata": {},
   "source": [
    "### 4.1 勾配クリッピングのアルゴリズム\n",
    "\n",
    "勾配クリッピングは、逆伝播によって計算された勾配の大きさが、あらかじめ定めた閾値を超えた場合に、勾配のベクトルを縮小して大きさをしきい値に抑える処理です。\n",
    "\n",
    "**数式:**\n",
    "パラメータ全体の勾配を連結したベクトルを $\\boldsymbol{g}$ とします。\n",
    "まず、勾配のL2ノルム（大きさ） $||\\boldsymbol{g}||_2$ を計算します。\n",
    "$$\n",
    "||\\boldsymbol{g}||_2 = \\sqrt{\\sum_{i} g_i^2}\n",
    "$$\n",
    "もし $||\\boldsymbol{g}||_2$ が閾値 `max_norm` を超えていたら、勾配を以下のように更新します。\n",
    "$$\n",
    "\\text{if } \\|\\boldsymbol{g}\\|_2 > \\text{max\\_norm}:\\quad \\boldsymbol{g} \\leftarrow \\frac{\\text{max\\_norm}}{\\|\\boldsymbol{g}\\|_2} \\boldsymbol{g}\n",
    "$$\n",
    "この処理により、勾配ベクトルの**方向は変えずに、その大きさ（ノルム）だけを`max_norm`に制限**することができます。これにより、稀に発生する巨大な勾配によるパラメータの過剰な更新を防ぎ、学習プロセスを安定させることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3597d206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm = 14.28 > max_norm = 10.0. Clipping applied.\n",
      "Norm after clipping: 10.00\n",
      "\n",
      "--- No clipping case ---\n",
      "Gradient norm = 14.28 <= max_norm = 20.0. No clipping.\n"
     ]
    }
   ],
   "source": [
    "def gradient_clipping(grads, max_norm):\n",
    "    \"\"\"\n",
    "    勾配クリッピングの実装\n",
    "    \"\"\"\n",
    "    # 全ての勾配をフラットにして連結\n",
    "    all_grads = np.concatenate([g.flatten() for g in grads])\n",
    "    \n",
    "    # L2ノルムを計算\n",
    "    norm = np.linalg.norm(all_grads)\n",
    "    \n",
    "    # ノルムがしきい値を超えていれば、クリッピングを適用\n",
    "    rate = max_norm / (norm + 1e-6) # ゼロ除算防止\n",
    "    \n",
    "    if rate < 1:\n",
    "        print(f\"Gradient norm = {norm:.2f} > max_norm = {max_norm}. Clipping applied.\")\n",
    "        clipped_grads = [g * rate for g in grads]\n",
    "        return clipped_grads\n",
    "    else:\n",
    "        print(f\"Gradient norm = {norm:.2f} <= max_norm = {max_norm}. No clipping.\")\n",
    "        return grads\n",
    "\n",
    "# 簡単な実験\n",
    "# 2つのパラメータに対する勾配を想定\n",
    "grad1 = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "grad2 = np.array([[5.0, 6.0], [7.0, 8.0]])\n",
    "grads = [grad1, grad2]\n",
    "max_norm = 10.0\n",
    "\n",
    "clipped_grads = gradient_clipping(grads, max_norm)\n",
    "\n",
    "# クリッピング後のノルムを確認\n",
    "clipped_all_grads = np.concatenate([g.flatten() for g in clipped_grads])\n",
    "clipped_norm = np.linalg.norm(clipped_all_grads)\n",
    "print(f\"Norm after clipping: {clipped_norm:.2f}\")\n",
    "\n",
    "print(\"\\n--- No clipping case ---\")\n",
    "max_norm_large = 20.0\n",
    "_ = gradient_clipping(grads, max_norm_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0708ddb",
   "metadata": {},
   "source": [
    "## 5. まとめと考察\n",
    "\n",
    "このノートブックでは、モデルの汎化性能を高めるためのDropoutと、学習を安定させるためのGradient Clippingについて学びました。\n",
    "\n",
    "*   **Dropout**は、学習時にニューロンをランダムに非活性化することで、アンサンブル学習に似た効果を生み出し、ニューロン間の共適応を抑制します。これにより、モデルはより頑健な特徴を学習し、過学習を防ぐことができます。実装はInverted Dropoutが標準的で、推論時の追加処理が不要です。\n",
    "\n",
    "*   **Gradient Clipping**は、勾配爆発を防ぐための安全装置です。勾配の大きさが一定のしきい値を超えた場合に、その方向を維持したまま大きさを制限します。これにより、学習中の突然の発散を防ぎ、特にRNNのような勾配が不安定になりやすいモデルの学習を安定させます。\n",
    "\n",
    "これらのテクニックは、これまで学んできた他のコンポーネント（オプティマイザ、正規化層など）と組み合わせて使用することで、ディープニューラルネットワークの学習をより成功に導くための強力なツールとなります。\n",
    "\n",
    "これで、ニューラルネットワークの主要な構成要素と学習テクニックを一通り学びました。これらの知識を基に、より複雑なアーキテクチャであるCNNやRNN、そしてTransformerへと学びを進めていく準備が整ったと言えるでしょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
