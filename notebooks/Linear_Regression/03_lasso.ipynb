{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe8f883",
   "metadata": {},
   "source": [
    "# ラッソ回帰の実装 (NumPy)\n",
    "\n",
    "このノートブックでは、NumPyのみを使用してラッソ回帰をスクラッチから実装します。\n",
    "ラッソ回帰は、通常の最小二乗法にL1正則化項を加えることで、一部の回帰係数を正確に0にし、変数選択を行うことができる手法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c1496",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a01341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2128bb",
   "metadata": {},
   "source": [
    "## 2. ラッソ回帰の理論的背景\n",
    "\n",
    "通常の線形回帰では、残差平方和 (RSS) を最小化します。\n",
    "$$ L_{OLS}(\\beta) = ||\\mathbf{y} - \\mathbf{X}\\beta||^2_2 $$\n",
    "ラッソ回帰は、このRSSにL1正則化項（係数の絶対値の和、L1ノルム）を加えたものを最小化します。\n",
    "$$ L_{Lasso}(\\beta_0, \\mathbf{\\beta}_{rest}) = \\frac{1}{2N} ||\\mathbf{y} - (\\mathbf{X}_{rest}\\mathbf{\\beta}_{rest} + \\beta_0)||^2_2 + \\alpha ||\\mathbf{\\beta}_{rest}||_1 $$\n",
    "または、単純化して以下のように書かれることもあります（$\\alpha$ のスケールが変わるだけです）。\n",
    "$$ L_{Lasso}(\\beta_0, \\mathbf{\\beta}_{rest}) = \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\sum_{j=1}^{p} x_{ij}\\beta_j))^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "ここでは、実装の都合上、以下の目的関数を考えます（係数 $1/2$ をRSS項の前に置くのが一般的です）。\n",
    "$$ L_{Lasso}(\\beta_0, \\mathbf{\\beta}_{rest}) = \\frac{1}{2} \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\sum_{j=1}^{p} x_{ij}\\beta_j))^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "ここで、\n",
    "*   $N$ はサンプル数です。\n",
    "*   $\\beta_0$ は切片項です。\n",
    "*   $\\mathbf{\\beta}_{rest} = (\\beta_1, ..., \\beta_p)^T$ は切片以外の回帰係数のベクトルです。\n",
    "*   $\\mathbf{X}_{rest}$ は切片項に対応する列を除いた特徴量行列です。\n",
    "*   $\\alpha \\ge 0$ は正則化パラメータで、ペナルティの強さを調整します。\n",
    "*   $||\\mathbf{\\beta}_{rest}||_1 = \\sum_{j=1}^{p} |\\beta_j|$ はL1ノルムです。\n",
    "\n",
    "切片 $\\beta_0$ は通常、正則化の対象外とします。\n",
    "\n",
    "### 最適化: 座標降下法 (Coordinate Descent)\n",
    "L1ノルムの項は微分可能でない点（$\\beta_j=0$）があるため、リッジ回帰のような正規方程式で閉じた形の解を求めることができません。\n",
    "代わりに、座標降下法などの反復的な最適化アルゴリズムが用いられます。\n",
    "\n",
    "座標降下法では、一度に一つの係数 $\\beta_k$ を更新し、他の係数は固定します。この処理を全ての係数に対して繰り返し行い、係数ベクトルが収束するまで続けます。\n",
    "\n",
    "$\\beta_k$ ($k \\in \\{1, ..., p\\}$) を更新する際、他の係数を固定した目的関数を最小化します。\n",
    "$\\beta_k$ に関する偏導関数を0とおくと（実際には劣勾配を考える）、更新式はソフトしきい値関数 (Soft Thresholding operator) $S$ を用いて以下のように表されます。\n",
    "\n",
    "$$ \\rho_k = \\sum_{i=1}^{N} x_{ik} \\left( y_i - \\beta_0 - \\sum_{j \\neq k} x_{ij}\\beta_j \\right) $$\n",
    "$$ \\beta_k \\leftarrow \\frac{S(\\rho_k, \\alpha)}{\\sum_{i=1}^{N} x_{ik}^2} $$\n",
    "\n",
    "ここで、ソフトしきい値関数 $S(z, \\lambda)$ は次のように定義されます。\n",
    "$$ S(z, \\lambda) = \\begin{cases} z - \\lambda & \\text{if } z > 0 \\text{ and } \\lambda < |z| \\\\ z + \\lambda & \\text{if } z < 0 \\text{ and } \\lambda < |z| \\\\ 0 & \\text{if } \\lambda \\ge |z| \\end{cases} $$\n",
    "これは $\\text{sign}(z) \\max(0, |z|-\\lambda)$ とも書けます。\n",
    "$\\sum_{i=1}^{N} x_{ik}^2$ の項は、特徴量が標準化されている場合は $N$ (または $1$、標準化の仕方による) になります。今回は標準化を外部で行うことを想定し、この項を計算に含めます。\n",
    "\n",
    "切片 $\\beta_0$ は、各イテレーションの最後に次のように更新します。\n",
    "$$ \\beta_0 \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\sum_{j=1}^{p} x_{ij}\\beta_j) $$\n",
    "\n",
    "### $\\alpha$ の効果\n",
    "*   $\\alpha = 0$: ラッソ回帰は通常の最小二乗法の結果に近づきます（ただし、座標降下法で解く場合、完全に一致しないこともあります）。\n",
    "*   $\\alpha$ が大きくなるほど、より多くの係数が0になり、モデルはスパースになります（変数選択）。\n",
    "*   非常に大きな $\\alpha$ では、全ての係数が0になることもあります。\n",
    "適切な $\\alpha$ の値は、交差検証などを用いて選択する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50f07a",
   "metadata": {},
   "source": [
    "## 3. データセットの準備\n",
    "\n",
    "簡単なサンプルデータを作成してテストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167fd7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプル特徴量 X_train_sample (shape): (8, 2)\n",
      "[[1 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [3 4]\n",
      " [4 4]\n",
      " [4 5]]\n",
      "\n",
      "サンプル目的変数 y_train_sample (shape): (8,)\n",
      "[ 2.74835708  2.93086785  5.32384427  6.26151493  6.88292331  7.88293152\n",
      " 10.78960641 10.88371736]\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータ\n",
    "X_train_sample = np.array([\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 2],\n",
    "    [2, 3],\n",
    "    [3, 2],\n",
    "    [3, 4],\n",
    "    [4, 4],\n",
    "    [4, 5]\n",
    "])\n",
    "np.random.seed(42)\n",
    "y_train_sample = 2 * X_train_sample[:, 0] + 0.5 * X_train_sample[:, 1] + np.random.normal(0, 0.5, X_train_sample.shape[0])\n",
    "# 係数の一方を小さくして、スパース性が見えやすくする\n",
    "\n",
    "print(\"サンプル特徴量 X_train_sample (shape):\", X_train_sample.shape)\n",
    "print(X_train_sample)\n",
    "print(\"\\nサンプル目的変数 y_train_sample (shape):\", y_train_sample.shape)\n",
    "print(y_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7c366",
   "metadata": {},
   "source": [
    "### 4.1 `LassoRegression` クラスの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714b2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression:\n",
    "    def __init__(self, alpha=1.0, n_iterations=1000, tol=1e-4):\n",
    "        self.alpha = alpha              # 正則化パラメータ\n",
    "        self.n_iterations = n_iterations # 座標降下法の最大反復回数\n",
    "        self.tol = tol                  # 収束判定のための許容誤差\n",
    "        self.intercept_ = None          # 切片 (β₀)\n",
    "        self.coef_ = None               # 回帰係数 (β₁, ..., βₚ)\n",
    "\n",
    "    def _soft_thresholding(self, rho, lambda_val):\n",
    "        if rho < -lambda_val:\n",
    "            return rho + lambda_val\n",
    "        elif rho > lambda_val:\n",
    "            return rho - lambda_val\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        訓練データを用いてモデルを学習する\n",
    "        Parameters:\n",
    "            X(ndarray): 特徴量行列 (サンプル数, 特徴量数). 事前に標準化されていることが望ましい。\n",
    "            y(ndarray): 目的変数ベクトル (サンプル数,)\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # 係数の初期化\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "        self.intercept_ = np.mean(y) # 初期値をyの平均とする\n",
    "\n",
    "        # 座標降下法\n",
    "        for iteration in range(self.n_iterations):\n",
    "            coef_old = np.copy(self.coef_)\n",
    "\n",
    "            # 切片の更新 (イテレーションの最初または最後に更新)\n",
    "            self.intercept_ = np.mean(y - X @ self.coef_)\n",
    "\n",
    "            # 各特徴量の係数を更新\n",
    "            for j in range(n_features):\n",
    "                # ρ_j の計算\n",
    "                # y_pred_without_j = self.intercept_ + X[:, np.arange(n_features) != j] @ self.coef_[np.arange(n_features) != j]\n",
    "                # rho_j = X[:, j] @ (y - y_pred_without_j)\n",
    "                \n",
    "                # より直接的な計算\n",
    "                current_prediction = self.intercept_ + X @ self.coef_\n",
    "                residual_for_j = y - (current_prediction - X[:, j] * self.coef_[j]) # β_j * x_j の効果を除いた残差\n",
    "                rho_j = X[:, j] @ residual_for_j\n",
    "\n",
    "\n",
    "                # 分母の計算 (事前に計算しておくことも可能)\n",
    "                sum_sq_xj = np.sum(X[:, j]**2)\n",
    "                if sum_sq_xj == 0: # 特徴量の列が全て0の場合 (通常は標準化で回避)\n",
    "                    self.coef_[j] = 0\n",
    "                else:\n",
    "                    # ソフトしきい値関数を適用\n",
    "                    # 注意: ここでの alpha は目的関数の lambda に対応。\n",
    "                    # 目的関数を 1/2 RSS + alpha * ||beta||_1 とした場合、ソフトしきい値の第2引数は alpha\n",
    "                    self.coef_[j] = self._soft_thresholding(rho_j, self.alpha) / sum_sq_xj\n",
    "            \n",
    "            # 収束判定\n",
    "            if np.sum(np.abs(self.coef_ - coef_old)) < self.tol:\n",
    "                # print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "        # if iteration == self.n_iterations -1:\n",
    "            # print(\"Warning: Lasso did not converge within max_iterations.\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        学習済みモデルを用いて予測を行う\n",
    "        Parameters:\n",
    "            X(ndarray): 特徴量行列 (サンプル数, 特徴量数)\n",
    "        Returns:\n",
    "            ndarray: 予測値ベクトル (サンプル数,)\n",
    "        '''\n",
    "        if self.intercept_ is None or self.coef_ is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Call 'fit' before 'predict'.\")\n",
    "        \n",
    "        return X @ self.coef_ + self.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a68030",
   "metadata": {},
   "source": [
    "## 5. モデルの学習と予測 (サンプルデータ)\n",
    "\n",
    "異なる `alpha` の値でラッソ回帰モデルを学習させ、係数のスパース性を確認します。\n",
    "ラッソ回帰は特徴量のスケールに敏感なため、通常は学習前に標準化を行いますが、\n",
    "この小さなサンプルデータでは標準化せずに実行してみます。\n",
    "ただし、実践では標準化が重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b22dc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルデータでのラッソ回帰モデルの学習結果 (Xは非標準化):\n",
      "\n",
      "Alpha = 0.1\n",
      "  切片 (β₀): 0.0554\n",
      "  回帰係数 (β₁, β₂): [2.0026 0.5743]\n",
      "---\n",
      "Alpha = 0.5\n",
      "  切片 (β₀): 0.1580\n",
      "  回帰係数 (β₁, β₂): [1.9675 0.5691]\n",
      "---\n",
      "Alpha = 1.0\n",
      "  切片 (β₀): 0.2862\n",
      "  回帰係数 (β₁, β₂): [1.9237 0.5626]\n",
      "---\n",
      "Alpha = 2.0\n",
      "  切片 (β₀): 0.5427\n",
      "  回帰係数 (β₁, β₂): [1.836  0.5496]\n",
      "---\n",
      "Alpha = 5.0\n",
      "  切片 (β₀): 1.3122\n",
      "  回帰係数 (β₁, β₂): [1.5731 0.5107]\n",
      "---\n",
      "Alpha = 10.0\n",
      "  切片 (β₀): 2.5947\n",
      "  回帰係数 (β₁, β₂): [1.1347 0.4457]\n",
      "---\n",
      "Alpha = 20.0\n",
      "  切片 (β₀): 5.1596\n",
      "  回帰係数 (β₁, β₂): [0.2581 0.3159]\n",
      "---\n",
      "\n",
      "訓練データに対する予測結果 (Alpha=1.0):\n",
      "\n",
      "  実測値: 2.75, 予測値: 2.77, 誤差: -0.02\n",
      "  実測値: 2.93, 予測値: 3.34, 誤差: -0.40\n",
      "  実測値: 5.32, 予測値: 5.26, 誤差: 0.07\n",
      "  実測値: 6.26, 予測値: 5.82, 誤差: 0.44\n",
      "  実測値: 6.88, 予測値: 7.18, 誤差: -0.30\n",
      "  実測値: 7.88, 予測値: 8.31, 誤差: -0.42\n",
      "  実測値: 10.79, 予測値: 10.23, 誤差: 0.56\n",
      "  実測値: 10.88, 予測値: 10.79, 誤差: 0.09\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータでテスト (標準化なし)\n",
    "# 注意: 実際にはXを標準化することが推奨されます\n",
    "alphas_to_test_sample = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0] # alpha=0は座標降下法では扱いにくい場合がある\n",
    "\n",
    "print(\"サンプルデータでのラッソ回帰モデルの学習結果 (Xは非標準化):\\n\")\n",
    "for alpha_val in alphas_to_test_sample:\n",
    "    lasso_model_sample = LassoRegression(alpha=alpha_val, n_iterations=2000, tol=1e-5)\n",
    "    lasso_model_sample.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "    print(f\"Alpha = {alpha_val}\")\n",
    "    print(f\"  切片 (β₀): {lasso_model_sample.intercept_:.4f}\")\n",
    "    print(f\"  回帰係数 (β₁, β₂): {np.round(lasso_model_sample.coef_, 4)}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# alpha=1.0のモデルで予測\n",
    "lasso_model_sample_pred = LassoRegression(alpha=1.0, n_iterations=2000, tol=1e-5)\n",
    "lasso_model_sample_pred.fit(X_train_sample, y_train_sample)\n",
    "y_pred_sample = lasso_model_sample_pred.predict(X_train_sample)\n",
    "\n",
    "print(\"\\n訓練データに対する予測結果 (Alpha=1.0):\\n\")\n",
    "for i in range(len(y_train_sample)):\n",
    "    print(f\"  実測値: {y_train_sample[i]:.2f}, 予測値: {y_pred_sample[i]:.2f}, 誤差: {y_train_sample[i] - y_pred_sample[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089c5b8",
   "metadata": {},
   "source": [
    "## 6. より実践的なデータセットでの利用と評価 (Diabetes Dataset)\n",
    "\n",
    "scikit-learnのDiabetesデータセットを使用して、モデルの性能を評価します。\n",
    "ラッソ回帰では特徴量の標準化が特に重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a9e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "\n",
    "# 特徴量の標準化\n",
    "scaler_diabetes = StandardScaler()\n",
    "X_diabetes_scaled = scaler_diabetes.fit_transform(X_diabetes)\n",
    "\n",
    "# 訓練データとテストデータに分割\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(\n",
    "    X_diabetes_scaled, y_diabetes, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ebdd663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "実践的なデータセット (Diabetes, Xは標準化済み) での評価:\n",
      "\n",
      "Alpha = 0.1\n",
      "  切片: 151.3456\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2900.1410\n",
      "  決定係数 (R²): 0.4526\n",
      "---\n",
      "Alpha = 10.0\n",
      "  切片: 151.3432\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2895.1848\n",
      "  決定係数 (R²): 0.4535\n",
      "---\n",
      "Alpha = 100\n",
      "  切片: 151.3429\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2873.8842\n",
      "  決定係数 (R²): 0.4576\n",
      "---\n",
      "Alpha = 1000\n",
      "  切片: 151.6634\n",
      "  係数の非ゼロ要素数: 7 / 10\n",
      "  平均二乗誤差 (MSE): 2800.3160\n",
      "  決定係数 (R²): 0.4715\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Diabetesデータセットで評価\n",
    "# alphaの値はデータセットによって調整が必要\n",
    "# scikit-learnのLassoのalphaは 1/(2*n_samples) * RSS + alpha * ||beta||_1 のalphaに対応することが多い。\n",
    "# 今回の実装のalphaは 1/2 * RSS + alpha * ||beta||_1 のalpha。\n",
    "# そのため、scikit-learnと同じ効果を得るにはalphaを調整する必要がある。\n",
    "# (例: scikit-learnのalpha=0.1 なら、今回のalphaは 0.1 * n_samples 程度になることがある)\n",
    "# ここでは、いくつかの値を試す。\n",
    "alphas_practical_lasso = [0.1, 10.0, 100, 1000] \n",
    "\n",
    "print(\"\\n実践的なデータセット (Diabetes, Xは標準化済み) での評価:\\n\")\n",
    "for alpha_val in alphas_practical_lasso:\n",
    "    model_practical_lasso = LassoRegression(alpha=alpha_val, n_iterations=3000, tol=1e-5) # イテレーション数を増やす\n",
    "    model_practical_lasso.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "    y_pred_diabetes_test = model_practical_lasso.predict(X_diabetes_test)\n",
    "\n",
    "    mse_diabetes = mean_squared_error(y_diabetes_test, y_pred_diabetes_test)\n",
    "    r2_diabetes = r2_score(y_diabetes_test, y_pred_diabetes_test)\n",
    "\n",
    "    print(f\"Alpha = {alpha_val}\")\n",
    "    print(f\"  切片: {model_practical_lasso.intercept_:.4f}\")\n",
    "    print(f\"  係数の非ゼロ要素数: {np.sum(np.abs(model_practical_lasso.coef_) > 1e-6)} / {len(model_practical_lasso.coef_)}\")\n",
    "    # print(f\"  係数: {np.round(model_practical_lasso.coef_, 4)}\") # 全係数を表示すると長いのでコメントアウト\n",
    "    print(f\"  平均二乗誤差 (MSE): {mse_diabetes:.4f}\")\n",
    "    print(f\"  決定係数 (R²): {r2_diabetes:.4f}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c9927",
   "metadata": {},
   "source": [
    "## 7. 考察\n",
    "\n",
    "*   このスクラッチ実装では、ラッソ回帰の係数推定に座標降下法とソフトしきい値関数を用いました。L1正則化項により、一部の係数が正確に0になるスパースな解が得られることが特徴です。\n",
    "*   **長所**:\n",
    "    *   **変数選択**: 不要な特徴量の係数を0にすることで、解釈しやすく、予測に重要な特徴量を特定するのに役立ちます。\n",
    "    *   **過学習の抑制**: リッジ回帰と同様に、係数の大きさを制約することで過学習を抑える効果があります。\n",
    "    *   多重共線性がある場合でも、ラッソはいくつかの変数を選択し、他を0にする傾向があります（リッジは関連する変数の係数を全体的に小さくします）。\n",
    "\n",
    "*   **短所・注意点**:\n",
    "    *   **最適化アルゴリズム**: 閉じた解がないため、座標降下法のような反復解法が必要です。収束性や計算速度はリッジ回帰の正規方程式より劣る場合があります。`n_iterations` や `tol` の設定が重要です。\n",
    "    *   **ハイパーパラメータ `alpha` の選択**: ラッソ回帰の性能は `alpha` の値に大きく依存します。交差検証などによる適切な選択が不可欠です。\n",
    "    *   **特徴量のスケーリング**: L1ペナルティは係数の絶対値にかかるため、特徴量のスケールが異なると、スケールの大きな特徴量が不当に選択されにくくなる（またはその逆）可能性があります。学習前に特徴量を標準化することが強く推奨されます。\n",
    "    *   **相関の高い変数群の扱い**: 相関の強い変数が複数ある場合、ラッソはその中から一つ（あるいは少数）の変数を選択し、他を0にする傾向があります。どの変数が選択されるかはデータやアルゴリズムの挙動に依存することがあり、安定しない場合があります。リッジ回帰はこのような場合に、相関する変数群に係数を分散させる傾向があります。\n",
    "    *   **飽和**: 特徴量数 $p$ がサンプル数 $N$ より大きい場合 ($p > N$)、ラッソが選択できる非ゼロ係数の最大数は $N$ 個です。\n",
    "\n",
    "*   **scikit-learnとの比較**:\n",
    "    *   scikit-learnの `sklearn.linear_model.Lasso` は、より高度で効率的な座標降下法のソルバーを実装しており、大規模データにも対応できます。\n",
    "    *   `alpha` の定義が異なる場合があります。scikit-learnでは目的関数が $\\frac{1}{2N} ||y - X\\beta||_2^2 + \\alpha ||\\beta||_1$ のように正規化されていることが多く、本実装の $\\alpha$ と直接比較する際には注意が必要です。\n",
    "    *   `LassoCV` のように交差検証による `alpha` の自動選択機能も備えています。本スクラッチ実装は、基本的なアルゴリズムを理解するためのものです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
