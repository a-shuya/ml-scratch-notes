{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecf0b45",
   "metadata": {},
   "source": [
    "# Elastic Net回帰の実装 (NumPy)\n",
    "\n",
    "このノートブックでは、NumPyのみを使用してElastic Net回帰をスクラッチから実装します。\n",
    "Elastic Netは、L1正則化（Lassoペナルティ）とL2正則化（Ridgeペナルティ）を組み合わせた手法で、Lassoの変数選択能力とRidgeの多重共線性への対処能力を両立させることを目指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c8337",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f1590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002d8a5",
   "metadata": {},
   "source": [
    "## 2. Elastic Net回帰の理論的背景\n",
    "\n",
    "Elastic Netの目的関数は、残差平方和 (RSS) にL1ペナルティとL2ペナルティの両方を加えたものです。\n",
    "$$ L_{ElasticNet}(\\beta_0, \\mathbf{\\beta}_{rest}) = \\frac{1}{2N} ||\\mathbf{y} - (\\mathbf{X}_{rest}\\mathbf{\\beta}_{rest} + \\beta_0)||^2_2 + \\alpha \\left( \\rho ||\\mathbf{\\beta}_{rest}||_1 + \\frac{1-\\rho}{2} ||\\mathbf{\\beta}_{rest}||_2^2 \\right) $$\n",
    "または、単純化して以下のように書かれることもあります（$1/2N$ の係数の有無や$\\alpha$のスケールに注意）。\n",
    "$$ L_{ElasticNet}(\\beta_0, \\mathbf{\\beta}_{rest}) = \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\sum_{j=1}^{p} x_{ij}\\beta_j))^2 + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "scikit-learnのドキュメントに近い形で、$\\alpha$ を全体のペナルティの強さ、$\\rho$ (または `l1_ratio`) をL1ペナルティとL2ペナルティの混合比率として表現します。\n",
    "ここでは、実装の都合上、以下の目的関数を考えます（係数 $1/2$ をRSS項の前に置くのが一般的）。\n",
    "$$ L_{ElasticNet}(\\beta_0, \\mathbf{\\beta}_{rest}) = \\frac{1}{2} \\sum_{i=1}^{N} (y_i - (\\beta_0 + \\sum_{j=1}^{p} x_{ij}\\beta_j))^2 + \\alpha \\rho \\sum_{j=1}^{p} |\\beta_j| + \\alpha \\frac{1-\\rho}{2} \\sum_{j=1}^{p} \\beta_j^2 $$\n",
    "\n",
    "ここで、\n",
    "*   $N$ はサンプル数です。\n",
    "*   $\\beta_0$ は切片項です。\n",
    "*   $\\mathbf{\\beta}_{rest} = (\\beta_1, ..., \\beta_p)^T$ は切片以外の回帰係数のベクトルです。\n",
    "*   $\\mathbf{X}_{rest}$ は切片項に対応する列を除いた特徴量行列です。\n",
    "*   $\\alpha > 0$ は全体の正則化の強さを制御するパラメータです。\n",
    "*   $0 \\le \\rho \\le 1$ (l1\\_ratio) はL1ペナルティとL2ペナルティの混合比率を制御するパラメータです。\n",
    "    *   $\\rho = 1$ のとき、Elastic NetはLasso回帰と一致します。\n",
    "    *   $\\rho = 0$ のとき、Elastic NetはRidge回帰と一致します（ただし、目的関数のL2項の係数が $\\alpha/2$ となる）。\n",
    "*   切片 $\\beta_0$ は通常、正則化の対象外とします。\n",
    "\n",
    "### 最適化: 座標降下法 (Coordinate Descent)\n",
    "Lassoと同様に、L1ノルムの項があるため、座標降下法を用います。\n",
    "$\\beta_k$ ($k \\in \\{1, ..., p\\}$) を更新する際、他の係数を固定した目的関数を最小化します。\n",
    "更新式は、ソフトしきい値関数 $S$ を用いて以下のように表されます。\n",
    "\n",
    "$$ \\rho_k = \\sum_{i=1}^{N} x_{ik} \\left( y_i - \\beta_0 - \\sum_{j \\neq k} x_{ij}\\beta_j \\right) $$\n",
    "$$ \\beta_k \\leftarrow \\frac{S(\\rho_k, \\alpha \\rho)}{\\sum_{i=1}^{N} x_{ik}^2 + \\alpha (1-\\rho)} $$\n",
    "\n",
    "ここで、ソフトしきい値関数 $S(z, \\lambda)$ は次のように定義されます。\n",
    "$$ S(z, \\lambda) = \\begin{cases} z - \\lambda & \\text{if } z > 0 \\text{ and } \\lambda < |z| \\\\ z + \\lambda & \\text{if } z < 0 \\text{ and } \\lambda < |z| \\\\ 0 & \\text{if } \\lambda \\ge |z| \\end{cases} $$\n",
    "分母の $\\sum_{i=1}^{N} x_{ik}^2$ は、特徴量が標準化されていれば $N$ (または $1$) に近くなります。\n",
    "Lassoの更新式と比較すると、ソフトしきい値の第2引数が $\\alpha \\rho$ になり、分母に $\\alpha (1-\\rho)$ が加わっています。これはL2ペナルティの効果です。\n",
    "\n",
    "切片 $\\beta_0$ は、各イテレーションの最後に次のように更新します。\n",
    "$$ \\beta_0 \\leftarrow \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\sum_{j=1}^{p} x_{ij}\\beta_j) $$\n",
    "\n",
    "### $\\alpha$ と $\\rho$ の効果\n",
    "*   $\\alpha$: 全体的なペナルティの強さ。大きいほど係数は0に近づく。\n",
    "*   $\\rho$: L1とL2の混合比。\n",
    "    *   $\\rho=1$: Lasso回帰。変数選択性が強い。\n",
    "    *   $\\rho=0$: Ridge回帰（に近い形）。係数を全体的に縮小。多重共線性に強い。\n",
    "    *   $0 < \\rho < 1$: LassoとRidgeの特性を併せ持つ。相関の高い変数群がある場合、Lassoが一つだけを選択するのに対し、Elastic Netはグループ全体を選択する傾向がある（グルーピング効果）。\n",
    "適切な $\\alpha$ と $\\rho$ の値は、交差検証などを用いて選択する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6146b",
   "metadata": {},
   "source": [
    "## 3. データセットの準備\n",
    "\n",
    "簡単なサンプルデータを作成してテストします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483ed67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプル特徴量 X_train_sample (shape): (8, 3)\n",
      "[[1.  1.  0.9]\n",
      " [1.  2.  1.1]\n",
      " [2.  2.  1.9]\n",
      " [2.  3.  2.2]\n",
      " [3.  2.  2.8]\n",
      " [3.  4.  3.2]\n",
      " [4.  4.  3.9]\n",
      " [4.  5.  4.1]]\n",
      "\n",
      "サンプル目的変数 y_train_sample (shape): (8,)\n",
      "[ 4.09835708  4.58086785  8.17384427  9.56151493 11.08292331 12.68293152\n",
      " 16.63960641 17.03371736]\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータ\n",
    "X_train_sample = np.array([\n",
    "    [1, 1, 0.9], # 特徴量1と3を似せる\n",
    "    [1, 2, 1.1],\n",
    "    [2, 2, 1.9],\n",
    "    [2, 3, 2.2],\n",
    "    [3, 2, 2.8],\n",
    "    [3, 4, 3.2],\n",
    "    [4, 4, 3.9],\n",
    "    [4, 5, 4.1]\n",
    "])\n",
    "np.random.seed(42)\n",
    "y_train_sample = (2 * X_train_sample[:, 0] + \n",
    "                  0.5 * X_train_sample[:, 1] + \n",
    "                  1.5 * X_train_sample[:, 2] + # 特徴量3にも係数を持たせる\n",
    "                  np.random.normal(0, 0.5, X_train_sample.shape[0]))\n",
    "\n",
    "print(\"サンプル特徴量 X_train_sample (shape):\", X_train_sample.shape)\n",
    "print(X_train_sample)\n",
    "print(\"\\nサンプル目的変数 y_train_sample (shape):\", y_train_sample.shape)\n",
    "print(y_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc0e59",
   "metadata": {},
   "source": [
    "## 4. Elastic Net回帰モデルクラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231ffa6",
   "metadata": {},
   "source": [
    "### 4.1 `ElasticNetRegression` クラスの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b96fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNetRegression:\n",
    "    def __init__(self, alpha=1.0, rho=0.5, n_iterations=1000, tol=1e-4):\n",
    "        self.alpha = alpha              # 全体の正則化パラメータ\n",
    "        self.rho = rho                  # L1混合比 (l1_ratio)\n",
    "        self.n_iterations = n_iterations # 座標降下法の最大反復回数\n",
    "        self.tol = tol                  # 収束判定のための許容誤差\n",
    "        self.intercept_ = None          # 切片 (β₀)\n",
    "        self.coef_ = None               # 回帰係数 (β₁, ..., βₚ)\n",
    "\n",
    "    def _soft_thresholding(self, z, lambda_val):\n",
    "        if z < -lambda_val:\n",
    "            return z + lambda_val\n",
    "        elif z > lambda_val:\n",
    "            return z - lambda_val\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        訓練データを用いてモデルを学習する\n",
    "        Parameters:\n",
    "            X(ndarray): 特徴量行列 (サンプル数, 特徴量数). 事前に標準化されていることが望ましい。\n",
    "            y(ndarray): 目的変数ベクトル (サンプル数,)\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # 係数の初期化\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "        self.intercept_ = np.mean(y) # 初期値をyの平均とする\n",
    "\n",
    "        # L1ペナルティとL2ペナルティの係数\n",
    "        l1_penalty = self.alpha * self.rho\n",
    "        l2_penalty_factor = self.alpha * (1 - self.rho) #分母に加えるのは alpha*(1-rho)\n",
    "\n",
    "        # 座標降下法\n",
    "        for iteration in range(self.n_iterations):\n",
    "            coef_old = np.copy(self.coef_)\n",
    "\n",
    "            # 切片の更新\n",
    "            self.intercept_ = np.mean(y - X @ self.coef_)\n",
    "\n",
    "            # 各特徴量の係数を更新\n",
    "            for j in range(n_features):\n",
    "                # ρ_j (rho_k in theory section) の計算\n",
    "                current_prediction = self.intercept_ + X @ self.coef_\n",
    "                # β_j * x_j の効果を除いた残差\n",
    "                residual_for_j = y - (current_prediction - X[:, j] * self.coef_[j]) \n",
    "                rho_j_val = X[:, j] @ residual_for_j\n",
    "                \n",
    "                # 分母の計算\n",
    "                sum_sq_xj = np.sum(X[:, j]**2)\n",
    "                \n",
    "                denominator = sum_sq_xj + l2_penalty_factor # Lassoとの違い：L2ペナルティ項\n",
    "                if denominator == 0: \n",
    "                    self.coef_[j] = 0\n",
    "                else:\n",
    "                    # ソフトしきい値関数を適用\n",
    "                    self.coef_[j] = self._soft_thresholding(rho_j_val, l1_penalty) / denominator\n",
    "            \n",
    "            # 収束判定\n",
    "            if np.sum(np.abs(self.coef_ - coef_old)) < self.tol:\n",
    "                # print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "        # if iteration == self.n_iterations - 1:\n",
    "            # print(\"Warning: ElasticNet did not converge within max_iterations.\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        学習済みモデルを用いて予測を行う\n",
    "        Parameters:\n",
    "            X(ndarray): 特徴量行列 (サンプル数, 特徴量数)\n",
    "        Returns:\n",
    "            ndarray: 予測値ベクトル (サンプル数,)\n",
    "        '''\n",
    "        if self.intercept_ is None or self.coef_ is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Call 'fit' before 'predict'.\")\n",
    "        \n",
    "        return X @ self.coef_ + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62258ee1",
   "metadata": {},
   "source": [
    "## 5. モデルの学習と予測 (サンプルデータ)\n",
    "\n",
    "異なる `alpha` と `rho` の値でElastic Netモデルを学習させ、係数の変化を確認します。\n",
    "実践では標準化が重要ですが、この小さなサンプルでは標準化せずに行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2771e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルデータでのElastic Netモデルの学習結果 (Xは非標準化):\n",
      "\n",
      "Alpha = 0.1, Rho (l1_ratio) = 0.1\n",
      "  切片 (β₀): 0.0562\n",
      "  回帰係数: [2.0343 0.6236 1.4117]\n",
      "---\n",
      "Alpha = 0.1, Rho (l1_ratio) = 0.5\n",
      "  切片 (β₀): 0.0440\n",
      "  回帰係数: [2.1568 0.6369 1.2794]\n",
      "---\n",
      "Alpha = 0.1, Rho (l1_ratio) = 0.9\n",
      "  切片 (β₀): -0.0139\n",
      "  回帰係数: [2.7662 0.768  0.5461]\n",
      "---\n",
      "Alpha = 0.1, Rho (l1_ratio) = 1.0\n",
      "  切片 (β₀): -0.0552\n",
      "  回帰係数: [3.2138 0.8704 0.    ]\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 0.1\n",
      "  切片 (β₀): 0.1867\n",
      "  回帰係数: [1.8002 0.6911 1.5155]\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 0.5\n",
      "  切片 (β₀): 0.1744\n",
      "  回帰係数: [1.8681 0.6446 1.506 ]\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 0.9\n",
      "  切片 (β₀): 0.1468\n",
      "  回帰係数: [2.1179 0.6303 1.2849]\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 1.0\n",
      "  切片 (β₀): 0.0474\n",
      "  回帰係数: [3.1787 0.8652 0.    ]\n",
      "---\n",
      "Alpha = 1.0, Rho (l1_ratio) = 0.1\n",
      "  切片 (β₀): 0.3362\n",
      "  回帰係数: [1.6851 0.7724 1.4774]\n",
      "---\n",
      "Alpha = 1.0, Rho (l1_ratio) = 0.5\n",
      "  切片 (β₀): 0.3145\n",
      "  回帰係数: [1.761  0.6965 1.4974]\n",
      "---\n",
      "Alpha = 1.0, Rho (l1_ratio) = 0.9\n",
      "  切片 (β₀): 0.2871\n",
      "  回帰係数: [1.9486 0.6124 1.4179]\n",
      "---\n",
      "Alpha = 1.0, Rho (l1_ratio) = 1.0\n",
      "  切片 (β₀): 0.1757\n",
      "  回帰係数: [3.1349 0.8587 0.    ]\n",
      "---\n",
      "\n",
      "訓練データに対する予測結果 (Alpha=0.5, Rho=0.5):\n",
      "\n",
      "  実測値: 4.10, 予測値: 4.04, 誤差: 0.06\n",
      "  実測値: 4.58, 予測値: 4.99, 誤差: -0.41\n",
      "  実測値: 8.17, 予測値: 8.06, 誤差: 0.11\n",
      "  実測値: 9.56, 予測値: 9.16, 誤差: 0.40\n",
      "  実測値: 11.08, 予測値: 11.28, 誤差: -0.20\n",
      "  実測値: 12.68, 予測値: 13.18, 誤差: -0.49\n",
      "  実測値: 16.64, 予測値: 16.10, 誤差: 0.54\n",
      "  実測値: 17.03, 予測値: 17.04, 誤差: -0.01\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータでテスト (標準化なし)\n",
    "alphas_sample = [0.1, 0.5, 1.0]\n",
    "rhos_sample = [0.1, 0.5, 0.9, 1.0] # rho=1.0はLasso\n",
    "\n",
    "print(\"サンプルデータでのElastic Netモデルの学習結果 (Xは非標準化):\\n\")\n",
    "for alpha_val in alphas_sample:\n",
    "    for rho_val in rhos_sample:\n",
    "        if rho_val == 0 and alpha_val == 0: # Ridgeでalpha=0は特異行列問題の可能性\n",
    "            print(f\"Skipping Alpha = {alpha_val}, Rho = {rho_val} (Potential singularity for Ridge with alpha=0)\")\n",
    "            continue\n",
    "        if rho_val == 0 and alpha_val > 0 : # 今回の実装はrho=0を直接扱えない (l1_penaltyが0になりソフトしきい値の挙動)\n",
    "                                         # Ridgeは別途実装するか、scikit-learnのElasticNetでrhoを非常に小さくする\n",
    "            print(f\"Skipping Rho = {rho_val} (Pure Ridge behavior not directly handled by this soft-thresholding based ElasticNet, use RidgeRegression class)\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        model_enet_sample = ElasticNetRegression(alpha=alpha_val, rho=rho_val, n_iterations=3000, tol=1e-5)\n",
    "        model_enet_sample.fit(X_train_sample, y_train_sample)\n",
    "        \n",
    "        print(f\"Alpha = {alpha_val}, Rho (l1_ratio) = {rho_val}\")\n",
    "        print(f\"  切片 (β₀): {model_enet_sample.intercept_:.4f}\")\n",
    "        print(f\"  回帰係数: {np.round(model_enet_sample.coef_, 4)}\")\n",
    "        print(\"---\")\n",
    "\n",
    "# alpha=0.5, rho=0.5 のモデルで予測\n",
    "model_enet_sample_pred = ElasticNetRegression(alpha=0.5, rho=0.5, n_iterations=3000, tol=1e-5)\n",
    "model_enet_sample_pred.fit(X_train_sample, y_train_sample)\n",
    "y_pred_sample_enet = model_enet_sample_pred.predict(X_train_sample)\n",
    "\n",
    "print(\"\\n訓練データに対する予測結果 (Alpha=0.5, Rho=0.5):\\n\")\n",
    "for i in range(len(y_train_sample)):\n",
    "    print(f\"  実測値: {y_train_sample[i]:.2f}, 予測値: {y_pred_sample_enet[i]:.2f}, 誤差: {y_train_sample[i] - y_pred_sample_enet[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99132c",
   "metadata": {},
   "source": [
    "## 6. より実践的なデータセットでの利用と評価 (Diabetes Dataset)\n",
    "\n",
    "scikit-learnのDiabetesデータセットを使用して、モデルの性能を評価します。\n",
    "特徴量の標準化が特に重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45a842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "実践的なデータセット (Diabetes, Xは標準化済み) での評価:\n",
      "\n",
      "Alpha = 0.01, Rho (l1_ratio) = 0.1\n",
      "  切片: 151.3455\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2900.0867\n",
      "  決定係数 (R²): 0.4526\n",
      "---\n",
      "Alpha = 0.01, Rho (l1_ratio) = 0.5\n",
      "  切片: 151.3456\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2900.1318\n",
      "  決定係数 (R²): 0.4526\n",
      "---\n",
      "Alpha = 0.01, Rho (l1_ratio) = 0.9\n",
      "  切片: 151.3456\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2900.1770\n",
      "  決定係数 (R²): 0.4526\n",
      "---\n",
      "Alpha = 0.1, Rho (l1_ratio) = 0.1\n",
      "  切片: 151.3448\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2899.1622\n",
      "  決定係数 (R²): 0.4528\n",
      "---\n",
      "Alpha = 0.1, Rho (l1_ratio) = 0.5\n",
      "  切片: 151.3451\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2899.5878\n",
      "  決定係数 (R²): 0.4527\n",
      "---\n",
      "Alpha = 0.1, Rho (l1_ratio) = 0.9\n",
      "  切片: 151.3455\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2900.0285\n",
      "  決定係数 (R²): 0.4526\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 0.1\n",
      "  切片: 151.3419\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2895.7339\n",
      "  決定係数 (R²): 0.4534\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 0.5\n",
      "  切片: 151.3433\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2897.4102\n",
      "  決定係数 (R²): 0.4531\n",
      "---\n",
      "Alpha = 0.5, Rho (l1_ratio) = 0.9\n",
      "  切片: 151.3450\n",
      "  係数の非ゼロ要素数: 10 / 10\n",
      "  平均二乗誤差 (MSE): 2899.3833\n",
      "  決定係数 (R²): 0.4528\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "\n",
    "# 特徴量の標準化\n",
    "scaler_diabetes = StandardScaler()\n",
    "X_diabetes_scaled = scaler_diabetes.fit_transform(X_diabetes)\n",
    "\n",
    "# 訓練データとテストデータに分割\n",
    "X_diabetes_train, X_diabetes_test, y_diabetes_train, y_diabetes_test = train_test_split(\n",
    "    X_diabetes_scaled, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Diabetesデータセットで評価\n",
    "alphas_practical_enet = [0.01, 0.1, 0.5] # scikit-learnのalphaとはスケールが異なる可能性\n",
    "rhos_practical_enet = [0.1, 0.5, 0.9]    # l1_ratio\n",
    "\n",
    "print(\"\\n実践的なデータセット (Diabetes, Xは標準化済み) での評価:\\n\")\n",
    "for alpha_val in alphas_practical_enet:\n",
    "    for rho_val in rhos_practical_enet:\n",
    "        if rho_val == 0: # 純粋なRidgeは別途\n",
    "            print(f\"Skipping Rho = {rho_val}\")\n",
    "            continue\n",
    "\n",
    "        model_practical_enet = ElasticNetRegression(alpha=alpha_val, rho=rho_val, n_iterations=5000, tol=1e-6) # イテレーション数とtolを調整\n",
    "        model_practical_enet.fit(X_diabetes_train, y_diabetes_train)\n",
    "\n",
    "        y_pred_diabetes_test = model_practical_enet.predict(X_diabetes_test)\n",
    "\n",
    "        mse_diabetes = mean_squared_error(y_diabetes_test, y_pred_diabetes_test)\n",
    "        r2_diabetes = r2_score(y_diabetes_test, y_pred_diabetes_test)\n",
    "\n",
    "        print(f\"Alpha = {alpha_val}, Rho (l1_ratio) = {rho_val}\")\n",
    "        print(f\"  切片: {model_practical_enet.intercept_:.4f}\")\n",
    "        print(f\"  係数の非ゼロ要素数: {np.sum(np.abs(model_practical_enet.coef_) > 1e-6)} / {len(model_practical_enet.coef_)}\")\n",
    "        # print(f\"  係数: {np.round(model_practical_enet.coef_, 4)}\")\n",
    "        print(f\"  平均二乗誤差 (MSE): {mse_diabetes:.4f}\")\n",
    "        print(f\"  決定係数 (R²): {r2_diabetes:.4f}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e63f3",
   "metadata": {},
   "source": [
    "## 7. 考察\n",
    "\n",
    "*   このスクラッチ実装では、Elastic Net回帰の係数推定に座標降下法とElastic Netに対応するソフトしきい値の更新式を用いました。L1ペナルティとL2ペナルティを組み合わせることで、Lassoの変数選択能力とRidgeの安定性を両立させることを目指します。\n",
    "*   **長所**:\n",
    "    *   **変数選択とグルーピング効果**: Lassoのように変数を選択しつつ、Ridgeのように相関の高い特徴量群をまとめて扱う（係数を0にするか、グループとして残すか）傾向があります。これは「グルーピング効果」と呼ばれます。\n",
    "    *   **安定性**: $p > N$ の場合や多重共線性が強い場合に、Lassoよりも安定した解を与えることがあります。\n",
    "    *   **LassoとRidgeの一般化**: $\\rho=1$ でLasso、$\\rho \\approx 0$ でRidge（の挙動に近い形）となり、両手法の一般化と見なせます。\n",
    "\n",
    "*   **短所・注意点**:\n",
    "    *   **最適化アルゴリズム**: Lassoと同様に反復解法が必要です。`n_iterations` や `tol` の設定が収束に影響します。\n",
    "    *   **ハイパーパラメータの選択**: $\\alpha$ と $\\rho$ の2つのハイパーパラメータを調整する必要があり、探索空間が広がります。交差検証による適切な選択がより重要になります。\n",
    "    *   **特徴量のスケーリング**: L1ペナルティとL2ペナルティの両方が係数の大きさに依存するため、学習前に特徴量を標準化することが強く推奨されます。\n",
    "    *   **$\\rho=0$ (純粋なRidge) の扱い**: 今回の座標降下法の実装では、$\\rho=0$ の場合、L1ペナルティ項が消え、ソフトしきい値関数が単純な除算になりますが、これはRidge回帰の正規方程式による解と完全に一致するわけではありません（特に$\\alpha (1-\\rho)$ の項が分母にだけ入るため）。純粋なRidge回帰を座標降下法で解く場合は更新式が異なります。scikit-learnのElasticNetでは $\\rho=0$ でRidgeと等価になります。\n",
    "\n",
    "*   **scikit-learnとの比較**:\n",
    "    *   scikit-learnの `sklearn.linear_model.ElasticNet` は、効率的なソルバーを実装しており、ハイパーパラメータの調整も容易です。\n",
    "    *   `alpha` と `l1_ratio` (本実装の `rho`) の定義や目的関数の正規化係数が異なる場合があるため、パラメータを直接比較する際には注意が必要です。\n",
    "    *   `ElasticNetCV` のように交差検証によるハイパーパラメータの自動選択機能も備えています。本スクラッチ実装は、アルゴリズムの基本的な流れを理解するためのものです。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
