{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a924492",
   "metadata": {},
   "source": [
    "# GRU (Gated Recurrent Unit) の実装\n",
    "\n",
    "このノートブックでは、LSTMと同様に長期依存性の問題を解決するために提案された、もう一つの主要なゲート付きリカレントニューラルネットワークであるGRU (Gated Recurrent Unit) のアーキテクチャと動作原理を学びます。\n",
    "NumPyを使ってGRUセルの主要な計算ステップを実装し、その後PyTorchの`nn.GRU`モジュールを使ってモデルを構築・学習させます。\n",
    "\n",
    "**参考論文:**\n",
    "*   Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. *arXiv preprint arXiv:1406.1078*. (GRUが提案された主要論文の一つ)\n",
    "*   Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. *arXiv preprint arXiv:1412.3555*. (GRUの性能評価)\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  GRUのアーキテクチャ：更新ゲートとリセットゲート。\n",
    "2.  LSTMとの比較を通じたGRUの構造と特徴の理解。\n",
    "3.  NumPyによるGRUセルの順伝播計算の実装。\n",
    "4.  PyTorchの `nn.GRU` モジュールを使ったGRUモデルの実装と学習。\n",
    "\n",
    "**前提知識:**\n",
    "*   LSTMの基本的な構造と動作原理の理解（前のノートブック）。\n",
    "*   PyTorchの基本的な使い方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86dc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
