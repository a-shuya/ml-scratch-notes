{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d5d71d",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "以下の論文にしたがって実装を進めていきます。  \n",
    "https://www.researchgate.net/publication/227658748_Classification_and_Regression_Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66af76",
   "metadata": {},
   "source": [
    "## 0. 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ab0e4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0387bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルデータ (X_sample_np):\n",
      "[[ 1  0]\n",
      " [ 2  1]\n",
      " [ 3  0]\n",
      " [ 4  2]\n",
      " [ 5  1]\n",
      " [ 6  2]\n",
      " [ 7  0]\n",
      " [ 8  0]\n",
      " [ 9  1]\n",
      " [10  2]]\n",
      "\n",
      "サンプルクラスラベル (y_sample_np):\n",
      "[0 1 0 1 0 1 0 0 1 1]\n",
      "\n",
      "特徴量タイプ:\n",
      "['numeric', 'categorical']\n"
     ]
    }
   ],
   "source": [
    "# 特徴量: X_sample\n",
    "# 列0: 数値特徴量\n",
    "# 列1: カテゴリ特徴量 (0: 'A', 1: 'B', 2: 'C' と仮定)\n",
    "X_sample = np.array([\n",
    "    [1, 0], [2, 1], [3, 0], [4, 2], [5, 1],\n",
    "    [6, 2], [7, 0], [8, 0], [9, 1], [10, 2]\n",
    "])\n",
    "# クラスラベル: y_sample\n",
    "y_sample = np.array([0, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
    "\n",
    "# 特徴量のタイプ ('numeric' or 'categorical')\n",
    "feature_types_sample = ['numeric', 'categorical']\n",
    "# カテゴリ特徴量のマッピング (デバッグや表示用)\n",
    "category_mapping_sample = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "\n",
    "print(\"サンプルデータ (X_sample_np):\")\n",
    "print(X_sample)\n",
    "print(\"\\nサンプルクラスラベル (y_sample_np):\")\n",
    "print(y_sample)\n",
    "print(\"\\n特徴量タイプ:\")\n",
    "print(feature_types_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3173e14",
   "metadata": {},
   "source": [
    "## 1. Gini不純度の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492c358",
   "metadata": {},
   "source": [
    "\n",
    "*   **ステップ 1.1: Gini不純度の概念説明**\n",
    "    *   決定木は、データをより「純粋な」サブセットに分割していくことで構築されます。純粋とは、サブセット内のクラスラベルがほぼ同じであることを意味します。\n",
    "    *   Gini不純度は、ノード内のデータがどれだけ混ざっているかを示す指標の一つです。\n",
    "    *   計算式: $Gini(D) = 1 - Σ (p_k)^2$\n",
    "        *   $D$: データセット（またはノード内のデータ）\n",
    "        *   $p_k$: クラス $k$ に属するサンプルの割合\n",
    "    *   Gini不純度の値の範囲は 0 から 0.5 (2クラスの場合) または最大で '1 - 1/num_classes' です。\n",
    "        *   0 の場合: ノードは完全に純粋（すべてのサンプルが同じクラス）。\n",
    "        *   値が大きいほど: ノードの不純度が高い（クラスが混在している）。\n",
    "    *   論文のCARTのセクションでは、Gini Indexが不純度関数として使われることが言及されています。\n",
    "\n",
    "*   **ステップ 1.2: `calculate_gini` 関数の実装**\n",
    "    *   **目的**: 与えられたクラスラベルのリスト（またはpandas Series）からGini不純度を計算します。\n",
    "    *   **引数**:\n",
    "        *   `y_subset` (pd.Series or np.array): クラスラベルのサブセット。\n",
    "    *   **戻り値**:\n",
    "        *   `imprity`: 計算されたGini不純度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f973b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(y_subset) -> float:\n",
    "    # 空のサブセットは不純度 = 0\n",
    "    if len(y_subset) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # 各クラスのサンプル数をカウント\n",
    "    counts = Counter(y_subset)\n",
    "    imprity = 1.0\n",
    "    for key in counts:\n",
    "        # クラスkの確率\n",
    "        p_k = counts[key] / len(y_subset)\n",
    "        # Gini不純度の計算\n",
    "        imprity -= p_k ** 2\n",
    "    return imprity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "43afc8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gini impurity for test_labels1: 0.5\n",
      "Gini impurity for test_labels2: 0.4200000000000001\n",
      "Gini impurity for test_labels3: 0.48\n"
     ]
    }
   ],
   "source": [
    "test_labels1 = np.array([0, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
    "test_labels2 = np.array([0, 0, 1, 1, 0, 1, 1, 1, 1, 1])\n",
    "test_labels3 = np.array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "print(\"\\nGini impurity for test_labels1:\", calculate_gini(test_labels1))\n",
    "print(\"Gini impurity for test_labels2:\", calculate_gini(test_labels2))\n",
    "print(\"Gini impurity for test_labels3:\", calculate_gini(test_labels3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e14471",
   "metadata": {},
   "source": [
    "## 2. 最適な分割の探索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23674254",
   "metadata": {},
   "source": [
    "\n",
    "*   **ステップ 2.1: 分割の概念説明**\n",
    "    *   決定木は、各ノードでデータセットを2つの子ノードに分割します。\n",
    "    *   最適な分割とは、分割後の子ノードの不純度（Gini不純度の加重平均）が最も小さくなるような分割です。これは、親ノードの不純度からの「Gini不純度の減少量 (Gini Reduction)」が最大になる分割とも言えます。\n",
    "    *   `Gini Reduction = Gini(parent) - (weight_left * Gini(left_child) + weight_right * Gini(right_child))`\n",
    "    *   数値特徴量の場合: `X <= threshold` のような条件で分割します。全てのユニークな値の間の中点が分割候補となります。\n",
    "    *   カテゴリ特徴量の場合: `X == category_value` (True/False) のような条件で分割します。各ユニークなカテゴリ値が分割候補となります。(論文のTHAIDはより一般的にサブセットSを探索します。)\n",
    "\n",
    "*   **ステップ 2.2: `find_best_split` 関数の実装**\n",
    "    *   **目的**: 与えられたデータサブセットに対して、すべての特徴量とすべての可能な分割点を試し、Gini不純度を最も減少させる分割を見つけます。\n",
    "    *   **引数**:\n",
    "        *   `X_subset`: 特徴量のサブセット。\n",
    "        *   `y_subset`: クラスラベルのサブセット。\n",
    "        *   `feature_types`: 各特徴量のタイプ ('numeric' or 'categorical')。\n",
    "    *   **戻り値**:\n",
    "        *   `best_split_info`: 最適な分割情報 (例: `{'feature_index': ..., 'threshold': ..., 'type': 'numeric', 'gini_reduction': ..., 'left_indices': ..., 'right_indices': ...}`)\n",
    "        *   分割によって不純度が減少しない、または分割できない場合は `None`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0474d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X_subset, y_subset, feature_types) -> dict:\n",
    "    '''\n",
    "    最適な分割特徴量と分割点を見つける関数\n",
    "    '''\n",
    "\n",
    "    best_split_info = None\n",
    "    max_gini_reduction = -1.0\n",
    "\n",
    "    parent_gini = calculate_gini(y_subset)\n",
    "    n_samples = len(y_subset)\n",
    "\n",
    "    if n_samples <= 1:\n",
    "        return None\n",
    "    \n",
    "    for feature_index in range(X_subset.shape[1]):\n",
    "        feature_values = X_subset[:, feature_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        if feature_types[feature_index] == 'numeric':\n",
    "            sorted_unique_values = np.sort(unique_values)\n",
    "            threshold_candidates = []\n",
    "\n",
    "            if len(sorted_unique_values) > 1:\n",
    "                threshold_candidates = (sorted_unique_values[:-1] + sorted_unique_values[1:]) / 2\n",
    "\n",
    "            for threshold in threshold_candidates:\n",
    "                # ブールインデックスを作成\n",
    "                left_bool_indices = feature_values <= threshold\n",
    "                right_bool_indices = feature_values > threshold\n",
    "\n",
    "                y_left = y_subset[left_bool_indices]\n",
    "                y_right = y_subset[right_bool_indices]\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini_left = calculate_gini(y_left)\n",
    "                gini_right = calculate_gini(y_right)\n",
    "\n",
    "                weighted_gini_children = (\n",
    "                    (len(y_left) / n_samples) * gini_left +\n",
    "                    (len(y_right) / n_samples) * gini_right\n",
    "                )\n",
    "\n",
    "                current_gini_reduction = parent_gini - weighted_gini_children\n",
    "\n",
    "                if current_gini_reduction > max_gini_reduction:\n",
    "                    max_gini_reduction = current_gini_reduction\n",
    "                    best_split_info = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'type': 'numeric',\n",
    "                        'gini_reduction': current_gini_reduction,\n",
    "                        'left_indices_bool': left_bool_indices,\n",
    "                        'right_indices_bool': right_bool_indices\n",
    "                    }\n",
    "\n",
    "        elif feature_types[feature_index] == 'categorical':\n",
    "            for category in unique_values:\n",
    "                left_bool_indices = feature_values == category\n",
    "                right_bool_indices = feature_values != category\n",
    "                \n",
    "                y_left = y_subset[left_bool_indices]\n",
    "                y_right = y_subset[right_bool_indices]\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini_left = calculate_gini(y_left)\n",
    "                gini_right = calculate_gini(y_right)\n",
    "\n",
    "                weighted_gini_children = (\n",
    "                    (len(y_left) / n_samples) * gini_left +\n",
    "                    (len(y_right) / n_samples) * gini_right\n",
    "                )\n",
    "\n",
    "                current_gini_reduction = parent_gini - weighted_gini_children\n",
    "\n",
    "                if current_gini_reduction > max_gini_reduction:\n",
    "                    max_gini_reduction = current_gini_reduction\n",
    "                    best_split_info = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'category': category,\n",
    "                        'type': 'categorical',\n",
    "                        'gini_reduction': current_gini_reduction,\n",
    "                        'left_indices_bool': left_bool_indices,\n",
    "                        'right_indices_bool': right_bool_indices\n",
    "                    }\n",
    "    \n",
    "    if max_gini_reduction <= 0:\n",
    "        return None\n",
    "    \n",
    "    return best_split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "813eed28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最適な分割特徴量のインデックス: 1\n",
      "カテゴリ特徴量の値: A\n",
      "Gini不純度の減少: 0.33333333333333337\n",
      "左のインデックスブール: [ True False  True False False False  True  True False False]\n",
      "右のインデックスブール: [False  True False  True  True  True False False  True  True]\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "split_result = find_best_split(X_sample, y_sample, feature_types_sample)\n",
    "if split_result:\n",
    "    fest_idx = split_result['feature_index']\n",
    "    print(\"\\n最適な分割特徴量のインデックス:\", fest_idx)\n",
    "    if split_result['type'] == 'numeric':\n",
    "        print(\"分割点:\", split_result['threshold'])\n",
    "    else:\n",
    "        cat_val = split_result['category']\n",
    "        cat_name = category_mapping_sample[cat_val]\n",
    "        print(\"カテゴリ特徴量の値:\", cat_name)\n",
    "    print(\"Gini不純度の減少:\", split_result['gini_reduction'])\n",
    "    print(\"左のインデックスブール:\", split_result['left_indices_bool'])\n",
    "    print(\"右のインデックスブール:\", split_result['right_indices_bool'])\n",
    "\n",
    "else:\n",
    "    print(\"\\n最適な分割が見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ba6a6d",
   "metadata": {},
   "source": [
    "## 3. 決定木のノード構造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86de5da",
   "metadata": {},
   "source": [
    "*   **ステップ 3.1: `Node` クラスの概念説明**\n",
    "    *   決定木はノードの集まりで構成されます。各ノードは以下の情報を持つことができます。\n",
    "        *   **中間ノード (Internal Node)**: データを分割するための条件（特徴量、閾値/カテゴリ）。左右の子ノードへの参照。\n",
    "        *   **葉ノード (Leaf Node)**: 最終的な予測クラスラベル。\n",
    "    *   また、デバッグや分析のために、ノードのGini不純度や含まれるサンプル数などの情報も保持しておくと便利です。\n",
    "*   **ステップ 3.2: `Node` クラスの実装**\n",
    "    *   **目的**: 決定木の各ノードを表すクラスを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5a12842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "            self,\n",
    "            feature_index=None,\n",
    "            threshold=None,\n",
    "            category=None,\n",
    "            feature_type=None,\n",
    "            left_child=None,\n",
    "            right_child=None,\n",
    "            value=None,\n",
    "            gini=None,\n",
    "            num_samples=None,\n",
    "    ):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.category = category\n",
    "        self.feature_type = feature_type\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.value = value\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e928b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf_node_exampleは葉ノードですか？: True\n",
      "leaf_node_exampleの値: 0, Gini: 0.0, サンプル数: 10\n"
     ]
    }
   ],
   "source": [
    "leaf_node_example = Node(value=0, gini=0.0, num_samples=10)\n",
    "print(f\"leaf_node_exampleは葉ノードですか？: {leaf_node_example.is_leaf_node()}\")\n",
    "print(f\"leaf_node_exampleの値: {leaf_node_example.value}, Gini: {leaf_node_example.gini}, サンプル数: {leaf_node_example.num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21204bcd",
   "metadata": {},
   "source": [
    "## 4. 決定木の構築"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f6861",
   "metadata": {},
   "source": [
    "*   **ステップ 4.1: `_build_tree` メソッドの概念説明 (ClassificationTreeクラス内)**\n",
    "    *   このメソッドは、決定木を再帰的に構築する主要なロジックを含みます。\n",
    "    *   **処理の流れ**:\n",
    "        1.  **停止条件のチェック**:\n",
    "            *   現在のノードの深さが `max_depth` に達したか？\n",
    "            *   現在のノードのサンプル数が `min_samples_split` より少ないか？\n",
    "            *   現在のノードのクラスラベルがすべて同じか (Gini不純度が0か)？\n",
    "            *   もし停止条件を満たせば、葉ノードを作成して終了。葉ノードの値は、そのノード内のサンプルの多数決で決定します。\n",
    "        2.  **最適な分割の探索**: `find_best_split` を呼び出して、現在のデータセットに対する最適な分割を見つけます。\n",
    "        3.  **分割不可の場合**: もし有益な分割が見つからなければ（例: Gini Reductionが0以下）、葉ノードを作成して終了。\n",
    "        4.  **葉ノードの最小サンプル数チェック**: 分割によって生成される子ノードのサンプル数が `min_samples_leaf` を下回る場合、分割せずに葉ノードを作成します。\n",
    "        5.  **再帰的な分割**:\n",
    "            *   見つかった最適な分割に基づいて、データセットを左の子ノード用と右の子ノード用に分割します。\n",
    "            *   左の子ノードと右の子ノードに対して、`_build_tree` を再帰的に呼び出します。\n",
    "            *   返された左右の子ノードを使って、現在のノードを作成します。\n",
    "    *   論文の Algorithm 1 \"Pseudocode for tree construction by exhaustive search\" の Step 3 \"If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn.\" がこの再帰構造に対応します。\n",
    "\n",
    "*   **ステップ 4.2: `ClassificationTree` クラスと `_build_tree` メソッドの実装**\n",
    "    *   **目的**: 分類木全体を管理するクラスと、その中で木を再帰的に構築するメソッドを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5a206b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.feature_types = None\n",
    "\n",
    "    def _majority_vote(self, y_data):\n",
    "        '''\n",
    "        最頻のクラスラベルを返す\n",
    "        '''\n",
    "        if len(y_data) == 0:\n",
    "            return None\n",
    "        counts = Counter(y_data)\n",
    "        return counts.most_common(1)[0][0]\n",
    "    \n",
    "    def _build_tree(self, X_data, y_data, depth=0):\n",
    "        '''\n",
    "        決定木を再帰的に構築する\n",
    "        '''\n",
    "        n_samples, n_features = X_data.shape\n",
    "        current_gini = calculate_gini(y_data)\n",
    "        \n",
    "        # 停止条件のチェック\n",
    "        if(\n",
    "            (self.max_depth is not None and depth >= self.max_depth) or\n",
    "            (n_samples < self.min_samples_split) or\n",
    "            (len(np.unique(y_data)) == 1) \n",
    "        ):\n",
    "            leaf_value = self._majority_vote(y_data)\n",
    "            return Node(value=leaf_value, gini=current_gini, num_samples=n_samples)\n",
    "        \n",
    "        best_split_info = find_best_split(X_data, y_data, self.feature_types)\n",
    "\n",
    "        # 最適な分割が見つからない場合は葉ノードを返す\n",
    "        if best_split_info is None:\n",
    "            leaf_value = self._majority_vote(y_data)\n",
    "            return Node(value=leaf_value, gini=current_gini, num_samples=n_samples)\n",
    "        \n",
    "        left_indices_bool = best_split_info['left_indices_bool']\n",
    "        right_indices_bool = best_split_info['right_indices_bool']\n",
    "\n",
    "        # min_samples_leafを下回る場合は葉ノードを返す\n",
    "        if(\n",
    "            np.sum(left_indices_bool) < self.min_samples_leaf or\n",
    "            np.sum(right_indices_bool) < self.min_samples_leaf\n",
    "        ):\n",
    "            leaf_value = self._majority_vote(y_data)\n",
    "            return Node(value=leaf_value, gini=current_gini, num_samples=n_samples)\n",
    "        \n",
    "        # 再帰的に左の子ノードを構築\n",
    "        left_node = self._build_tree(\n",
    "            X_data[left_indices_bool],\n",
    "            y_data[left_indices_bool],\n",
    "            depth + 1\n",
    "        )\n",
    "\n",
    "        # 再帰的に右の子ノードを構築\n",
    "        right_node = self._build_tree(\n",
    "            X_data[right_indices_bool],\n",
    "            y_data[right_indices_bool],\n",
    "            depth + 1\n",
    "        )\n",
    "\n",
    "        node_params = {\n",
    "            'feature_index': best_split_info['feature_index'],\n",
    "            'feature_type': best_split_info['type'],\n",
    "            'left_child': left_node,\n",
    "            'right_child': right_node,\n",
    "            'gini': current_gini,\n",
    "            'num_samples': n_samples\n",
    "        }\n",
    "\n",
    "        if best_split_info['type'] == 'numeric':\n",
    "            node_params['threshold'] = best_split_info['threshold']\n",
    "        else:\n",
    "            node_params['category'] = best_split_info['category']\n",
    "\n",
    "        return Node(**node_params)\n",
    "    \n",
    "    def fit(self, X_train, y_train, feature_types_list):\n",
    "        self.feature_types = feature_types_list\n",
    "        self.root = self._build_tree(X_train, y_train)\n",
    "        print(\"決定木が構築されました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "97c7a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定木が構築されました。\n",
      "エラーが発生しませんでした。決定木モデルが正常に構築されました。\n"
     ]
    }
   ],
   "source": [
    "tree_model_test = ClassificationTree(max_depth=2)\n",
    "tree_model_test.fit(X_sample, y_sample, feature_types_sample)\n",
    "print(\"エラーが発生しませんでした。決定木モデルが正常に構築されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b64c5",
   "metadata": {},
   "source": [
    "## 5. 予測処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef81bda",
   "metadata": {},
   "source": [
    "*   新しいデータサンプルが与えられたとき、構築された決定木の根ノードから葉ノードまでをたどり、予測クラスを見つけます。\n",
    "*   **処理の流れ**:\n",
    "    1.  現在のノードが葉ノードなら、そのノードの予測値を返します。\n",
    "    2.  現在のノードが中間ノードなら、そのノードの分割条件（特徴量と閾値/カテゴリ）を使って、データサンプルが左の子ノードに進むべきか、右の子ノードに進むべきかを判断します。\n",
    "    3.  選ばれた子ノードに対して、`_traverse_tree` を再帰的に呼び出します。\n",
    "\n",
    "*   **ステップ 5.2: `_traverse_tree` および `predict` メソッドの実装**\n",
    "    *   **目的**: `_traverse_tree` は単一サンプルの予測を行い、`predict` は複数のサンプルに対して予測を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "870a2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassficationTree(ClassificationTree): # 継承\n",
    "    def _traverse_tree(self, X_sample, node): # X_sampleは1D Numpy\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        \n",
    "        feature_val = X_sample[node.feature_index]\n",
    "\n",
    "        if node.feature_type == 'numeric':\n",
    "            if feature_val <= node.threshold:\n",
    "                return self._traverse_tree(X_sample, node.left_child)\n",
    "            else:\n",
    "                return self._traverse_tree(X_sample, node.right_child)\n",
    "        elif node.feature_type == 'categorical':\n",
    "            if feature_val == node.category:\n",
    "                return self._traverse_tree(X_sample, node.left_child)\n",
    "            else:\n",
    "                return self._traverse_tree(X_sample, node.right_child)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown feature type: {}\".format(node.feature_type))\n",
    "        \n",
    "    def predict(self, X_test): # X_testは2D Numpy\n",
    "        if self.root is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            row_sample = X_test[i, :]\n",
    "            predictions.append(self._traverse_tree(row_sample, self.root))\n",
    "        return np.array(predictions)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8380d8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定木が構築されました。\n",
      "\n",
      "サンプルデータの予測結果:\n",
      "[0 1 0 1 0 1 0 0 1 1]\n",
      "オリジナルのクラスラベル:\n",
      "[0 1 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "tree_model_test_for_predict = ClassficationTree(max_depth=3)\n",
    "tree_model_test_for_predict.fit(X_sample, y_sample, feature_types_sample)\n",
    "\n",
    "if tree_model_test_for_predict.root:\n",
    "    sample_prediction = tree_model_test_for_predict.predict(X_sample)\n",
    "    print(\"\\nサンプルデータの予測結果:\")\n",
    "    print(sample_prediction)\n",
    "    print(\"オリジナルのクラスラベル:\")\n",
    "    print(y_sample)\n",
    "else:\n",
    "    print(\"\\n決定木モデルが正しく構築されていません。予測を行うことができません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af3d7e",
   "metadata": {},
   "source": [
    "## 6. ツリーの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f346e",
   "metadata": {},
   "source": [
    "*   **ステップ 6.1: ツリー表示関数の概念説明**\n",
    "    *   構築された決定木を視覚的に確認できると、モデルの理解が深まります。\n",
    "    *   ここでは、テキストベースで木の構造を簡易的に表示する関数を作成します。\n",
    "*   **ステップ 6.2: `print_tree` 関数の実装**\n",
    "    *   **目的**: 構築されたツリーを再帰的にたどり、各ノードの情報を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a659568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(\n",
    "        node,\n",
    "        depth=0,\n",
    "        feature_names=None,\n",
    "        class_names=None,\n",
    "        category_names=None,\n",
    "        indent_char='  '\n",
    "):\n",
    "    if node is None:\n",
    "        print(f'{indent_char * depth}Empty node')\n",
    "        return\n",
    "    \n",
    "    indent = indent_char * depth\n",
    "\n",
    "    if node.is_leaf_node():\n",
    "        class_name = str(node.value)\n",
    "        if class_names and node.value in class_names:\n",
    "            class_name = class_names[node.value]\n",
    "        print(f'{indent}Leaf: {class_name} (Gini: {node.gini:.4f}, Samples: {node.num_samples})')\n",
    "        return\n",
    "    \n",
    "    feature_name = f'Feature_{node.feature_index}'\n",
    "    if feature_names and node.feature_index < len(feature_names):\n",
    "        feature_name = feature_names[node.feature_index]\n",
    "\n",
    "    if node.feature_type == 'numeric':\n",
    "        condition = f'{feature_name} <= {node.threshold:.4f}'\n",
    "        condition_else = f'{feature_name} > {node.threshold:.4f}'\n",
    "    else:\n",
    "        cat_code = node.category\n",
    "        cat_display_name = str(cat_code)\n",
    "        if category_names and node.feature_index in category_names and cat_code in category_names[node.feature_index]:\n",
    "            cat_display_name = category_names[node.feature_index][cat_code]\n",
    "\n",
    "        condition = f'{feature_name} == {cat_display_name}'\n",
    "        condition_else = f'{feature_name} != {cat_display_name}'\n",
    "\n",
    "    print(f'{indent}If {condition} (Gini: {node.gini:.4f}, Samples: {node.num_samples})')\n",
    "    print_tree(node.left_child, depth + 1, feature_names, class_names, category_names, indent_char)\n",
    "    print(f'{indent}Else {condition_else}')\n",
    "    print_tree(node.right_child, depth + 1, feature_names, class_names, category_names, indent_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b38f37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "決定木の構造:\n",
      "If Categorical Feature == A (Gini: 0.5000, Samples: 10)\n",
      "  Leaf: Class A (Gini: 0.0000, Samples: 4)\n",
      "Else Categorical Feature != A\n",
      "  If Numeric Feature <= 5.5000 (Gini: 0.2778, Samples: 6)\n",
      "    If Numeric Feature <= 4.5000 (Gini: 0.4444, Samples: 3)\n",
      "      Leaf: Class B (Gini: 0.0000, Samples: 2)\n",
      "    Else Numeric Feature > 4.5000\n",
      "      Leaf: Class A (Gini: 0.0000, Samples: 1)\n",
      "  Else Numeric Feature > 5.5000\n",
      "    Leaf: Class B (Gini: 0.0000, Samples: 3)\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "print(\"\\n決定木の構造:\")\n",
    "if tree_model_test_for_predict.root:\n",
    "    sample_feature_names = ['Numeric Feature', 'Categorical Feature']\n",
    "    sample_class_names = {0: 'Class A', 1: 'Class B'}\n",
    "    sample_category_map = {\n",
    "        1: category_mapping_sample\n",
    "    }\n",
    "\n",
    "    print_tree(tree_model_test_for_predict.root,\n",
    "                    feature_names=sample_feature_names,\n",
    "                    class_names=sample_class_names,\n",
    "                    category_names=sample_category_map)\n",
    "else:\n",
    "    print(\"決定木モデルが正しく構築されていません。ツリーの表示ができません。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849c35b",
   "metadata": {},
   "source": [
    "### 7. モデルの学習と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a5b2bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "決定木が構築されました。\n",
      "\n",
      "Irisデータセットの決定木の構造:\n",
      "If petal length (cm) <= 2.5000 (Gini: 0.6667, Samples: 75)\n",
      "  Leaf: setosa (Gini: 0.0000, Samples: 25)\n",
      "Else petal length (cm) > 2.5000\n",
      "  If petal width (cm) <= 1.6500 (Gini: 0.5000, Samples: 50)\n",
      "    Leaf: versicolor (Gini: 0.0740, Samples: 26)\n",
      "  Else petal width (cm) > 1.6500\n",
      "    Leaf: virginica (Gini: 0.0000, Samples: 24)\n",
      "\n",
      "テストデータの予測精度: 0.9333333333333333\n",
      "学習データの予測精度: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "iris_feature_names = iris.feature_names\n",
    "\n",
    "# IrisデータセットはすべてNumeric\n",
    "feature_types_iris = ['numeric'] * X_iris.shape[1]\n",
    "\n",
    "# データセットの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.5, random_state=0, stratify=y_iris\n",
    ")\n",
    "\n",
    "# モデルのインスタンス化と学習\n",
    "iris_tree_model = ClassficationTree(max_depth=3, min_samples_split=5, min_samples_leaf=2)\n",
    "iris_tree_model.fit(X_train, y_train, feature_types_iris)\n",
    "\n",
    "# ツリーの表示\n",
    "print(\"\\nIrisデータセットの決定木の構造:\")\n",
    "iris_class = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "print_tree(\n",
    "    iris_tree_model.root,\n",
    "    feature_names=iris_feature_names,\n",
    "    class_names=iris_class,\n",
    ")\n",
    "\n",
    "# 予測と精度の評価\n",
    "if iris_tree_model.root:\n",
    "    test_pred = iris_tree_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    print(\"\\nテストデータの予測精度:\", test_accuracy)\n",
    "\n",
    "    train_pred = iris_tree_model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    print(\"学習データの予測精度:\", train_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
