{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c8fa61",
   "metadata": {},
   "source": [
    "# ランダムフォレストの実装\n",
    "\n",
    "このノートブックでは、Leo Breiman氏の論文「Random Forests」のアイデアに基づき、NumPyとPython標準ライブラリのみを使用してランダムフォレスト分類器をスクラッチから実装します。\n",
    "\n",
    "**主なステップ:**\n",
    "1. 必要なライブラリと以前に作成した決定木クラスのインポート\n",
    "2. データセットの準備\n",
    "3. ランダムフォレストのアルゴリズム概要\n",
    "4. ブートストラップサンプリングの実装\n",
    "5. RandomForestClassifierクラスの実装\n",
    "    - 個々の木の学習 (特徴量のランダム選択を含む)\n",
    "    - 予測 (多数決)\n",
    "6. モデルの学習と評価`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162a770",
   "metadata": {},
   "source": [
    "## 1. ライブラリと決定木クラスのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys, os\n",
    "\n",
    "# 作成した決定木クラス\n",
    "sys.path.append(os.path.abspath('../src/Tree'))\n",
    "from decision_tree import DecisionTreeClassification, Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fb814",
   "metadata": {},
   "source": [
    "## 2. データセットの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883403c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルデータ X_sample (shape): (20, 2)\n",
      "サンプルラベル y_sample (shape): (20,)\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータ\n",
    "# 特徴量: X_sample\n",
    "# 列0: 数値特徴量\n",
    "# 列1: カテゴリ特徴量 (0: 'A', 1: 'B', 2: 'C' と仮定)\n",
    "X_sample = np.array([\n",
    "    [1, 0], [2, 1], [3, 0], [4, 2], [5, 1],\n",
    "    [6, 2], [7, 0], [8, 0], [9, 1], [10, 2],\n",
    "    [1.5, 0], [2.5, 1], [3.5, 0], [4.5, 2], [5.5, 1],\n",
    "    [6.5, 2], [7.5, 0], [8.5, 0], [9.5, 1], [10.5, 2]\n",
    "])\n",
    "# クラスラベル: y_sample\n",
    "y_sample = np.array([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
    "\n",
    "# 特徴量のタイプ\n",
    "feature_types_sample = ['numeric', 'categorical']\n",
    "# カテゴリ特徴量のマッピング (表示用)\n",
    "category_mapping_sample = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "print(\"サンプルデータ X_sample (shape):\", X_sample.shape)\n",
    "print(\"サンプルラベル y_sample (shape):\", y_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af570a8",
   "metadata": {},
   "source": [
    "## 3. ランダムフォレストのアルゴリズム概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3998ee",
   "metadata": {},
   "source": [
    "Breiman (2001) の論文で説明されているランダムフォレストの基本的なアルゴリズムは以下の通りです。\n",
    "\n",
    "1.  **N個の決定木を構築する (Nはハイパーパラメータ `n_estimators`)。**  \n",
    "    各木を構築するために：  \n",
    "    a.  元の訓練データから、**ブートストラップサンプル**（復元抽出による同じサイズのサブサンプル）を作成する。  \n",
    "    b.  このブートストラップサンプルを使って決定木を成長させる。各ノードで分割を行う際：  \n",
    "           i.   全特徴量の中から、**ランダムにm個の特徴量を選択**する（mはハイパーパラメータ `max_features`）。  \n",
    "           ii.  選択されたm個の特徴量の中から、最適な分割（例：Gini不純度を最小化する分割）を見つける。  \n",
    "    c.  個々の木は、**枝刈りせずに**最大限に成長させる（または、`max_depth`, `min_samples_leaf`などの停止条件まで）。  \n",
    "\n",
    "2.  **予測を行う。**  \n",
    "    *   **分類の場合**: 新しいデータポイントに対して、フォレスト内の各木に予測させる。最終的な予測は、各木の予測の**多数決**によって決定する。  \n",
    "    *   **回帰の場合**: 各木の予測の**平均値**を最終的な予測とする。\n",
    "\n",
    "**ランダムフォレストの主な利点:**  \n",
    "*   高い予測精度。  \n",
    "*   過学習しにくい（個々の木は過学習する可能性があるが、アンサンブルすることで緩和される）。  \n",
    "*   特徴量の重要度を評価できる。  \n",
    "*   大規模なデータセットや高次元データにも対応可能。  \n",
    "*   並列処理が容易。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73ce2c",
   "metadata": {},
   "source": [
    "## 4. ブートストラップサンプリングの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8efd6a",
   "metadata": {},
   "source": [
    "\n",
    "*   **ステップ 4.1: ブートストラップサンプリングの概念説明**\n",
    "    *   ランダムフォレストでは、各決定木を学習させるために、元の訓練データから少しずつ異なるデータセットを作成します。この手法の一つがブートストラップサンプリングです。\n",
    "    *   ブートストラップサンプリングとは、元のデータセット（サンプル数N）から、**復元抽出**（一度選んだサンプルを元に戻してから再度選ぶ）によってN個のサンプルを選び出し、新しいデータセットを作成する手法です。\n",
    "    *   この操作を繰り返すことで、複数の異なる（しかし元のデータに基づいた）訓練サブセットが得られます。各決定木は、これらの異なるサブセットで学習されるため、木々の間に多様性が生まれます。これがランダムフォレストの過学習抑制と汎化性能向上に寄与します。\n",
    "    *   平均して、ブートストラップサンプルには元のデータの約63.2%のユニークなサンプルが含まれ、残りの約36.8%のサンプルは含まれません（これらはOut-of-Bagサンプルと呼ばれ、極限を計算することで簡単に求められます。）\n",
    "\n",
    "*   **ステップ 4.2: `bootstrap_sample` 関数の実装**\n",
    "    *   **目的**: 与えられた特徴量データ `X` とラベルデータ `y` から、1つのブートストラップサンプルを生成します。\n",
    "    *   **処理の流れ**:\n",
    "        1.  元のデータセットのサンプル数を取得します。\n",
    "        2.  `np.random.choice` を使用して、0から (サンプル数-1) までのインデックスを、サンプル数と同じ個数だけ**復元抽出**します (`replace=True`)。\n",
    "        3.  得られたインデックスを使って、元の `X` と `y` から新しいデータセット `X_bootstrap` と `y_bootstrap` を作成して返します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37076763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrap_sample(X,y) -> tuple:\n",
    "    '''\n",
    "    ブートストラップサンプリングを行う関数\n",
    "    Parameters:\n",
    "        X (np.ndarray): 特徴量データ\n",
    "        y (np.ndarray): クラスラベル\n",
    "    Returns:\n",
    "        tuple: ブートストラップサンプルの特徴量とラベル\n",
    "    '''\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    # 復元抽出でインデックスを選択\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d547c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元のデータ X_sample (shape): (20, 2)\n",
      "ブートストラップサンプル X_bs_test (shape): (20, 2)\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "X_bs_test, y_bs_test = boostrap_sample(X_sample, y_sample)\n",
    "print(\"元のデータ X_sample (shape):\", X_sample.shape)\n",
    "print(\"ブートストラップサンプル X_bs_test (shape):\", X_bs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61fd29",
   "metadata": {},
   "source": [
    "## 5. RandomForestClassifierクラスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed3e57",
   "metadata": {},
   "source": [
    "*   **ステップ 5.1: `RandomForestClassifier` クラスの概念説明**\n",
    "    *   このクラスは、ランダムフォレスト分類器全体を管理します。\n",
    "    *   複数の決定木 (`DecisionTree` クラスのインスタンス) を保持し、それらの学習と予測の取りまとめを行います。\n",
    "    *   **主な機能**:\n",
    "        *   `__init__`: ハイパーパラメータ（木の数、各木の深さ制限など）を初期化します。\n",
    "        *   `fit`: 訓練データを受け取り、指定された数の決定木をブートストラップサンプルと特徴量のランダム選択を用いて学習させます。\n",
    "        *   `predict`: 新しいデータに対して、フォレスト内の全ての木に予測を行わせ、その結果を多数決で集約して最終的な予測クラスを返します。\n",
    "        *   `predict_proba`: 各クラスに属する確率を予測します（各木の投票の割合）。\n",
    "\n",
    "*   **ステップ 5.2: `RandomForestClassifier` クラスの `__init__` メソッドの実装**\n",
    "    *   **目的**: ランダムフォレストのハイパーパラメータを初期化し、学習済みモデル（決定木のリストなど）を格納するための変数を準備します。\n",
    "    *   **引数**:\n",
    "        *   `n_estimators`: フォレストを構成する決定木の数。\n",
    "        *   `max_depth`: 個々の決定木の最大深さ。`None` の場合は制限なし。\n",
    "        *   `min_samples_split`: ノードを分割するために必要な最小サンプル数。\n",
    "        *   `min_samples_leaf`: 葉ノードを形成するために必要な最小サンプル数。\n",
    "        *   `max_features`: 各ノードで分割を検討する際にランダムに選択する特徴量の数。文字列（'sqrt', 'log2'）、整数、または浮動小数点数（割合）で指定可能。\n",
    "        *   `random_state`: 乱数生成のシード。再現性を確保するために使用。\n",
    "    *   **初期化する主なインスタンス変数**:\n",
    "        *   `self.trees`: 学習済みの個々の決定木を格納するリスト。\n",
    "        *   `self.feature_types`: `fit` メソッドで渡される特徴量の型情報。\n",
    "        *   `self.classes_`: `fit` メソッドで学習データから取得するユニークなクラスラベルのリスト。`predict_proba` などで使用。\n",
    "\n",
    "*   **ステップ 5.3: `RandomForestClassifier` クラスの `fit` メソッドの実装**\n",
    "    *   **目的**: 訓練データ `X_train`, `y_train` と特徴量の型情報 `feature_types_list` を用いて、ランダムフォレストを構成する複数の決定木を学習させます。\n",
    "    *   **処理の流れ**:\n",
    "        1.  `random_state` が設定されていれば、乱数シードを固定します。\n",
    "        2.  学習済みの木を格納する `self.trees` リストを初期化します。\n",
    "        3.  訓練データからユニークなクラスラベルを取得し、`self.classes_` に保存します。\n",
    "        4.  指定された `self.n_estimators` の数だけ以下のループを実行します:\n",
    "            a.  `bootstrap_sample` 関数を呼び出して、現在の訓練データからブートストラップサンプル `X_bootstrap`, `y_bootstrap` を作成します。\n",
    "            b.  `DecisionTree` クラスのインスタンス（個々の木）を、`max_depth` や `max_features` などのハイパーパラメータと共に初期化します。**特に `max_features` は、個々の木が分割時に特徴量をランダムに選択するために重要です。**\n",
    "            c.  作成したブートストラップサンプル `X_bootstrap`, `y_bootstrap` と特徴量の型情報 `self.feature_types` を使って、個々の決定木を `fit` させます。\n",
    "            d.  学習済みの木を `self.trees` リストに追加します。\n",
    "        5.  学習の進捗を表示します（オプション）。\n",
    "\n",
    "*   **ステップ 5.4: `RandomForestClassifier` クラスの `_predict_tree_outputs` ヘルパーメソッドの実装**\n",
    "    *   **目的**: `predict` や `predict_proba` で共通して使用される、フォレスト内の各木からの予測結果を効率的に収集します。\n",
    "    *   **処理の流れ**:\n",
    "        1.  フォレストが学習済みか（`self.trees` が空でないか）を確認します。\n",
    "        2.  `self.trees` リスト内の各決定木インスタンスに対して、入力されたテストデータ `X_test` の予測を `predict_single_tree` メソッド（`DecisionTree`クラスのメソッド）を呼び出して取得します。\n",
    "        3.  得られた各木の予測結果をNumPy配列にまとめ、転置して返します。結果の配列の形状は `(n_samples, n_estimators)` となり、各行が1つのサンプルに対する全ツリーの予測、各列が1つのツリーによる全サンプルの予測を表します。\n",
    "\n",
    "*   **ステップ 5.5: `RandomForestClassifier` クラスの `predict` メソッドの実装**\n",
    "    *   **目的**: 新しいデータ `X_test` に対して、ランダムフォレストによる最終的なクラス予測を行います。分類タスクでは、これは通常、各木の予測の多数決によって決定されます。\n",
    "    *   **処理の流れ**:\n",
    "        1.  `_predict_tree_outputs` メソッドを呼び出して、`X_test` の各サンプルに対する全ツリーの予測を取得します（形状: `(n_samples, n_estimators)`）。\n",
    "        2.  得られた予測結果の各行（各サンプル）に対して、最も多く出現したクラス（多数決）を計算します。`collections.Counter` を使うと効率的です。\n",
    "        3.  各サンプルの多数決結果をリストに集め、NumPy配列として返します。\n",
    "\n",
    "*   **ステップ 5.6: `RandomForestClassifier` クラスの `predict_proba` メソッドの実装**\n",
    "    *   **目的**: 新しいデータ `X_test` に対して、各クラスに属する確率を予測します。これは、フォレスト内の各木がそのクラスに投票した割合として計算されます。\n",
    "    *   **処理の流れ**:\n",
    "        1.  `_predict_tree_outputs` メソッドを呼び出して、`X_test` の各サンプルに対する全ツリーの予測を取得します。\n",
    "        2.  `fit` 時に保存した `self.classes_` を参照して、クラスの数と各クラスラベルに対応する出力配列のインデックスを決定します。\n",
    "        3.  各サンプルについて、全ツリーの予測の中で各クラスが何回出現したかをカウントします。\n",
    "        4.  各クラスの出現回数を木の総数 `self.n_estimators` で割ることで、そのクラスに属する確率を計算します。\n",
    "        5.  結果を形状 `(n_samples, n_classes)` のNumPy配列として返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d69953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_estimators=10,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            random_state=None\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.trees = []\n",
    "        self.feature_types = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X_train, y_train, feature_types_ls):\n",
    "        '''\n",
    "        ランダムフォレストの学習を行うメソッド\n",
    "        Parameters:\n",
    "            X_train (np.ndarray): 学習用特徴量データ\n",
    "            y_train (np.ndarray): 学習用クラスラベル\n",
    "            feature_types_ls (list): 特徴量のタイプリスト\n",
    "        '''\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            random.seed(self.random_state)\n",
    "\n",
    "        self.trees = []\n",
    "        self.feature_types = feature_types_ls\n",
    "        self.classes = np.unique(y_train)\n",
    "        n_samples, n_features = X_train.shape\n",
    "\n",
    "        print(\"ランダムフォレストの学習を開始します...\")\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # 1. ブートストラップサンプリング\n",
    "            X_bs, y_bs = boostrap_sample(X_train, y_train)\n",
    "\n",
    "            # 2. 個々の決定木を初期化して学習\n",
    "            tree = DecisionTreeClassification(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features,\n",
    "            )\n",
    "\n",
    "            tree.fit(X_bs, y_bs, self.feature_types)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # 3. 学習の進捗を表示\n",
    "            progress_interval = self.n_estimators // 10 if self.n_estimators >= 10 else 1\n",
    "\n",
    "            if (i + 1) % progress_interval == 0 or i == self.n_estimators - 1:\n",
    "                print(f\"決定木 {i + 1}/{self.n_estimators} の学習が完了しました。\")\n",
    "            \n",
    "        print(\"ランダムフォレストの学習が完了しました。\")\n",
    "\n",
    "    def predict_tree_outputs(self, X_test):\n",
    "        '''\n",
    "        フォレスト内の各木からの予測を取得するヘルパー関数。\n",
    "        Returns:\n",
    "            np.array: 各サンプルに対する各木の予測 (n_samples, n_estimators)\n",
    "        '''\n",
    "        \n",
    "        if not self.trees:\n",
    "            raise ValueError(\"モデルが学習されていません。`fit`メソッドを呼び出してください。\")\n",
    "        \n",
    "        # DecisionTreeクラスのpredictがpredict_single_treeメソッドに変更されているので注意\n",
    "        tree_predictions = np.array([tree.predict_single_tree(X_test) for tree in self.trees])\n",
    "\n",
    "        # (n_estimators, n_samples) の形状にするために転地\n",
    "        return tree_predictions.T\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        多数決で予測を行うメソッド\n",
    "        '''\n",
    "        tree_preds = self.predict_tree_outputs(X_test) # (n_samples, n_estimators)\n",
    "\n",
    "        forest_preds = []\n",
    "\n",
    "        for sample_preds in tree_preds: # sample_predsは各サンプルに対する全ての木の予測\n",
    "            counts = Counter(sample_preds)\n",
    "            majority_vote = counts.most_common(1)[0][0]  # 最も頻出のクラスを取得\n",
    "            forest_preds.append(majority_vote)\n",
    "\n",
    "        return np.array(forest_preds)\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        '''\n",
    "        新しいデータポイントの各クラスの確率を予測するメソッド\n",
    "        '''\n",
    "\n",
    "        tree_preds = self.predict_tree_outputs(X_test) # (n_samples, n_estimators)\n",
    "        \n",
    "        n_samples = X_test.shape[0]\n",
    "        if self.classes is None:\n",
    "            raise ValueError(\"モデルが学習されていません。`fit`メソッドを呼び出してください。\")\n",
    "        num_classes = len(self.classes)\n",
    "        \n",
    "        # self.classesはfit時にソートされているので\n",
    "        # self.classesを使って，出力確率配列の列インデックスとクラスラベルを対応させる\n",
    "        class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "        # 確率を格納する配列\n",
    "        proba = np.zeros((n_samples, num_classes))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            sample_tree_preds = tree_preds[i, :] # i番目のサンプルに対する全ての木の予測\n",
    "            counts = Counter(sample_tree_preds)\n",
    "\n",
    "            for class_label, count in counts.items():\n",
    "                # 予測されたクラスラベルがself.classesに存在する場合のみ確率を更新\n",
    "                if class_label in class_to_index:\n",
    "                    proba[i, class_to_index[class_label]] = count / self.n_estimators\n",
    "\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398742f",
   "metadata": {},
   "source": [
    "## 6. モデルの学習と評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cd605",
   "metadata": {},
   "source": [
    "### 6.1 サンプルデータでのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a15265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ランダムフォレストの学習を開始します...\n",
      "決定木 1/10 の学習が完了しました。\n",
      "決定木 2/10 の学習が完了しました。\n",
      "決定木 3/10 の学習が完了しました。\n",
      "決定木 4/10 の学習が完了しました。\n",
      "決定木 5/10 の学習が完了しました。\n",
      "決定木 6/10 の学習が完了しました。\n",
      "決定木 7/10 の学習が完了しました。\n",
      "決定木 8/10 の学習が完了しました。\n",
      "決定木 9/10 の学習が完了しました。\n",
      "決定木 10/10 の学習が完了しました。\n",
      "ランダムフォレストの学習が完了しました。\n",
      "\n",
      "Sample Data - Random Forest Predictions: [0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1]\n",
      "Sample Data - True Labels:           [0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1]\n",
      "Sample Data - Random Forest Accuracy: 0.9000\n",
      "\n",
      "Sample Data - Random Forest Probabilities (first 5 samples):\n",
      "[[0.9 0.1]\n",
      " [0.3 0.7]\n",
      " [1.  0. ]\n",
      " [0.1 0.9]\n",
      " [0.4 0.6]]\n"
     ]
    }
   ],
   "source": [
    "rf_classifier_sample = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_classifier_sample.fit(X_sample, y_sample, feature_types_sample)\n",
    "\n",
    "predictions_sample = rf_classifier_sample.predict(X_sample)\n",
    "print(\"\\nSample Data - Random Forest Predictions:\", predictions_sample)\n",
    "print(\"Sample Data - True Labels:          \", y_sample)\n",
    "\n",
    "accuracy_sample = np.sum(predictions_sample == y_sample) / len(y_sample)\n",
    "print(f\"Sample Data - Random Forest Accuracy: {accuracy_sample:.4f}\")\n",
    "\n",
    "proba_sample = rf_classifier_sample.predict_proba(X_sample)\n",
    "print(\"\\nSample Data - Random Forest Probabilities (first 5 samples):\")\n",
    "print(proba_sample[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea0da1",
   "metadata": {},
   "source": [
    "### 6.2 Irisデータセットでのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb6063ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Training data shape: (105, 4)\n",
      "Iris Test data shape: (45, 4)\n",
      "ランダムフォレストの学習を開始します...\n",
      "決定木 5/50 の学習が完了しました。\n",
      "決定木 10/50 の学習が完了しました。\n",
      "決定木 15/50 の学習が完了しました。\n",
      "決定木 20/50 の学習が完了しました。\n",
      "決定木 25/50 の学習が完了しました。\n",
      "決定木 30/50 の学習が完了しました。\n",
      "決定木 35/50 の学習が完了しました。\n",
      "決定木 40/50 の学習が完了しました。\n",
      "決定木 45/50 の学習が完了しました。\n",
      "決定木 50/50 の学習が完了しました。\n",
      "ランダムフォレストの学習が完了しました。\n",
      "\n",
      "Iris Test Data - Random Forest Accuracy: 0.9556\n",
      "Iris Training Data - Random Forest Accuracy: 1.0000\n",
      "\n",
      "Iris Test Data - Random Forest Probabilities (first 5 samples):\n",
      "Sample 0: True=versicolor, Predicted Probs=[0.   0.44 0.56]\n",
      "Sample 1: True=virginica, Predicted Probs=[0.   0.06 0.94]\n",
      "Sample 2: True=versicolor, Predicted Probs=[0. 1. 0.]\n",
      "Sample 3: True=versicolor, Predicted Probs=[0.   0.96 0.04]\n",
      "Sample 4: True=virginica, Predicted Probs=[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "iris_feature_names = iris.feature_names\n",
    "feature_types_iris = ['numeric'] * X_iris.shape[1]\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=123, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(\"Iris Training data shape:\", X_train_iris.shape)\n",
    "print(\"Iris Test data shape:\", X_test_iris.shape)\n",
    "\n",
    "rf_classifier_iris = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "rf_classifier_iris.fit(X_train_iris, y_train_iris, feature_types_iris)\n",
    "\n",
    "predictions_iris_test = rf_classifier_iris.predict(X_test_iris)\n",
    "accuracy_iris_test = accuracy_score(y_test_iris, predictions_iris_test)\n",
    "print(f\"\\nIris Test Data - Random Forest Accuracy: {accuracy_iris_test:.4f}\")\n",
    "\n",
    "predictions_iris_train = rf_classifier_iris.predict(X_train_iris)\n",
    "accuracy_iris_train = accuracy_score(y_train_iris, predictions_iris_train)\n",
    "print(f\"Iris Training Data - Random Forest Accuracy: {accuracy_iris_train:.4f}\")\n",
    "\n",
    "proba_iris_test = rf_classifier_iris.predict_proba(X_test_iris)\n",
    "print(\"\\nIris Test Data - Random Forest Probabilities (first 5 samples):\")\n",
    "for i in range(5):\n",
    "    # iris.target_names は ['setosa', 'versicolor', 'virginica']\n",
    "    # y_test_iris[i] は 0, 1, or 2\n",
    "    print(f\"Sample {i}: True={iris.target_names[y_test_iris[i]]}, Predicted Probs={proba_iris_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6c981",
   "metadata": {},
   "source": [
    "### 6.3 Covertypeデータセットでのテスト (大規模データセットの例)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb115de",
   "metadata": {},
   "source": [
    "\n",
    "次に、より大規模なデータセットであるCovertypeデータセットでランダムフォレストの動作を確認します。\n",
    "このデータセットはサンプル数が多く、特徴量も多いため、モデルの性能と学習時間を確認するのに適しています。\n",
    "注意: `fetch_covtype`は初回実行時にデータのダウンロードが発生し、時間がかかることがあります。また、全データを使用すると学習に非常に時間がかかるため、ここではデータの一部（例えば最初の50,000サンプル）を使用します。クラスラベルは1から7ですが、0から始まるように調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a60b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Covertype dataset... (This may take a while)\n",
      "Covertype dataset loaded.\n",
      "Full data shape X: (581012, 54)\n",
      "Full data shape y: (581012,)\n",
      "Unique classes: [0 1 2 3 4 5 6]\n",
      "Using a subset of 50000 samples for Covertype test.\n",
      "Covertype Training data subset shape: (35000, 54)\n",
      "Covertype Test data subset shape: (15000, 54)\n",
      "\n",
      "Starting Covertype Random Forest training...\n",
      "ランダムフォレストの学習を開始します...\n",
      "決定木 3/30 の学習が完了しました。\n",
      "決定木 6/30 の学習が完了しました。\n",
      "決定木 9/30 の学習が完了しました。\n",
      "決定木 12/30 の学習が完了しました。\n",
      "決定木 15/30 の学習が完了しました。\n",
      "決定木 18/30 の学習が完了しました。\n",
      "決定木 21/30 の学習が完了しました。\n",
      "決定木 24/30 の学習が完了しました。\n",
      "決定木 27/30 の学習が完了しました。\n",
      "決定木 30/30 の学習が完了しました。\n",
      "ランダムフォレストの学習が完了しました。\n",
      "Covertype Random Forest training finished in 1475.72 seconds.\n",
      "\n",
      "Predicting on Covertype test data...\n",
      "Prediction finished in 2.01 seconds.\n",
      "\n",
      "Covertype Test Data - Random Forest Accuracy: 0.7495\n",
      "\n",
      "Predicting on Covertype training data (for comparison)...\n",
      "Covertype Training Data - Random Forest Accuracy: 0.7710\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time # 学習時間の計測用\n",
    "\n",
    "# Covertypeデータセットのロード\n",
    "print(\"Fetching Covertype dataset... (This may take a while)\")\n",
    "covtype = fetch_covtype()\n",
    "X_covtype_full = covtype.data\n",
    "y_covtype_full = covtype.target \n",
    "# クラスラベルは1から7なので、0から6に変換します\n",
    "y_covtype_full = y_covtype_full - 1 \n",
    "covtype_feature_names = [f\"feature_{i}\" for i in range(X_covtype_full.shape[1])] # 特徴量名 (仮)\n",
    "# Covertypeデータセットは全て数値特徴量 (一部バイナリ含む)\n",
    "feature_types_covtype = ['numeric'] * X_covtype_full.shape[1]\n",
    "\n",
    "print(\"Covertype dataset loaded.\")\n",
    "print(\"Full data shape X:\", X_covtype_full.shape) # (581012, 54)\n",
    "print(\"Full data shape y:\", y_covtype_full.shape) # (581012,)\n",
    "print(\"Unique classes:\", np.unique(y_covtype_full)) # 0 to 6\n",
    "\n",
    "# データが非常に大きいため、一部をサンプリングして使用\n",
    "n_samples_to_use = 50000 # 使用するサンプル数を調整可能\n",
    "if X_covtype_full.shape[0] > n_samples_to_use:\n",
    "    # サンプリングするインデックスをランダムに選択 (再現性のためシード固定)\n",
    "    # stratifyはここでは難しいので単純にランダムサンプリング\n",
    "    # もしくは、より良いサンプリング方法を検討 (例: train_test_splitで取得)\n",
    "    # ここでは簡単のため、最初のn_samples_to_useを使用\n",
    "    # random_indices = np.random.choice(X_covtype_full.shape[0], n_samples_to_use, replace=False)\n",
    "    # X_covtype = X_covtype_full[random_indices]\n",
    "    # y_covtype = y_covtype_full[random_indices]\n",
    "    \n",
    "    # より代表的なサンプルを得るために、層化サンプリングを試みる (元のデータが大きいので大変かも)\n",
    "    # 簡単のため、まず全体を分割し、その訓練データの一部を使う\n",
    "    _, X_covtype_subset, _, y_covtype_subset = train_test_split(\n",
    "        X_covtype_full, y_covtype_full, test_size=n_samples_to_use/X_covtype_full.shape[0] if n_samples_to_use < X_covtype_full.shape[0] else 0.1, \n",
    "        stratify=y_covtype_full, random_state=123\n",
    "    )\n",
    "    if n_samples_to_use < X_covtype_full.shape[0] :\n",
    "        X_covtype = X_covtype_subset\n",
    "        y_covtype = y_covtype_subset\n",
    "    else: # 全データ使う場合（非推奨、時間がかかる）\n",
    "        X_covtype = X_covtype_full\n",
    "        y_covtype = y_covtype_full\n",
    "\n",
    "    print(f\"Using a subset of {X_covtype.shape[0]} samples for Covertype test.\")\n",
    "else:\n",
    "    X_covtype = X_covtype_full\n",
    "    y_covtype = y_covtype_full\n",
    "    print(\"Using the full (small) Covertype dataset for test.\")\n",
    "\n",
    "\n",
    "# データを学習用とテスト用に分割\n",
    "X_train_cov, X_test_cov, y_train_cov, y_test_cov = train_test_split(\n",
    "    X_covtype, y_covtype, test_size=0.3, random_state=123, stratify=y_covtype\n",
    ")\n",
    "\n",
    "print(\"Covertype Training data subset shape:\", X_train_cov.shape)\n",
    "print(\"Covertype Test data subset shape:\", X_test_cov.shape)\n",
    "\n",
    "# Covertypeデータセット用ランダムフォレストモデル\n",
    "# 木の数や深さを調整して学習時間を考慮\n",
    "rf_classifier_covtype = RandomForestClassifier(\n",
    "    n_estimators=30,      # 木の数をIrisより少なめに (時間短縮のため)\n",
    "    max_depth=15,         # 深さも少し制限 (時間とメモリのため)\n",
    "    min_samples_leaf=5,   # 過学習を少し抑制\n",
    "    max_features='sqrt',  # sqrt(54) approx 7\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Covertype Random Forest training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 学習\n",
    "rf_classifier_covtype.fit(X_train_cov, y_train_cov, feature_types_covtype)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Covertype Random Forest training finished in {training_time:.2f} seconds.\")\n",
    "\n",
    "# テストデータで予測\n",
    "print(\"\\nPredicting on Covertype test data...\")\n",
    "start_pred_time = time.time()\n",
    "predictions_covtype_test = rf_classifier_covtype.predict(X_test_cov)\n",
    "end_pred_time = time.time()\n",
    "prediction_time = end_pred_time - start_pred_time\n",
    "print(f\"Prediction finished in {prediction_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# 精度評価\n",
    "accuracy_covtype_test = accuracy_score(y_test_cov, predictions_covtype_test)\n",
    "print(f\"\\nCovertype Test Data - Random Forest Accuracy: {accuracy_covtype_test:.4f}\")\n",
    "\n",
    "# (比較) 学習データでの精度\n",
    "print(\"\\nPredicting on Covertype training data (for comparison)...\")\n",
    "predictions_covtype_train = rf_classifier_covtype.predict(X_train_cov)\n",
    "accuracy_covtype_train = accuracy_score(y_train_cov, predictions_covtype_train)\n",
    "print(f\"Covertype Training Data - Random Forest Accuracy: {accuracy_covtype_train:.4f}\")\n",
    "\n",
    "# (おまけ) 確率予測 (最初の数サンプルのみ)\n",
    "# print(\"\\nCovertype Test Data - Random Forest Probabilities (first 3 samples):\")\n",
    "# if len(X_test_cov) > 0:\n",
    "#     proba_covtype_test = rf_classifier_covtype.predict_proba(X_test_cov[:3])\n",
    "#     covtype_target_names = [f\"Type {i+1}\" for i in range(len(np.unique(y_covtype_full)))] # 仮のクラス名\n",
    "#     for i in range(min(3, len(X_test_cov))):\n",
    "#         true_class_idx = y_test_cov[i]\n",
    "#         true_class_name = covtype_target_names[true_class_idx] if true_class_idx < len(covtype_target_names) else f\"Raw_{true_class_idx}\"\n",
    "#         print(f\"Sample {i}: True={true_class_name}, Predicted Probs={proba_covtype_test[i]}\")\n",
    "# else:\n",
    "#     print(\"Test set is empty, cannot show probabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23166fd",
   "metadata": {},
   "source": [
    "## 7. 考察 (論文との関連など)\n",
    "\n",
    "*   **実装したアルゴリズム**:\n",
    "    *   ブートストラップサンプリング（バギング）と特徴量のランダム選択を組み合わせたアンサンブル学習。\n",
    "    *   個々の決定木は、指定された `max_features` に基づいてランダムに選択された特徴量サブセットから最適な分割を選んで成長する。\n",
    "    *   予測は、分類の場合は多数決。\n",
    "*   **論文 (Breiman, 2001) との関連**:\n",
    "    *   論文の基本的なアイデアである「複数の木を成長させ、それぞれにランダム性を導入し、それらを組み合わせる」という方針に従っている。\n",
    "    *   論文で強調されている「個々の木は枝刈りしない（do not prune）」という点は、`max_depth=None` や `min_samples_leaf=1` などの設定で近似的に実現できる。\n",
    "    *   特徴量のランダム選択 (Random Subspace Method の一種) は、`max_features` パラメータによって制御される。論文では `F` という記号で、各ノードでランダムに選択する入力変数の数を表している (Section 4. Random forests using random input selection)。\n",
    "    *   Out-of-Bag (OOB) 推定による誤差評価や変数重要度の計算は、この基本的な実装には含まれていないが、論文では重要な要素として議論されている (Section 3.1, Section 10)。\n",
    "*   **改善点や今後の課題**:\n",
    "    *   **Out-of-Bag (OOB) 誤差推定の実装**: 各木を学習する際に使用されなかったサンプル（OOBサンプル）を使って、モデルの汎化性能を評価する。\n",
    "    *   **特徴量重要度の計算**: OOBサンプルを使って、各特徴量が予測精度にどれだけ貢献しているかを評価する。\n",
    "    *   **並列処理**: 個々の木の学習は独立して行えるため、並列化することで学習時間を大幅に短縮できる。\n",
    "    *   **回帰タスクへの対応**: 現在は分類のみだが、葉ノードの値を平均値にし、アンサンブルの予測も平均値にすることで回帰に対応可能。\n",
    "    *   **`predict_proba` の堅牢性向上**: `fit` 時にクラスラベルの情報を保存し（`self.classes_`として実装済み）、それを利用して確率計算の際のクラス数を正確に把握するようにした。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
