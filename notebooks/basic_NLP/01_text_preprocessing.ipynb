{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bba9e8",
   "metadata": {},
   "source": [
    "# NLP基礎 (1): テキスト前処理、Bag-of-Words (BoW)、TF-IDF\n",
    "\n",
    "このノートブックでは、自然言語処理 (NLP) の最も基本的なステップであるテキストデータの前処理と、古典的かつ重要なテキスト表現手法であるBag-of-Words (BoW) および TF-IDF (Term Frequency-Inverse Document Frequency) について学びます。\n",
    "これらの手法は、テキストデータを機械学習アルゴリズムが扱える数値形式に変換するための基礎となります。\n",
    "主にNumPyを使って、これらの概念をスクラッチで実装し、その動作を理解します。\n",
    "\n",
    "**参考論文 (TF-IDFの背景として):**\n",
    "*   Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. *Information processing & management*, 24(5), 513-523. (この論文は主に情報検索における様々な単語重み付け手法を論じています)\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  基本的なテキスト前処理（正規化、トークナイゼーション）。\n",
    "2.  Bag-of-Words (BoW) モデルの概念と実装。\n",
    "3.  TF-IDFの概念（TF、IDF）と実装。\n",
    "4.  これらの手法の長所と短所、そして限界。\n",
    "\n",
    "**前提知識:**\n",
    "*   基本的なPythonプログラミングとNumPyの操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2d7c1",
   "metadata": {},
   "source": [
    " ## 1. 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bcee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re # 正規表現によるテキストクリーニング用\n",
    "from collections import Counter # 単語の頻度カウント用\n",
    "import math # log計算用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30087c52",
   "metadata": {},
   "source": [
    "## 2. テキストデータの前処理\n",
    "\n",
    "機械がテキストデータを理解できるようにするためには、まず生テキストを扱いやすい形に整える「前処理」が必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db5438",
   "metadata": {},
   "source": [
    "### 2.1 テキストのクリーニングと正規化\n",
    "\n",
    "実際のテキストデータには、ノイズとなる文字や、表記の揺れが含まれていることがあります。\n",
    "\n",
    "*   **小文字化 (Lowercasing):** \"Apple\" と \"apple\" を同じ単語として扱うために、全ての文字を小文字（または大文字）に統一します。\n",
    "*   **句読点・特殊文字の除去:** 文の意味に直接関与しないことが多い句読点（ピリオド、カンマなど）や特殊文字（HTMLタグなど）を除去、または空白に置換します。ただし、文脈によっては重要になる場合もあります（例: 感嘆符が感情を表す）。\n",
    "*   **数字の扱い:** 数字をそのまま残すか、特別なトークン（例: `<NUM>`）に置き換えるか、あるいは除去するかをタスクに応じて決定します。\n",
    "*   **Unicode正規化:** 全角文字を半角に統一するなど、文字コードレベルでの正規化。\n",
    "\n",
    "ここでは、簡単な例として小文字化と、英数字以外の除去を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e52c4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_simple(text):\n",
    "    '''テキストを単純に小文字化し、数字と記号を削除する'''\n",
    "    text = text.lower()  # 小文字化\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # 英小文字，数字，スペース以外を削除\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 連続するスペースを1つにまとめ、前後のスペースを削除\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87d80e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: 'Hello World! This is a Test sentence with numbers 123 and symbols #@$.'\n",
      "Processed text: 'hello world this is a test sentence with numbers 123 and symbols'\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "sample_text_raw = \"Hello World! This is a Test sentence with numbers 123 and symbols #@$.\"\n",
    "processed_text = preprocess_text_simple(sample_text_raw)\n",
    "print(f\"Raw text: '{sample_text_raw}'\")\n",
    "print(f\"Processed text: '{processed_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6226b",
   "metadata": {},
   "source": [
    "### 2.2 トークナイゼーション (Tokenization)\n",
    "\n",
    "トークナイゼーションは、前処理されたテキストを個々の単語（またはトークン）のリストに分割するプロセスです。\n",
    "最も単純な方法は、空白文字（スペース、タブ、改行など）で区切ることです。\n",
    "より高度な方法としては、句読点も考慮したり、言語の文法構造に基づいて分割する形態素解析などがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09b75fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_simple(text):\n",
    "    '''テキストを単語に分割する'''\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20f44ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original processed text: 'hello world this is a test sentence with numbers 123 and symbols'\n",
      "Tokens: ['hello', 'world', 'this', 'is', 'a', 'test', 'sentence', 'with', 'numbers', '123', 'and', 'symbols']\n",
      "\n",
      "Processed Corpus (list of token lists):\n",
      "Doc 0: ['this', 'is', 'the', 'first', 'document']\n",
      "Doc 1: ['this', 'document', 'is', 'the', 'second', 'document']\n",
      "Doc 2: ['and', 'this', 'is', 'the', 'third', 'one']\n",
      "Doc 3: ['is', 'this', 'the', 'first', 'document']\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "tokens = tokenize_simple(processed_text)\n",
    "print(f\"Original processed text: '{processed_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# コーパスの準備 (複数の文書からなるリスト)\n",
    "corpus_raw = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "processed_corpus = [tokenize_simple(preprocess_text_simple(doc)) for doc in corpus_raw]\n",
    "print(\"\\nProcessed Corpus (list of token lists):\")\n",
    "for i, doc_tokens in enumerate(processed_corpus):\n",
    "    print(f\"Doc {i}: {doc_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab660e",
   "metadata": {},
   "source": [
    "## 2.3 ステミングとレンマ化\n",
    "\n",
    "*   **ステミング (Stemming):** 単語を語幹（例: \"running\" -> \"run\", \"studies\" -> \"studi\"）に変換する処理。単純なルールベースで行われることが多く、必ずしも正しい語幹になるとは限りません。\n",
    "*   **レンマ化 (Lemmatization):** 単語を見出し語（辞書形、例: \"ran\" -> \"run\", \"better\" -> \"good\"）に変換する処理。品詞情報などを考慮するため、ステミングより高度で正確ですが、計算コストも高くなります。\n",
    "\n",
    "これらは語彙のバリエーションを減らし、異なる形の同じ単語を統一的に扱うために行われます。今回のスクラッチ実装では省略しますが、重要な前処理ステップの一つです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc132cab",
   "metadata": {},
   "source": [
    "## 3. Bag-of-Words (BoW) モデル\n",
    "\n",
    "Bag-of-Words (BoW) は、テキストを数値ベクトルとして表現するための最も基本的な手法の一つです。\n",
    "その名の通り、「単語の袋」として扱い、**文書中での単語の出現順序は無視し、各単語が何回出現したか**という情報のみで文書を表現します。\n",
    "\n",
    "Salton & Buckley (1988) の論文では、情報検索の文脈で文書やクエリを「term vectors」として表現する考え方が述べられています。\n",
    "$D = (t_1, t_2, \\dots, t_p)$\n",
    "ここで、$t_k$ は文書 $D$ における $k$番目の単語（またはその重み）を表します。\n",
    "\n",
    "**BoWベクトルの作成手順:**\n",
    "1.  **語彙の作成 (Vocabulary Building):**\n",
    "    コーパス全体（全ての文書）に出現するユニークな単語のリストを作成します。これが語彙（ボキャブラリ）となります。各単語には一意のインデックスが割り当てられます。\n",
    "2.  **ベクトル化:**\n",
    "    各文書を、語彙と同じ長さのベクトルに変換します。ベクトルの各要素は、語彙内の対応する単語がその文書中に何回出現したか（単語頻度 - Term Frequency, TF）を表します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd4f9a",
   "metadata": {},
   "source": [
    "### 3.1 NumPyによるBoWベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18fa26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_corpus):\n",
    "    '''コーパスから語彙を作成し，単語->インデックスのマッピングを返す'''\n",
    "    all_tokens = [token for doc in tokenized_corpus for token in doc]\n",
    "    unique_tokens = sorted(list(set(all_tokens)))\n",
    "    vocab = {token: i for i, token in enumerate(unique_tokens)}\n",
    "    return vocab\n",
    "\n",
    "def document_to_bow_vector(doc_tokens, vocab):\n",
    "    '''1つの文章（トークンのリスト）をBoWベクトルに変換する'''\n",
    "    vocab_size = len(vocab)\n",
    "    bow_vector = np.zeros(vocab_size, dtype=np.float32)\n",
    "    word_counts = Counter(doc_tokens)\n",
    "    for token, count in word_counts.items():\n",
    "        if token in vocab:\n",
    "            bow_vector[vocab[token]] = count\n",
    "    return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eee30371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (token -> index):\n",
      " {'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n",
      "Vocabulary size: 9\n",
      "\n",
      "BoW Matrix (documents x vocabulary_size):\n",
      " [[0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
      " [0. 2. 0. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0. 0. 1. 0. 1.]]\n",
      "BoW Matrix shape: (4, 9)\n",
      "\n",
      "Vocabulary List (ordered by index):\n",
      " ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "BoW for Document 1 ('this document is the second document'):\n",
      "  'document': 2.0\n",
      "  'is': 1.0\n",
      "  'second': 1.0\n",
      "  'the': 1.0\n",
      "  'this': 1.0\n"
     ]
    }
   ],
   "source": [
    "# 語彙の作成\n",
    "vocabulary = build_vocabulary(processed_corpus)\n",
    "print(\"Vocabulary (token -> index):\\n\", vocabulary)\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# 各文書をBoWベクトルに変換\n",
    "bow_vectors = []\n",
    "for doc_tokens in processed_corpus:\n",
    "    bow_vectors.append(document_to_bow_vector(doc_tokens, vocabulary))\n",
    "\n",
    "bow_matrix = np.array(bow_vectors)\n",
    "print(\"\\nBoW Matrix (documents x vocabulary_size):\\n\", bow_matrix)\n",
    "print(\"BoW Matrix shape:\", bow_matrix.shape)\n",
    "\n",
    "# 語彙の単語リスト (表示用)\n",
    "vocab_list = [token for token, index in sorted(vocabulary.items(), key=lambda item: item[1])]\n",
    "print(\"\\nVocabulary List (ordered by index):\\n\", vocab_list)\n",
    "print(f\"\\nBoW for Document 1 ('{ ' '.join(processed_corpus[1]) }'):\")\n",
    "for word, count in zip(vocab_list, bow_matrix[1]):\n",
    "    if count > 0:\n",
    "        print(f\"  '{word}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f2cef",
   "metadata": {},
   "source": [
    "### 3.2 BoWの長所と短所\n",
    "\n",
    "*   **長所:**\n",
    "    *   シンプルで実装が容易。\n",
    "    *   テキストのトピックをある程度捉えることができる。\n",
    "*   **短所:**\n",
    "    *   **語順の無視:** 単語の出現順序が完全に失われるため、文法的な構造や文脈情報が考慮されない（例: \"A B\" と \"B A\" が同じ表現になる）。\n",
    "    *   **意味の曖昧性:** 同じ単語でも文脈によって意味が異なる場合（多義性）に対応できない。\n",
    "    *   **次元の爆発:** 語彙サイズが非常に大きくなると、BoWベクトルも高次元になり、スパース（ほとんどの要素が0）になりがち。\n",
    "    *   **頻出単語の重み:** \"the\", \"is\", \"a\" のような頻出するが重要度の低い単語の重みが大きくなりやすい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388718c",
   "metadata": {},
   "source": [
    "## 4. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDFは、BoWの欠点の一つである「頻出単語の重み」の問題を軽減するための手法です。\n",
    "各単語の重みを、その単語が文書内でどれだけ重要か（**TF**）と、コレクション全体でどれだけ珍しいか（**IDF**）の組み合わせで決定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a9c78",
   "metadata": {},
   "source": [
    "### 4.1 Term Frequency (TF)\n",
    "\n",
    "TFは、ある単語 $t$ が特定の文書 $d$ 内で出現する頻度です。\n",
    "最も単純なTFは生の出現回数 $tf(t,d)$ です。\n",
    "正規化されたTFもよく使われます（例: 文書内の総単語数で割る、対数を取るなど）。\n",
    "ここでは、簡単のため生の出現回数を使用します（これはBoWベクトルで既に計算済みです）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f520a3",
   "metadata": {},
   "source": [
    "### 4.2 Inverse Document Frequency (IDF)\n",
    "\n",
    "IDFは、ある単語 $t$ がコレクション全体 $D$ の中でどれだけ「珍しい」かを示す指標です。\n",
    "多くの文書に出現する一般的な単語ほどIDFは低く（重要度が低い）、特定の文書に集中して出現する珍しい単語ほどIDFは高くなります。\n",
    "\n",
    "計算式:\n",
    "$idf(t, D) = \\log \\left( \\frac{N}{df_t + 1} \\right)$  \n",
    "*   $N$: コレクション中の総文書数。\n",
    "*   $df_t$: 単語 $t$ を含む文書の数 (Document Frequency)。\n",
    "*   分母に `+1` を加えるのは、ある単語がどの文書にも出現しない場合にゼロ除算を防ぐためと、平滑化のためです。（対数の底はeでも10でも2でも良いですが、結果のスケールが変わるだけです。ここでは自然対数 `np.log` を使います。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd184944",
   "metadata": {},
   "source": [
    "### 4.3 TF-IDFスコア\n",
    "\n",
    "TF-IDFスコアは、TFとIDFの積で計算されます。\n",
    "$tfidf(t, d, D) = tf(t, d) \\times idf(t, D)$\n",
    "これにより、特定の文書で頻繁に出現し、かつコレクション全体では珍しい単語ほど高いスコアが与えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81182709",
   "metadata": {},
   "source": [
    "### 4.4 NumPyによるTF-IDFベクトルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c40cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(bow_matrix):\n",
    "    '''BoWベクトルからTFを計算する'''\n",
    "    # 今回は正規化しないのでそのまま返す\n",
    "    return bow_matrix\n",
    "\n",
    "def calculate_idf(tokenized_corpus, vocab):\n",
    "    '''IDFを計算する'''\n",
    "    num_docs = len(tokenized_corpus)\n",
    "    vocab_size = len(vocab)\n",
    "    idf_vector = np.zeros(vocab_size, dtype=np.float32)\n",
    "\n",
    "    # 各単語がいくつの文章に出現するかをカウント\n",
    "    doc_counts_per_term = np.zeros(vocab_size, dtype=np.float32)\n",
    "    for doc_tokens in tokenized_corpus:\n",
    "        unique_tokens = set(doc_tokens)\n",
    "        for token in unique_tokens:\n",
    "            if token in vocab:\n",
    "                doc_counts_per_term[vocab[token]] += 1\n",
    "\n",
    "    # IDFの計算\n",
    "    idf_vector = np.log((num_docs)/(doc_counts_per_term+1))\n",
    "\n",
    "    return idf_vector\n",
    "\n",
    "def calculate_tfidf(tf_matrix, idf_vector):\n",
    "    '''TF-IDFを計算する'''\n",
    "    return tf_matrix * idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e33ac4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF Vector:\n",
      " [ 0.6931472   0.          0.28768212 -0.22314353  0.6931472   0.6931472\n",
      " -0.22314353  0.6931472  -0.22314353]\n",
      "IDF for 'document': 0.00\n",
      "IDF for 'first': 0.29\n",
      "IDF for 'one': 0.69\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[ 0.          0.          0.28768212 -0.22314353  0.          0.\n",
      "  -0.22314353  0.         -0.22314353]\n",
      " [ 0.          0.          0.         -0.22314353  0.          0.6931472\n",
      "  -0.22314353  0.         -0.22314353]\n",
      " [ 0.6931472   0.          0.         -0.22314353  0.6931472   0.\n",
      "  -0.22314353  0.6931472  -0.22314353]\n",
      " [ 0.          0.          0.28768212 -0.22314353  0.          0.\n",
      "  -0.22314353  0.         -0.22314353]]\n",
      "\n",
      "TF-IDF for Document 0 ('this is the first document'):\n",
      "  'document': TF=1.0, IDF=0.00, TF-IDF=0.00\n",
      "  'first': TF=1.0, IDF=0.29, TF-IDF=0.29\n",
      "  'is': TF=1.0, IDF=-0.22, TF-IDF=-0.22\n",
      "  'the': TF=1.0, IDF=-0.22, TF-IDF=-0.22\n",
      "  'this': TF=1.0, IDF=-0.22, TF-IDF=-0.22\n"
     ]
    }
   ],
   "source": [
    "# 1. TFの計算 (ここではBoWの生の頻度をそのまま使う)\n",
    "tf_matrix_calc = calculate_tf(bow_matrix) \n",
    "# bow_matrix は (num_documents, vocab_size)\n",
    "\n",
    "# 2. IDFの計算\n",
    "idf_vector_calc = calculate_idf(processed_corpus, vocabulary)\n",
    "print(\"\\nIDF Vector:\\n\", idf_vector_calc)\n",
    "print(f\"IDF for '{vocab_list[vocabulary['document']]}': {idf_vector_calc[vocabulary['document']]:.2f}\") # 'document'は3文書に出現\n",
    "print(f\"IDF for '{vocab_list[vocabulary['first']]}': {idf_vector_calc[vocabulary['first']]:.2f}\")     # 'first'は2文書に出現\n",
    "print(f\"IDF for '{vocab_list[vocabulary['one']]}': {idf_vector_calc[vocabulary['one']]:.2f}\")       # 'one'は1文書にのみ出現 (IDF高)\n",
    "\n",
    "# 3. TF-IDFの計算\n",
    "tfidf_matrix_calc = calculate_tfidf(tf_matrix_calc, idf_vector_calc)\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tfidf_matrix_calc)\n",
    "\n",
    "# Document 0 のTF-IDFを確認\n",
    "print(f\"\\nTF-IDF for Document 0 ('{ ' '.join(processed_corpus[0]) }'):\")\n",
    "for word, tfidf_score in zip(vocab_list, tfidf_matrix_calc[0]):\n",
    "    if bow_matrix[0, vocabulary[word]] > 0: # 元のBoWで出現した単語のみ表示\n",
    "        print(f\"  '{word}': TF={bow_matrix[0, vocabulary[word]]}, IDF={idf_vector_calc[vocabulary[word]]:.2f}, TF-IDF={tfidf_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1956c",
   "metadata": {},
   "source": [
    "### 4.5 TF-IDFの利点\n",
    "*   **重要語の重み付け:** 単に頻出するだけでなく、その文書を特徴づけるような重要な単語（特定の文書にはよく出るが、他の文書にはあまり出ない単語）に高い重みを与えることができます。\n",
    "*   **ストップワードの抑制:** \"the\", \"is\", \"a\" のような多くの文書に共通して出現する単語（ストップワード）は、IDFが低くなるため、TF-IDFスコアも自然と低くなり、その影響を抑制できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac11828",
   "metadata": {},
   "source": [
    "## 5. 簡単な応用例 (概念)\n",
    "\n",
    "作成されたBoWベクトルやTF-IDFベクトルは、以下のようなタスクに利用できます。\n",
    "\n",
    "*   **文書類似度計算:**\n",
    "    2つの文書ベクトル間のコサイン類似度などを計算することで、内容がどれだけ似ているかを測ることができます。  \n",
    "    $\\text{cosine\\_similarity}( \\vec{A}, \\vec{B} ) = \\frac{ \\vec{A} \\cdot \\vec{B} }{ \\| \\vec{A} \\| \\| \\vec{B} \\| }$  \n",
    "*   **情報検索:**\n",
    "    クエリも同様にベクトル化し、クエリベクトルと各文書ベクトルとの類似度を計算して、関連性の高い文書をランキングします。\n",
    "*   **テキスト分類:**\n",
    "    各文書ベクトルを特徴量として、ロジスティック回帰、SVM、ナイーブベイズなどの機械学習分類器を学習させ、新しい文書のカテゴリを予測します。\n",
    "\n",
    "これらの応用は、このノートブックの範囲を超えますが、BoWやTF-IDFがどのように使われるかのイメージを持つことは重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4759602",
   "metadata": {},
   "source": [
    "## 6. 考察: BoWとTF-IDFの限界と次へのステップ\n",
    "\n",
    "BoWとTF-IDFは、テキストを数値ベクトル化するための古典的で強力な手法ですが、いくつかの本質的な限界も抱えています。\n",
    "\n",
    "*   **語順の完全な無視:** 文法的な構造や単語間の関係性（例: \"AがBをCする\" と \"BがAをCする\" の違い）を捉えることができません。\n",
    "*   **意味の表現の限界:**\n",
    "    *   **同義語:** \"buy\" と \"purchase\" のように意味が似ている単語も、異なるトークンとして扱われるため、ベクトル空間上では無関係な次元として表現されてしまいます。\n",
    "    *   **多義語:** 同じ単語でも文脈によって意味が異なる場合（例: \"bank\" が銀行か土手か）を区別できません。\n",
    "*   **高次元性とスパース性:** 語彙サイズが大きくなると、ベクトルは非常に高次元かつスパースになり、計算効率やモデルの性能に影響を与えることがあります。\n",
    "\n",
    "これらの限界を克服し、単語の「意味」をより豊かに、かつ低次元の密なベクトルで表現しようとする試みが、次の段階で学ぶ**単語埋め込み (Word Embeddings)**、特にword2vecやGloVeといった手法に繋がっていきます。これらの手法は、単語の分散表現（Distributed Representation）を獲得し、意味的に近い単語がベクトル空間上でも近くに配置されるように学習します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
