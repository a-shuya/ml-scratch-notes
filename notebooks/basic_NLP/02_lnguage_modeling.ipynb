{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90f980",
   "metadata": {},
   "source": [
    "# NLP基礎 (2): 言語モデリング (Language Modeling) と評価指標\n",
    "\n",
    "このノートブックでは、自然言語処理 (NLP) の中核的なタスクの一つである**言語モデリング (Language Modeling)** について学びます。\n",
    "特に、古典的かつ基本的な手法である **n-gram言語モデル** の概念、その学習方法（最尤推定とスムージING）、そして言語モデルの性能を測るための主要な評価指標である**パープレキシティ (Perplexity)** について解説します。\n",
    "NumPyを使って、これらの概念をスクラッチで実装し、その動作を理解します。\n",
    "\n",
    "**参考論文:**\n",
    "*   Chen, S. F., & Goodman, J. (1996). An empirical study of smoothing techniques for language modeling. (参照した論文。様々なスムージング手法を比較)\n",
    "*   Jelinek, F., & Mercer, R. L. (1980). Interpolated estimation of Markov source parameters from sparse data. (線形補間スムージングの基礎)\n",
    "*   Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model component of a speech recognizer. *IEEE transactions on acoustics, speech, and signal processing*, 35(3), 400-401. (Katz Backoff)\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  言語モデルの目的と基本的な考え方。\n",
    "2.  n-gram言語モデルの定義、確率計算、最尤推定。\n",
    "3.  データスパースネス問題と、その解決策としてのスムージング手法（特にラプラススムージングと線形補間の概念）。\n",
    "4.  言語モデルの評価指標である負対数尤度 (NLL) とパープレキシティ (PPL) の定義と計算。\n",
    "5.  NumPyによるn-gramモデルの構築とPPL計算の実装。\n",
    "\n",
    "**前提知識:**\n",
    "*   テキスト前処理、Bag-of-Words、TF-IDFの基本的な理解（NLP基礎(1)）。\n",
    "*   確率の基本的な概念（条件付き確率など）。\n",
    "*   Pythonの基本的なデータ構造とNumPyの操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d4ca4",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a02bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfb1dc",
   "metadata": {},
   "source": [
    "## 2. 言語モデルとは？\n",
    "\n",
    "**言語モデル (Language Model, LM)** とは、単語のシーケンス（文やフレーズなど）がどれだけ「自然」か、つまりそのシーケンスが出現する**確率**を与えるモデルです。\n",
    "\n",
    "$P(W) = P(w_1, w_2, \\dots, w_m)$\n",
    "\n",
    "ここで、$W = (w_1, w_2, \\dots, w_m)$ は単語のシーケンスです。\n",
    "\n",
    "言語モデルは、NLPの多くの応用で中心的な役割を果たします。\n",
    "*   **機械翻訳:** 生成される翻訳文が自然な文かどうかを評価する。\n",
    "*   **音声認識:** 音響モデルからの複数の候補のうち、より自然な単語列を選択する。\n",
    "*   **スペル訂正・かな漢字変換:** 最も確率の高い正しい単語列を提案する。\n",
    "*   **テキスト生成:** 次に来る単語を予測し、自然な文章を生成する。\n",
    "\n",
    "条件付き確率の連鎖律を用いると、シーケンスの同時確率は以下のように分解できます。\n",
    "$P(w_1, w_2, \\dots, w_m) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) \\times \\dots \\times P(w_m | w_1, \\dots, w_{m-1})$\n",
    "$P(W) = \\prod_{i=1}^{m} P(w_i | w_1, \\dots, w_{i-1})$\n",
    "\n",
    "この式は、各単語の出現確率を、それ以前に出現した全ての単語の履歴に基づいて計算することを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4715e",
   "metadata": {},
   "source": [
    "## 3. n-gram言語モデル\n",
    "\n",
    "上記の条件付き確率 $P(w_i | w_1, \\dots, w_{i-1})$ を正確に推定するには、非常に長い履歴を考慮する必要があり、データ量の観点から困難です（データスパースネス問題）。\n",
    "\n",
    "**n-gram言語モデル**は、この問題を解決するために、**マルコフ仮定**を導入します。マルコフ仮定とは、「現在の単語 $w_i$ の出現確率は、直前の $n-1$ 個の単語の履歴にのみ依存する」という仮定です。\n",
    "\n",
    "$P(w_i | w_1, \\dots, w_{i-1}) \\approx P(w_i | w_{i-n+1}, \\dots, w_{i-1})$\n",
    "\n",
    "*   **Unigram (1-gram) モデル ($n=1$):** 各単語は独立に出現すると仮定。\n",
    "    $P(w_i | w_1, \\dots, w_{i-1}) \\approx P(w_i)$\n",
    "*   **Bigram (2-gram) モデル ($n=2$):** 現在の単語は直前の1単語のみに依存。\n",
    "    $P(w_i | w_1, \\dots, w_{i-1}) \\approx P(w_i | w_{i-1})$\n",
    "*   **Trigram (3-gram) モデル ($n=3$):** 現在の単語は直前の2単語のみに依存。\n",
    "    $P(w_i | w_1, \\dots, w_{i-1}) \\approx P(w_i | w_{i-2}, w_{i-1})$\n",
    "\n",
    "一般に、$n$ が大きいほどより長い文脈を考慮できますが、データスパースネス問題が深刻になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fd92d",
   "metadata": {},
   "source": [
    "### 3.1 n-gram確率の最尤推定 (Maximum Likelihood Estimation - MLE)\n",
    "\n",
    "n-gramの条件付き確率は、訓練コーパスにおける出現頻度から最尤推定で計算できます。\n",
    "例えば、Bigram確率 $P(w_i | w_{i-1})$ は、\n",
    "\n",
    "$P_{ML}(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}$\n",
    "\n",
    "Trigram確率 $P(w_i | w_{i-2}, w_{i-1})$ は、\n",
    "\n",
    "$P_{ML}(w_i | w_{i-2}, w_{i-1}) = \\frac{\\text{Count}(w_{i-2}, w_{i-1}, w_i)}{\\text{Count}(w_{i-2}, w_{i-1})}$\n",
    "\n",
    "ここで、$\\text{Count}(\\cdot)$ はコーパス中でのn-gram（またはそのプレフィックス）の出現回数です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc668f91",
   "metadata": {},
   "source": [
    "### 3.2 データスパースネスとゼロ確率問題\n",
    "\n",
    "最尤推定には大きな問題があります。訓練コーパスが有限であるため、実際にはあり得るn-gramでも、たまたま訓練コーパスに出現しなかったものの確率は0になってしまいます（**ゼロ確率問題**）。\n",
    "例えば、\"thank you very much\" という自然なフレーズがあっても、訓練コーパスに \"thank you very\" というtrigramが出現しなかった場合、$P(\\text{much} | \\text{thank you very}) = 0$ となり、このフレーズ全体の確率も0になってしまいます。これは明らかに不適切です。\n",
    "この問題を解決するために、**スムージング (Smoothing)** というテクニックが使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba31cd",
   "metadata": {},
   "source": [
    "## 4. スムージング手法\n",
    "\n",
    "スムージングは、観測されなかったn-gramにもゼロでない小さな確率を割り当て、観測されたn-gramの確率を少し割り引くことで、確率の総和が1になるように調整する手法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c8ecd",
   "metadata": {},
   "source": [
    "### 4.1 ラプラススムージング (Add-one Smoothing)\n",
    "\n",
    "最も単純なスムージング手法で、全てのn-gramの出現回数に1を加えてから確率を計算します。\n",
    "例えば、Bigramの場合:\n",
    "\n",
    "$P_{Laplace}(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i) + 1}{\\text{Count}(w_{i-1}) + V}$\n",
    "\n",
    "ここで、$V$ は語彙サイズです。分母に $V$ を加えるのは、全ての可能な次の単語についてカウントが1増えるため、確率の総和を1に保つためです。\n",
    "\n",
    "*   **利点:** 実装が非常に簡単で、ゼロ確率を防げます。\n",
    "*   **欠点:** 訓練コーパスに出現しなかったn-gramに過大な確率質量を割り当ててしまう傾向があります。特に大規模な語彙の場合、この影響が大きくなります。\n",
    "\n",
    "一般的には、より洗練されたAdd-k Smoothing（1の代わりに小さな定数 $k$ を加える）も使われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55dd869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['</s>', '<s>', 'a', 'cat', 'dog', 'ran', 'sat', 'the']\n",
      "Vocabulary Size: 8\n",
      "Bigram Counts (例: '<s>'): ', dict(bigram_counts['<s>'])\n",
      "Unigram Counts (例: '<s>'):  4\n"
     ]
    }
   ],
   "source": [
    "# 簡単なコーパスでテスト\n",
    "test_corpus = [\n",
    "    \"<s> a cat sat </s>\", # <s>は文頭、</s>は文末トークン\n",
    "    \"<s> a dog sat </s>\",\n",
    "    \"<s> the cat ran </s>\",\n",
    "    \"<s> the dog ran </s>\"\n",
    "]\n",
    "\n",
    "# トークン化\n",
    "tokenized_corpus = [line.split() for line in test_corpus]\n",
    "\n",
    "# 語彙の作成\n",
    "all_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "vocab = sorted(list(set(all_words)))\n",
    "vocab_size = len(vocab)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Bigramカウント\n",
    "bigram_counts = defaultdict(Counter)\n",
    "unigram_counts = Counter()\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    for i in range(len(sentence) - 1):\n",
    "        prev_word = sentence[i]\n",
    "        curr_word = sentence[i+1]\n",
    "        bigram_counts[prev_word][curr_word] += 1\n",
    "        unigram_counts[prev_word] += 1\n",
    "\n",
    "print(\"Bigram Counts (例: '<s>'): ', dict(bigram_counts['<s>'])\")\n",
    "print(\"Unigram Counts (例: '<s>'): \", unigram_counts['<s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4249549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P(cat | a) (Laplace, Add-1): 0.2000\n",
      "P(dog | the) (Laplace, Add-1): 0.2000\n",
      "P(sat | unknown_word) (Laplace, Add-1): 0.1250\n"
     ]
    }
   ],
   "source": [
    "def get_laplace_bigram_prob(prev_word,\n",
    "                            curr_word,\n",
    "                            bigram_counts,\n",
    "                            unigram_counts,\n",
    "                            vocab_size,\n",
    "                            k=1):\n",
    "    ''' ラプラススムージング (Add-k) を用いたBigram確率の計算 '''\n",
    "    numerator = bigram_counts[prev_word][curr_word] + k\n",
    "    denominator = unigram_counts[prev_word] + k * vocab_size\n",
    "    return numerator / denominator\n",
    "\n",
    "# テスト\n",
    "prob_cat_given_a = get_laplace_bigram_prob(\"a\", \"cat\", bigram_counts, unigram_counts, vocab_size)\n",
    "print(f\"\\nP(cat | a) (Laplace, Add-1): {prob_cat_given_a:.4f}\") # (1+1)/(2+1*V)\n",
    "\n",
    "prob_dog_given_the = get_laplace_bigram_prob(\"the\", \"dog\", bigram_counts, unigram_counts, vocab_size)\n",
    "print(f\"P(dog | the) (Laplace, Add-1): {prob_dog_given_the:.4f}\") # (1+1)/(2+1*V)\n",
    "\n",
    "prob_sat_given_unknown = get_laplace_bigram_prob(\"unknown_word\", \"sat\", bigram_counts, unigram_counts, vocab_size)\n",
    "print(f\"P(sat | unknown_word) (Laplace, Add-1): {prob_sat_given_unknown:.4f}\") # 1/V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d48b0",
   "metadata": {},
   "source": [
    "### 4.2 線形補間 (Linear Interpolation)\n",
    "\n",
    "より高性能なスムージング手法の一つに線形補間があります。これは、異なる次数のn-gramモデルの予測を重み付きで組み合わせるものです。\n",
    "例えば、Trigramモデルの場合、TrigramのMLE、BigramのMLE、UnigramのMLEを以下のように補間します。\n",
    "\n",
    "$P_{interp}(w_i | w_{i-2}, w_{i-1}) = \\lambda_3 P_{ML}(w_i | w_{i-2}, w_{i-1}) + \\lambda_2 P_{ML}(w_i | w_{i-1}) + \\lambda_1 P_{ML}(w_i)$\n",
    "\n",
    "ここで、$\\lambda_1, \\lambda_2, \\lambda_3$ は重みで、$\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$ かつ $\\lambda_j \\ge 0$ です。\n",
    "これらの重み $\\lambda_j$ は、通常、開発セット（held-out data）を用いて、開発セットの尤度（またはパープレキシティ）を最大（最小）にするように学習されます。\n",
    "Jelinek-Mercer Smoothingはこの線形補間の一種で、$\\lambda$の値が文脈に依存する場合もあります。\n",
    "\n",
    "Chen & Goodman (1996) の論文では、この線形補間をベースとした様々なスムージング手法が比較検討されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23ebe6",
   "metadata": {},
   "source": [
    "### 4.3 Katz Backoff と Kneser-Ney Smoothing (概念紹介)\n",
    "\n",
    "*   **Katz Backoff:**\n",
    "    高次のn-gram（例: Trigram）が訓練データで観測されなかった場合、その確率を推定するために、より低次のn-gram（例: Bigram）の情報に「バックオフ」します。その際、確率の総和が1になるように、割り引かれた確率質量を低次モデルに分配します。Good-Turing推定などのアイデアが使われます。\n",
    "\n",
    "*   **Kneser-Ney Smoothing:**\n",
    "    現在最も性能が良いとされるスムージング手法の一つです。Katz Backoffを改良したもので、特に低頻度のn-gramの確率推定において優れています。単語がどれだけ多様な文脈で出現するか（continuation probability）を考慮する点が特徴的です。\n",
    "\n",
    "これらの高度なスムージング手法の実装は複雑なため、このノートブックでは扱いませんが、高性能な言語モデルを構築する上で重要であることを覚えておきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639588d",
   "metadata": {},
   "source": [
    "## 5. 言語モデルの評価指標\n",
    "\n",
    "学習された言語モデルの性能を評価するための主要な指標は、**負対数尤度 (Negative Log-Likelihood, NLL)** と**パープレキシティ (Perplexity, PPL)** です。\n",
    "これらの指標は、通常、学習に使われなかった**テストセット**に対して計算されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419f52b",
   "metadata": {},
   "source": [
    "### 5.1 負対数尤度 (NLL)\n",
    "\n",
    "テストセット $W_{test} = (w_1, w_2, \\dots, w_N)$ （$N$はテストセットの総単語数）に対するモデルの負対数尤度は、\n",
    "\n",
    "$NLL(W_{test}) = - \\sum_{i=1}^{N} \\log_2 P(w_i | \\text{context}_i)$\n",
    "\n",
    "ここで、$P(w_i | \\text{context}_i)$ はモデルが予測する、文脈 $\\text{context}_i$ の後に単語 $w_i$ が出現する確率です。\n",
    "対数の底は2、e、10のいずれでも使われますが、2を使うと単位がビットになります。\n",
    "**NLLは小さいほど、モデルがテストデータをより確からしいと評価していることを意味し、性能が良いとされます。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f8947",
   "metadata": {},
   "source": [
    "### 5.2 パープレキシティ (Perplexity - PPL)\n",
    "\n",
    "パープレキシティは、NLLをテストセットの総単語数 $N$ で割った平均負対数尤度（これはクロスエントロピー $H(p,q)$ の推定値に近い）を指数関数の肩に乗せたものです。\n",
    "\n",
    "$PPL(W_{test}) = 2^{H(p,q)} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | \\text{context}_i)}$\n",
    "\n",
    "**直感的な解釈:**\n",
    "パープレキシティは、「言語モデルが次の単語を予測する際に、平均していくつの選択肢に均等に迷っているか」を示す指標と解釈できます。\n",
    "例えば、PPLが100であれば、モデルは次の単語を予測する際に平均して100個の単語の中から1つを選ぶのと同じくらいの不確かさを持っている、と考えられます。\n",
    "**PPLも小さいほど、モデルの性能が良いとされます。** 語彙サイズが小さいほど、PPLのベースラインも小さくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184549d9",
   "metadata": {},
   "source": [
    "### 5.3 NumPyによるPPLの計算実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74760e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(log_probabilities):\n",
    "    '''\n",
    "    一連の対数確率からパープレキシティを計算する関数\n",
    "    Parameters:\n",
    "        log_probabilities(list or np.array): 各予測ステップでの真の次の単語の対数確率\n",
    "    Returns:\n",
    "        float: パープレキシティ\n",
    "    '''\n",
    "    num_predictions = len(log_probabilities)\n",
    "    if num_predictions == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # 平均負対数尤度\n",
    "    # H = - (1/N) * Σ log(P(w_i | w_{i-1}))\n",
    "    mean_log_prob = -np.mean(log_probabilities)\n",
    "    perplexity = np.power(2, mean_log_prob)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c189f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities: [-1.0, -2.0, -3.0]\n",
      "Perplexity: 4.0000\n",
      "\n",
      "Log probabilities (perfect): [0.0, 0.0]\n",
      "Perplexity (perfect): 1.0000\n",
      "\n",
      "Log probabilities (random guess, vocab=100): -6.64 ...\n",
      "Perplexity (random guess, vocab=100): 100.0000\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "# 例: 3つの単語を予測し、それぞれの真の単語の対数確率 (底2) が以下だったとする\n",
    "log_probs_example = [math.log2(0.5), math.log2(0.25), math.log2(0.125)] # [-1, -2, -3]\n",
    "# 平均NLL = -(-1 -2 -3)/3 = 6/3 = 2\n",
    "# PPL = 2^2 = 4\n",
    "ppl_example = calculate_perplexity(log_probs_example)\n",
    "print(f\"Log probabilities: {log_probs_example}\")\n",
    "print(f\"Perplexity: {ppl_example:.4f}\") # 期待値: 4.0\n",
    "\n",
    "log_probs_perfect = [math.log2(1.0), math.log2(1.0)] # 常に確率1で予測\n",
    "ppl_perfect = calculate_perplexity(log_probs_perfect)\n",
    "print(f\"\\nLog probabilities (perfect): {log_probs_perfect}\")\n",
    "print(f\"Perplexity (perfect): {ppl_perfect:.4f}\") # 期待値: 1.0 (迷いがない)\n",
    "\n",
    "log_probs_random_vocab100 = [math.log2(1/100)] * 10 # 語彙100でランダムに予測\n",
    "ppl_random_vocab100 = calculate_perplexity(log_probs_random_vocab100)\n",
    "print(f\"\\nLog probabilities (random guess, vocab=100): {log_probs_random_vocab100[0]:.2f} ...\")\n",
    "print(f\"Perplexity (random guess, vocab=100): {ppl_random_vocab100:.4f}\") # 期待値: 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805b688",
   "metadata": {},
   "source": [
    "## 6. 実験: 簡単なテキストコーパスでのn-gramモデル学習と評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2e0dc",
   "metadata": {},
   "source": [
    "### 6.1 トイコーパスと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11084677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単なテキスト前処理とトークン化関数（再掲）\n",
    "import re\n",
    "def preprocess_text_simple(text):\n",
    "    '''テキストを単純に小文字化し、数字と記号を削除する'''\n",
    "    text = text.lower()  # 小文字化\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # 英小文字，数字，スペース以外を削除\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 連続するスペースを1つにまとめ、前後のスペースを削除\n",
    "    return text\n",
    "\n",
    "def tokenize_simple(text):\n",
    "    '''テキストを単語に分割する'''\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa56f9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Toy Corpus:\n",
      "['<s>', 'the', 'cat', 'sat', 'on', 'the', 'mat', '</s>']\n",
      "['<s>', 'the', 'dog', 'sat', 'on', 'the', 'log', '</s>']\n",
      "['<s>', 'a', 'cat', 'chased', 'a', 'dog', '</s>']\n",
      "['<s>', 'a', 'dog', 'chased', 'the', 'cat', '</s>']\n",
      "\n",
      "Toy Vocabulary: ['</s>', '<s>', 'a', 'cat', 'chased', 'dog', 'log', 'mat', 'on', 'sat', 'the']\n",
      "Toy Vocabulary Size: 11\n"
     ]
    }
   ],
   "source": [
    "# 簡単なトイコーパス\n",
    "toy_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"a cat chased a dog\",\n",
    "    \"a dog chased the cat\"\n",
    "]\n",
    "\n",
    "# 前処理とトークナイズ、文頭・文末記号の追加\n",
    "BOS = \"<s>\" # Begin Of Sentence\n",
    "EOS = \"</s>\" # End Of Sentence\n",
    "tokenized_toy_corpus = []\n",
    "for sentence in toy_corpus:\n",
    "    processed = preprocess_text_simple(sentence)\n",
    "    tokens = [BOS] + tokenize_simple(processed) + [EOS]\n",
    "    tokenized_toy_corpus.append(tokens)\n",
    "\n",
    "print(\"Tokenized Toy Corpus:\")\n",
    "for sent_toks in tokenized_toy_corpus:\n",
    "    print(sent_toks)\n",
    "\n",
    "# 語彙の再作成\n",
    "all_toy_words = [word for sent_toks in tokenized_toy_corpus for word in sent_toks]\n",
    "toy_vocabulary = sorted(list(set(all_toy_words)))\n",
    "toy_vocab_size = len(toy_vocabulary)\n",
    "toy_word_to_idx = {word: i for i, word in enumerate(toy_vocabulary)}\n",
    "\n",
    "print(\"\\nToy Vocabulary:\", toy_vocabulary)\n",
    "print(\"Toy Vocabulary Size:\", toy_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0135c5",
   "metadata": {},
   "source": [
    "### 6.2 n-gramカウントと確率計算 (ラプラススムージング)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3d7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(cat | <s>) [Bigram, Laplace]: 0.0526\n",
      "P(sat | a cat) [Trigram, Laplace]: 0.0833\n"
     ]
    }
   ],
   "source": [
    "def count_ngrams(tokenized_corpus, n):\n",
    "    \"\"\" n-gramとその頻度をカウントする \"\"\"\n",
    "    ngram_counts = defaultdict(Counter) # history -> current_word -> count\n",
    "    history_counts = Counter()         # history -> count\n",
    "    \n",
    "    for sentence_tokens in tokenized_corpus:\n",
    "        # n-1個のBOSを文頭に追加して履歴を作成しやすくする\n",
    "        padded_sentence = [BOS]*(n-1) + sentence_tokens\n",
    "        for i in range(n-1, len(padded_sentence)):\n",
    "            history = tuple(padded_sentence[i-(n-1) : i]) # (n-1)長のタプル\n",
    "            current_word = padded_sentence[i]\n",
    "            \n",
    "            ngram_counts[history][current_word] += 1\n",
    "            history_counts[history] += 1\n",
    "            \n",
    "    return ngram_counts, history_counts\n",
    "\n",
    "def get_ngram_laplace_prob(history_tuple, current_word, ngram_counts, history_counts, vocab_size, k=1):\n",
    "    \"\"\" ラプラススムージングを用いたn-gram条件付き確率 \"\"\"\n",
    "    # history_tuple は (w_{i-n+1}, ..., w_{i-1})\n",
    "    count_ngram = ngram_counts[history_tuple][current_word]\n",
    "    count_history = history_counts[history_tuple]\n",
    "    \n",
    "    if count_history == 0 and k == 0: # Add-0で履歴がない場合は均等確率 (またはより低次のモデルへ)\n",
    "        return 1.0 / vocab_size \n",
    "        \n",
    "    prob = (count_ngram + k) / (count_history + k * vocab_size)\n",
    "    return prob\n",
    "\n",
    "# Bigramモデル (n=2)\n",
    "bigram_lm_counts, bigram_history_counts = count_ngrams(tokenized_toy_corpus, n=2)\n",
    "\n",
    "# Trigramモデル (n=3)\n",
    "trigram_lm_counts, trigram_history_counts = count_ngrams(tokenized_toy_corpus, n=3)\n",
    "\n",
    "# 例: P(cat | <s>) for Bigram\n",
    "prob_cat_given_bos_bi = get_ngram_laplace_prob(tuple([BOS]), \"cat\", bigram_lm_counts, bigram_history_counts, toy_vocab_size)\n",
    "print(f\"P(cat | <s>) [Bigram, Laplace]: {prob_cat_given_bos_bi:.4f}\")\n",
    "\n",
    "# 例: P(sat | a, cat) for Trigram\n",
    "prob_sat_given_a_cat_tri = get_ngram_laplace_prob(tuple([BOS, \"a\", \"cat\"][-2:]), \"sat\", # 履歴は直前2単語\n",
    "                                                 trigram_lm_counts, trigram_history_counts, toy_vocab_size)\n",
    "print(f\"P(sat | a cat) [Trigram, Laplace]: {prob_sat_given_a_cat_tri:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efacbc",
   "metadata": {},
   "source": [
    "### 6.3 テストセットでのパープレキシティ計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd6c01e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence PPL (Bigram, Laplace Add-1): 8.66\n",
      "Test sentence PPL (Trigram, Laplace Add-1): 9.66\n",
      "Test sentence PPL (Bigram, Laplace Add-0.01): 3.37\n"
     ]
    }
   ],
   "source": [
    "# テスト用文 (訓練データにはないが、語彙は共通と仮定)\n",
    "test_sentence_tokens = [BOS, \"the\", \"dog\", \"chased\", \"the\", \"mat\", EOS]\n",
    "\n",
    "def calculate_sentence_perplexity(sentence_tokens, n, ngram_counts, history_counts, vocab_size, smoothing_k=1):\n",
    "    \"\"\" 1つの文に対するパープレキシティを計算 (n-gramモデル) \"\"\"\n",
    "    log_prob_sum = 0.0\n",
    "    num_predicted_words = 0 # 実際に確率を計算した単語数 (BOSは除く)\n",
    "\n",
    "    # n-1個のBOSでパディング\n",
    "    padded_sentence = [BOS]*(n-1) + sentence_tokens\n",
    "    \n",
    "    for i in range(n-1, len(padded_sentence)):\n",
    "        history = tuple(padded_sentence[i-(n-1) : i])\n",
    "        current_word = padded_sentence[i]\n",
    "        \n",
    "        # BOSを予測対象にはしない (通常、文頭記号の確率は考慮しないか、別途扱う)\n",
    "        # ただし、ここでは単純化のため、P(word1 | <s>) から計算開始\n",
    "        # EOSの確率 P(</s> | context) は計算する\n",
    "        \n",
    "        prob = get_ngram_laplace_prob(history, current_word, ngram_counts, history_counts, vocab_size, k=smoothing_k)\n",
    "        if prob == 0: # ゼロ確率が発生した場合 (スムージングが不十分など)\n",
    "            # log(0) を避けるために非常に小さい値を代入するか、エラー処理\n",
    "            # print(f\"Warning: Zero probability for P({current_word} | {history})\")\n",
    "            log_prob_sum += math.log2(1e-100) # 非常に小さい確率の対数\n",
    "        else:\n",
    "            log_prob_sum += math.log2(prob)\n",
    "        \n",
    "        # 文末記号EOSは予測されるが、次の単語の履歴にはならない\n",
    "        if current_word != BOS : # BOS自身を予測する確率は数えない\n",
    "             num_predicted_words +=1\n",
    "             \n",
    "    if num_predicted_words == 0: return float('inf')\n",
    "    \n",
    "    average_neg_log_likelihood = -log_prob_sum / num_predicted_words\n",
    "    perplexity = np.power(2, average_neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "# BigramモデルでのPPL\n",
    "ppl_bigram_test = calculate_sentence_perplexity(test_sentence_tokens, 2, \n",
    "                                                bigram_lm_counts, bigram_history_counts, \n",
    "                                                toy_vocab_size, smoothing_k=1)\n",
    "print(f\"Test sentence PPL (Bigram, Laplace Add-1): {ppl_bigram_test:.2f}\")\n",
    "\n",
    "# TrigramモデルでのPPL\n",
    "ppl_trigram_test = calculate_sentence_perplexity(test_sentence_tokens, 3,\n",
    "                                                 trigram_lm_counts, trigram_history_counts,\n",
    "                                                 toy_vocab_size, smoothing_k=1)\n",
    "print(f\"Test sentence PPL (Trigram, Laplace Add-1): {ppl_trigram_test:.2f}\")\n",
    "\n",
    "# よりスムージングを弱く (Add-0.01)\n",
    "ppl_bigram_add_k = calculate_sentence_perplexity(test_sentence_tokens, 2, \n",
    "                                                 bigram_lm_counts, bigram_history_counts, \n",
    "                                                 toy_vocab_size, smoothing_k=0.01)\n",
    "print(f\"Test sentence PPL (Bigram, Laplace Add-0.01): {ppl_bigram_add_k:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcb885",
   "metadata": {},
   "source": [
    "## 7. 考察\n",
    "\n",
    "*   **n-gram言語モデルの基本:**\n",
    "    *   マルコフ仮定に基づいて、直前の数単語の履歴から次の単語の出現確率をモデル化する単純かつ効果的な手法です。\n",
    "    *   $n$ の値を大きくするほど長い文脈を考慮できますが、データスパースネス問題が深刻になります。一般的にTrigram ($n=3$) や4-gram程度が実用的な上限とされることが多いです。\n",
    "*   **スムージングの重要性:**\n",
    "    *   訓練データに出現しなかったn-gramに対してゼロでない確率を割り当てるために不可欠です。ラプラススムージングは最も単純ですが、より高度な手法（Kneser-Neyなど）が一般的には高性能です。\n",
    "    *   スムージングの度合い（例: Add-kの $k$ の値）は、モデルの性能に影響を与えます。\n",
    "*   **パープレキシティ (PPL):**\n",
    "    *   言語モデルの性能を測る標準的な指標であり、モデルがテストデータに対してどれだけ「驚いているか」を示します。低いほど良いモデルです。\n",
    "    *   異なるモデルや異なるコーパスでPPLを比較する際は、語彙サイズや前処理が同じであることを確認する必要があります。\n",
    "*   **n-gramモデルの限界:**\n",
    "    *   **長期依存性の欠如:** n-gramの「n」で定義される固定長の文脈しか考慮できないため、それより遠く離れた単語間の依存関係を捉えることができません。\n",
    "    *   **意味的類似性の欠如:** \"cat\" と \"dog\" のように意味が似ていても、異なる単語として扱われるため、これらの単語が同じような文脈で出現するという知識を利用できません。\n",
    "    *   **データスパースネス:** $n$ が大きくなると、観測されるn-gramの数が急増し、ほとんどのn-gramの出現頻度が0か非常に小さくなります。\n",
    "\n",
    "これらの限界を克服するために、ニューラルネットワークを用いた言語モデル（例: RNN LM、Transformer LM）や、単語の意味を密なベクトルで表現する単語埋め込み（Word Embeddings）といった技術が登場します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
