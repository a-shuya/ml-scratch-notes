{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a951af6",
   "metadata": {},
   "source": [
    "# NLP基礎 (3): サブワード分割 (Sub-word Tokenization) - BPE, WordPiece, SentencePiece\n",
    "\n",
    "このノートブックでは、現代の自然言語処理 (NLP) モデル、特に大規模言語モデルにおいて不可欠な技術となっている**サブワード分割 (Sub-word Tokenization)** について学びます。\n",
    "単語ベースのトークナイゼーションが抱える語彙爆発や未知語の問題を解決するために、Byte Pair Encoding (BPE)、WordPiece、SentencePieceといった代表的なサブワード分割アルゴリズムのアイデアと仕組みを解説します。\n",
    "BPEについては、PythonとNumPyを使って主要な処理を簡易的に実装し、SentencePieceについてはライブラリの基本的な使い方を紹介します。\n",
    "\n",
    "**参考論文:**\n",
    "*   Sennrich, R., Haddow, B., & Birch, A. (2016). Neural machine translation of rare words with subword units. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (pp. 1715-1725). (BPEのNLPへの応用)\n",
    "*   Schuster, M., & Nakajima, K. (2012). Japanese and korean voice search. In *2012 ieee international conference on acoustics, speech and signal processing (icassp)* (pp. 5149-5152). IEEE. (WordPieceの初期の言及の一つ)\n",
    "*   Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Dean, J. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*. (GNMTでWordPieceを使用)\n",
    "*   Kudo, T., & Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In *Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations* (pp. 66-71). (SentencePiece提案論文)\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  サブワード分割の必要性：語彙爆発と未知語問題。\n",
    "2.  Byte Pair Encoding (BPE) のアルゴリズムと簡易実装。\n",
    "3.  WordPieceの基本的なアイデアとBPEとの違い。\n",
    "4.  SentencePieceの設計思想、特徴、および基本的な使い方。\n",
    "5.  各サブワード分割手法の比較。\n",
    "\n",
    "**前提知識:**\n",
    "*   テキスト前処理（トークナイゼーションなど）の基本的な理解（NLP基礎(1)のノートブック）。\n",
    "*   Pythonの基本的なデータ構造（辞書、リスト、セット）とNumPyの操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751d333",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f84ef10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\user194\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict # defaultdictはBPEの実装に便利\n",
    "import heapq # BPEで最も頻繁なペアを見つけるのに使える（今回はCounterで代用）\n",
    "\n",
    "!pip install sentencepiece\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee46442",
   "metadata": {},
   "source": [
    "## 2. サブワード分割の必要性\n",
    "\n",
    "前のノートブックで学んだ単語埋め込み (Word Embeddings) は、単語を単位としてベクトル表現を獲得しました。しかし、このアプローチにはいくつかの課題があります。\n",
    "\n",
    "*   **語彙爆発 (Vocabulary Explosion):**\n",
    "    *   言語には非常に多くの単語が存在し（特に形態的に豊かな言語や、新しい単語が頻繁に生まれるドメイン）、全ての単語を語彙に含めようとすると、語彙サイズが非常に大きくなります。\n",
    "    *   これは、モデルのパラメータ数増加（特に埋め込み層）、メモリ使用量の増大、計算コストの増加に繋がります。\n",
    "*   **未知語 (Out-of-Vocabulary, OOV) 問題:**\n",
    "    *   訓練データに出現しなかった単語（未知語）は、テスト時や実際の応用時に問題となります。これらの単語は通常、特別な`<UNK>`トークンに置き換えられますが、それでは単語固有の情報が失われてしまいます。\n",
    "    *   特に、固有名詞、専門用語、タイプミス、新語などは未知語になりやすいです。\n",
    "\n",
    "**サブワード分割**は、これらの問題を解決するために、単語をより小さな単位（サブワード、例: \"unfortunately\" -> \"un\", \"fortunate\", \"ly\" や \"unfortunate\", \"ly\"）に分割するアプローチです。\n",
    "\n",
    "**サブワード分割の利点:**\n",
    "*   **語彙サイズの制御:** 頻繁に出現する単語はそのまま保持し、低頻度語や未知語はサブワードの組み合わせで表現することで、固定サイズの語彙で多くの単語をカバーできます。\n",
    "*   **未知語への対応:** 未知語であっても、既知のサブワードの組み合わせで表現できる可能性が高まります（例: \"jetliner\" が未知でも \"jet\" と \"liner\" が既知なら表現可能）。\n",
    "*   **形態情報の活用:** \"un-\", \"-ly\", \"-ing\" のような接辞がサブワードとして学習されれば、単語の形態的な構造や意味を捉えるのに役立ちます。\n",
    "*   **データスパースネスの緩和:** 単語よりもサブワードの方がコーパス中での出現頻度が高くなるため、より頑健な統計的学習が期待できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0bea4",
   "metadata": {},
   "source": [
    "## 3. Byte Pair Encoding (BPE)\n",
    "\n",
    "BPEは、元々はデータ圧縮アルゴリズムとして提案されましたが、Sennrichら (2016) によってニューラル機械翻訳のためのサブワード分割手法として導入され、広く使われるようになりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def5752",
   "metadata": {},
   "source": [
    "### 3.1 BPEアルゴリズムのステップ\n",
    "\n",
    "1.  **初期化:**\n",
    "    *   コーパス内の全単語を文字のシーケンスに分割し、初期の語彙を全てのユニークな文字（バイト）の集合とします。各単語の終わりには特別な終端記号（例: `</w>`）を付加することが多いです。\n",
    "    *   各単語の出現頻度も考慮します。\n",
    "2.  **繰り返し処理 (指定されたマージ回数または語彙サイズに達するまで):**  \n",
    "    a.  現在の単語分割状態において、コーパス全体で最も頻繁に出現する隣接するサブワード（または文字）のペアを見つけます。  \n",
    "    b.  その最も頻繁なペアを新しい1つのサブワードとしてマージし、語彙に追加します。  \n",
    "    c.  コーパス中の該当するペアを全て新しいサブワードで置き換えます。  \n",
    "\n",
    "このプロセスを繰り返すことで、頻繁に出現する文字の組み合わせが徐々に長いサブワードとして学習されていきます。最終的な語彙には、個々の文字、学習されたサブワード、そして頻繁な単語全体が含まれることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e416b16",
   "metadata": {},
   "source": [
    "### 3.2 PythonによるBPEの簡易実装 (学習とセグメンテーション)\n",
    "\n",
    "ここでは、BPEの核となるマージ処理のロジックを簡易的に実装してみます。\n",
    "\n",
    "**学習フェーズ:**\n",
    "1.  単語を文字に分割し、頻度をカウント。\n",
    "2.  最も頻繁な隣接ペアを見つける。\n",
    "3.  そのペアをマージし、単語の表現を更新。\n",
    "4.  2-3を繰り返す。\n",
    "\n",
    "**セグメンテーションフェーズ:**\n",
    "学習済みのマージ操作（優先順位の高いものから）を、新しい単語に適用していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e071c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_char_freqs(corpus_words_with_freqs):\n",
    "    '''単語を文字シーケンスに分割し，終端記号を付加，頻度も保持\n",
    "    例) 'apple': 5 -> 'a p p l e </w>': 5\n",
    "    '''\n",
    "    word_char_freqs = defaultdict(int)\n",
    "    for word, freq in corpus_words_with_freqs.items():\n",
    "        # 単語の終わりを示す特別な記号を追加\n",
    "        # 文字間にスペースを入れて，個々の文字をトークンとして扱う\n",
    "        spaced_word = ' '.join(list(word)) + ' </w>'\n",
    "        word_char_freqs[spaced_word] += freq\n",
    "    return word_char_freqs\n",
    "\n",
    "def get_stats(word_char_freqs):\n",
    "    '''現在の単語分割における隣接ペアの頻度を計算'''\n",
    "    pairs = defaultdict(int)\n",
    "    for word_chars, freq in word_char_freqs.items():\n",
    "        symbols = word_chars.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq # ペアの出現回数に単語の頻度を加算\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair_to_merge, v_in):\n",
    "    '''語彙内の指定されたペアをマージする'''\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair_to_merge))\n",
    "    merged_symbol = ''.join(pair_to_merge)  # ペアを結合して新しいシンボルを作成\n",
    "\n",
    "    # 正規表現で、スペースで区切られたペアを、スペースなしの結合シンボルに置き換える\n",
    "    # 例: \"t e s t </w>\" でペア (\"e\", \"s\") をマージする場合、 \"t es t </w>\" になる\n",
    "\n",
    "    for word_chars, freq in v_in.items():\n",
    "        symbols = word_chars.split()\n",
    "        j = 0\n",
    "        new_symbols = []\n",
    "        while j < len(symbols):\n",
    "            if j < len(symbols)-1 and (symbols[j], symbols[j+1]) == pair_to_merge:\n",
    "                new_symbols.append(merged_symbol)\n",
    "                j += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[j])\n",
    "                j += 1\n",
    "        v_out[' '.join(new_symbols)] = freq\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "277706bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Character Frequencies:\n",
      "a p p l e </w>: 5\n",
      "b a n a n a </w>: 3\n",
      "o r a n g e </w>: 2\n",
      "\n",
      "Adjacent Pairs Frequencies:\n",
      "('a', 'p'): 5\n",
      "('p', 'p'): 5\n",
      "('p', 'l'): 5\n",
      "('l', 'e'): 5\n",
      "('e', '</w>'): 7\n",
      "('b', 'a'): 3\n",
      "('a', 'n'): 8\n",
      "('n', 'a'): 6\n",
      "('a', '</w>'): 3\n",
      "('o', 'r'): 2\n",
      "('r', 'a'): 2\n",
      "('n', 'g'): 2\n",
      "('g', 'e'): 2\n",
      "\n",
      "Merged Vocabulary:\n",
      "a p p l e </w>: 5\n",
      "b an an a </w>: 3\n",
      "o r an g e </w>: 2\n"
     ]
    }
   ],
   "source": [
    "# get_word_char_freqsのテスト\n",
    "test_corpus = {'apple': 5, 'banana': 3, 'orange': 2}\n",
    "word_char_freqs = get_word_char_freqs(test_corpus)\n",
    "print(\"Word-Character Frequencies:\")\n",
    "for word, freq in word_char_freqs.items():\n",
    "    print(f\"{word}: {freq}\")\n",
    "# get_statsのテスト\n",
    "stats = get_stats(word_char_freqs)\n",
    "print(\"\\nAdjacent Pairs Frequencies:\")\n",
    "for pair, freq in stats.items():\n",
    "    print(f\"{pair}: {freq}\")\n",
    "# merge_vocabのテスト\n",
    "pair_to_merge = ('a', 'n')\n",
    "merged_vocab = merge_vocab(pair_to_merge, word_char_freqs)\n",
    "print(\"\\nMerged Vocabulary:\")\n",
    "for word, freq in merged_vocab.items():\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5a0d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BPE 学習シミュレーション ---\n",
      "初期Vocab (文字分割 + </w>):\n",
      "  'l o w </w>': 5\n",
      "  'l o w e r </w>': 2\n",
      "  'n e w e s t </w>': 6\n",
      "  'w i d e s t </w>': 3\n",
      "Merge 1: ('e', 's') (freq: 9) -> es\n",
      "Merge 2: ('es', 't') (freq: 9) -> est\n",
      "Merge 3: ('est', '</w>') (freq: 9) -> est</w>\n",
      "Merge 4: ('l', 'o') (freq: 7) -> lo\n",
      "Merge 5: ('lo', 'w') (freq: 7) -> low\n",
      "Merge 6: ('n', 'e') (freq: 6) -> ne\n",
      "Merge 7: ('ne', 'w') (freq: 6) -> new\n",
      "Merge 8: ('new', 'est</w>') (freq: 6) -> newest</w>\n",
      "Merge 9: ('low', '</w>') (freq: 5) -> low</w>\n",
      "Merge 10: ('w', 'i') (freq: 3) -> wi\n",
      "\n",
      "学習されたBPEマージ操作 (優先度順):\n",
      "  ('e', 's') -> es\n",
      "  ('es', 't') -> est\n",
      "  ('est', '</w>') -> est</w>\n",
      "  ('l', 'o') -> lo\n",
      "  ('lo', 'w') -> low\n",
      "  ('n', 'e') -> ne\n",
      "  ('ne', 'w') -> new\n",
      "  ('new', 'est</w>') -> newest</w>\n",
      "  ('low', '</w>') -> low</w>\n",
      "  ('w', 'i') -> wi\n",
      "\n",
      "--- BPE セグメンテーションテスト ---\n",
      "Word: 'lowest', BPE Segments: ['low', 'est</w>']\n",
      "Word: 'newer', BPE Segments: ['new', 'e', 'r', '</w>']\n",
      "Word: 'bottom', BPE Segments: ['b', 'o', 't', 't', 'o', 'm', '</w>']\n"
     ]
    }
   ],
   "source": [
    "# BPE学習の簡易シミュレーション\n",
    "print(\"--- BPE 学習シミュレーション ---\")\n",
    "# 初期コーパス (単語と頻度)\n",
    "corpus_example = {\"low\": 5, \"lower\": 2, \"newest\": 6, \"widest\": 3}\n",
    "# 1. 文字シーケンスと頻度に変換\n",
    "vocab_bpe_learn = get_word_char_freqs(corpus_example)\n",
    "print(\"初期Vocab (文字分割 + </w>):\")\n",
    "for k,v in vocab_bpe_learn.items(): print(f\"  '{k}': {v}\")\n",
    "\n",
    "num_merges = 10 # マージ操作の回数\n",
    "bpe_merges_learned = {} # 学習されたマージ操作 (ペア -> 優先度)\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab_bpe_learn)\n",
    "    if not pairs:\n",
    "        print(\"ペアがなくなりました。\")\n",
    "        break\n",
    "    # 最も頻繁なペアを選択\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    \n",
    "    # マージ操作を保存 (優先度としてマージ順を保存)\n",
    "    bpe_merges_learned[best_pair] = i \n",
    "    \n",
    "    print(f\"Merge {i+1}: {best_pair} (freq: {pairs[best_pair]}) -> {''.join(best_pair)}\")\n",
    "    vocab_bpe_learn = merge_vocab(best_pair, vocab_bpe_learn)\n",
    "    # print(\"  Updated Vocab:\")\n",
    "    # for k,v in vocab_bpe_learn.items(): print(f\"    '{k}': {v}\")\n",
    "\n",
    "print(\"\\n学習されたBPEマージ操作 (優先度順):\")\n",
    "sorted_merges = sorted(bpe_merges_learned.items(), key=lambda item: item[1])\n",
    "for pair, priority in sorted_merges:\n",
    "    print(f\"  {pair} -> {''.join(pair)}\")\n",
    "\n",
    "# BPEセグメンテーションの簡易シミュレーション\n",
    "def segment_bpe(word, learned_merges_sorted_by_priority):\n",
    "    \"\"\" 学習されたマージ操作を使って単語をBPEセグメントに分割 \"\"\"\n",
    "    # まず文字に分割 + </w>\n",
    "    symbols = list(word) + ['</w>']\n",
    "    \n",
    "    # 優先度の高いマージ操作から順に適用していく\n",
    "    # 実際には、最も頻繁なペアがなくなるまで、あるいは現在のセグメントに対して\n",
    "    # 適用可能な最も優先度の高いマージ操作を繰り返し適用する\n",
    "    \n",
    "    # ここでは非常に単純化:\n",
    "    # 優先度順にマージルールを適用していく。\n",
    "    # より正しいのは、現在のシンボル列に対して、学習済みマージ操作リストの中から\n",
    "    # 適用可能な最も優先度の高い（最も早く学習された）ペアを繰り返しマージすること。\n",
    "    \n",
    "    # 今回は、学習されたマージ操作のリストを「置き換えルール」として使う\n",
    "    # (これはBPEの真のセグメンテーションとは異なるが、概念を示すため)\n",
    "    \n",
    "    # より論文に近いセグメンテーション:\n",
    "    # 1. 単語を文字のシーケンスにする: 'l o w e r </w>'\n",
    "    # 2. 学習済みのマージ操作のリスト（優先度順）を取得\n",
    "    # 3. シーケンス内で、マージ操作リストに含まれる最も優先度の高いペアを見つけてマージ\n",
    "    # 4. マージできるペアがなくなるまで繰り返す\n",
    "    \n",
    "    word_str_with_spaces = ' '.join(symbols)\n",
    "    # print(f\"Initial segmentation for '{word}': '{word_str_with_spaces}'\")\n",
    "\n",
    "    # 優先度の高いマージから順に適用 (実際にはもっと効率的な方法がある)\n",
    "    # このループはBPEの正しいセグメンテーションではない。\n",
    "    # 正しくは、現在の状態から適用可能な最も良いマージを選ぶ。\n",
    "    # 下記は「学習されたマージ操作を使って新しい単語をどう処理するか」のイメージ。\n",
    "\n",
    "    # 正しいBPEセグメンテーションの考え方：\n",
    "    # 1. 単語を文字のリストにする。\n",
    "    # 2. 現在の文字リストから隣接ペアの頻度を数える（実際は学習済みマージリストを見る）。\n",
    "    # 3. 学習済みマージリストの中で、現在の文字リストに存在する最も優先度の高いペアをマージする。\n",
    "    # 4. マージできなくなるまで繰り返す。\n",
    "    \n",
    "    current_segments = list(word) + ['</w>']\n",
    "    \n",
    "    while True:\n",
    "        min_priority_found = float('inf')\n",
    "        best_pair_to_merge_idx = -1\n",
    "        \n",
    "        # 現在のセグメント列でマージ可能な最も優先度の高いペアを探す\n",
    "        for k in range(len(current_segments) - 1):\n",
    "            pair = (current_segments[k], current_segments[k+1])\n",
    "            if pair in bpe_merges_learned:\n",
    "                priority = bpe_merges_learned[pair]\n",
    "                if priority < min_priority_found:\n",
    "                    min_priority_found = priority\n",
    "                    best_pair_to_merge_idx = k\n",
    "        \n",
    "        if best_pair_to_merge_idx != -1: # マージ可能なペアが見つかった\n",
    "            merged_segment = current_segments[best_pair_to_merge_idx] + current_segments[best_pair_to_merge_idx+1]\n",
    "            current_segments = current_segments[:best_pair_to_merge_idx] + \\\n",
    "                               [merged_segment] + \\\n",
    "                               current_segments[best_pair_to_merge_idx+2:]\n",
    "            # print(f\"  Merged to: {current_segments}\")\n",
    "        else: # これ以上マージできない\n",
    "            break\n",
    "            \n",
    "    return current_segments\n",
    "\n",
    "\n",
    "print(\"\\n--- BPE セグメンテーションテスト ---\")\n",
    "# 学習されたマージ操作 (タプルキー -> 優先度)\n",
    "learned_merges_dict = {pair: prio for pair, prio in sorted_merges}\n",
    "\n",
    "test_word1 = \"lowest\"\n",
    "segments1 = segment_bpe(test_word1, learned_merges_dict)\n",
    "print(f\"Word: '{test_word1}', BPE Segments: {segments1}\") # 例: ['low', 'est</w>'] や ['lowe', 'st</w>'] など\n",
    "\n",
    "test_word2 = \"newer\" # 訓練データにない\n",
    "segments2 = segment_bpe(test_word2, learned_merges_dict)\n",
    "print(f\"Word: '{test_word2}', BPE Segments: {segments2}\") # 例: ['new', 'er</w>']\n",
    "\n",
    "test_word3 = \"bottom\" # 語彙にない文字が含まれる可能性 (今回はなし)\n",
    "segments3 = segment_bpe(test_word3, learned_merges_dict)\n",
    "print(f\"Word: '{test_word3}', BPE Segments: {segments3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a46f30",
   "metadata": {},
   "source": [
    "### 3.3 BPEの長所と短所\n",
    "\n",
    "*   **長所:**\n",
    "    *   シンプルで効果的なアルゴリズム。\n",
    "    *   データ駆動でサブワード語彙を自動的に学習できる。\n",
    "    *   語彙サイズを柔軟に制御できる（マージ回数で調整）。\n",
    "    *   未知語に対しても、文字レベルまで分解することで何らかの表現を与えることができる。\n",
    "*   **短所:**\n",
    "    *   **貪欲アルゴリズム:** 各ステップで最も頻繁なペアをマージするため、必ずしも全体として最適なサブワード分割が得られるとは限らない。\n",
    "    *   **意味的な考慮の欠如:** 純粋にペアの出現頻度に基づいてマージするため、意味的に不自然なサブワードが生成されることがある（例: \"ing\" と \"ly\" が別々に学習されても、\"ingly\" が一つの単位として学習されるとは限らない）。\n",
    "    *   **プリトークナイゼーションへの依存:** 元のBPE論文（NLP応用版）では、まずテキストを単語に分割し、その単語の境界を越えないようにBPEを適用することが多い。これは言語依存のプリトークナイザが必要になることを意味する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fa60d",
   "metadata": {},
   "source": [
    "## 4. WordPiece (Schuster & Nakajima, 2012; Wu et al., 2016)\n",
    "\n",
    "WordPieceは、Googleのニューラル機械翻訳システム (GNMT) やBERTで採用されているサブワード分割アルゴリズムです。\n",
    "基本的な考え方はBPEと似ていますが、ペアをマージする基準が異なります。\n",
    "\n",
    "*   **BPEとの主な違い:**\n",
    "    *   **マージ基準:** BPEが最も**出現頻度の高い**隣接ペアをマージするのに対し、WordPieceは、マージすることで訓練データの**尤度 (Likelihood)** を最も増加させるペアをマージします。\n",
    "    *   **初期語彙:** WordPieceも初期語彙として全ての文字を含みます。\n",
    "    *   **処理:**\n",
    "        1.  初期語彙と訓練データで言語モデルを構築。\n",
    "        2.  現在の語彙を使って、訓練データを最も尤もらしく表現できるペアを見つけ、そのペアを新しいサブワードとして語彙に追加する。\n",
    "        3.  指定された語彙サイズに達するまで繰り返す。\n",
    "\n",
    "*   **利点:**\n",
    "    *   言語モデルの尤度を最大化するようにサブワードが学習されるため、より言語的に自然で、下流タスクの性能向上に繋がりやすいと考えられています。\n",
    "*   **実装:**\n",
    "    *   WordPieceの学習アルゴリズムはBPEよりも複雑で、通常はGoogleが提供するツールや、Hugging Face Tokenizersライブラリなどに含まれる実装が利用されます。\n",
    "    *   スクラッチでの完全な実装は、このノートブックの範囲を超えます。\n",
    "\n",
    "WordPieceは、単語の先頭に特別なプレフィックス（例: `##`）を付けて、それが単語の途中から始まるサブワードであることを示すことが多いです（例: \"playing\" -> `[\"play\", \"##ing\"]`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17583eaf",
   "metadata": {},
   "source": [
    "## 5. SentencePiece (Kudo & Richardson, 2018)\n",
    "\n",
    "SentencePieceは、GoogleのTaku Kudo氏らによって開発された、言語非依存のサブワード分割器および脱トークン化器です。\n",
    "BPEやWordPieceが抱えるいくつかの課題、特にプリトークナイゼーションへの依存を解決することを目指しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502f467",
   "metadata": {},
   "source": [
    "### 5.1 SentencePieceの主要な特徴と設計思想\n",
    "\n",
    "*   **言語非依存性 (No Pre-tokenization):**\n",
    "    *   SentencePieceは、**生の文（raw sentences）から直接サブワードモデルを学習します。** 事前にテキストを単語に分割する（プリトークナイズする）必要がありません。\n",
    "    *   これにより、空白で単語を区切らない言語（日本語、中国語、タイ語など）や、複雑な複合語を持つ言語（ドイツ語、フィンランド語など）にも統一的に適用できます。\n",
    "*   **可逆的なトークナイゼーション (Lossless Tokenization):**\n",
    "    *   SentencePieceは、トークン化されたサブワードシーケンスから元の正規化された文を**完全に復元**できるように設計されています。\n",
    "    *   これを実現するために、**空白文字も通常の文字と同様に扱い**、必要であれば特別なメタシンボル（デフォルトでは ` ` (U+2581) という記号で空白を表す）に置き換えてからサブワード分割を行います。\n",
    "    *   デトークナイズ（脱トークン化）は、単純にサブワードを結合し、メタシンボル ` ` を実際の空白文字に戻すだけで完了します。\n",
    "*   **2つのサブワード分割アルゴリズムをサポート:**\n",
    "    *   **BPE:** Sennrichらのアルゴリズムに基づいています。\n",
    "    *   **Unigram Language Model Tokenizer (Kudo, 2018):**\n",
    "        *   全ての可能なサブワード分割に対して、それぞれの分割の確率（ユニグラム言語モデルで計算）を求め、全体の尤度が最大になるような分割を選択します。\n",
    "        *   学習時には、まず非常に大きなサブワード候補の集合から始め、期待される損失増加が最も小さいサブワードを徐々に削除していくことで、最終的な語彙サイズに収めます。\n",
    "*   **自己完結型のモデルファイル:**\n",
    "    *   学習されたサブワードモデル（語彙、マージルール、正規化ルールなど）は、単一のモデルファイルに保存されます。これにより、再現性が高く、モデルの配布も容易になります。\n",
    "*   **サブワード正則化 (Subword Regularization) - オプション:**\n",
    "    *   SentencePieceは、訓練時に複数の可能なサブワード分割を確率的にサンプリングする「サブワード正則化」というテクニックをサポートしています（主にUnigram LM Tokenizerの場合）。これにより、モデルの頑健性が向上し、過学習を抑制する効果が期待されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eeb73",
   "metadata": {},
   "source": [
    "### 5.2 SentencePieceライブラリの基本的な使い方\n",
    "ここでは、Pythonライブラリ `sentencepiece` を使った簡単な例を示します。\n",
    "まず、`pip install sentencepiece` でライブラリをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bac875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SentencePiece ライブラリテスト ---\n",
      "\n",
      "SentencePiece BPEモデル 'my_sp_model.model' の学習完了。\n",
      "学習済みモデルのロード完了。\n",
      "\n",
      "Text: 'This is a test for SentencePiece.'\n",
      "  Pieces: ['▁This', '▁is', '▁a', '▁te', 'st', '▁for', '▁SentencePiece', '.']\n",
      "  IDs: [98, 23, 49, 221, 46, 88, 26, 264]\n",
      "\n",
      "Text: 'ニューヨークは大きな都市です。'\n",
      "  Pieces: ['▁', 'ニューヨーク', 'は', '大きな都市で', 'す', '。']\n",
      "  IDs: [253, 0, 290, 0, 287, 276]\n",
      "\n",
      "OOV Text: 'NewUnseenWord'\n",
      "  Pieces: ['▁New', 'U', 'n', 'se', 'en', 'W', 'ord']\n",
      "  IDs: [83, 0, 255, 155, 3, 0, 70]\n",
      "\n",
      "Decoded IDs [98, 23, 49, 221, 46, 88, 26, 264] -> 'This is a test for SentencePiece.'\n",
      "Decoded IDs [253, 0, 290, 0, 287, 276] -> ' ⁇ は ⁇ す。'\n",
      "\n",
      "Subword Regularization (Sampling for BPE - 概念):\n",
      "  Sampled pieces for 'This is a test for SentencePiece.': ['▁This', '▁is', '▁a', '▁te', 'st', '▁f', 'or', '▁S', 'ent', 'en', 'ce', 'Piece', '.']\n",
      "  Sampled pieces for 'This is a test for SentencePiece.': ['▁This', '▁is', '▁a', '▁te', 'st', '▁f', 'o', 'r', '▁S', 'ent', 'ence', 'Piece', '.']\n",
      "  Sampled pieces for 'This is a test for SentencePiece.': ['▁This', '▁is', '▁a', '▁t', 'e', 'st', '▁', 'for', '▁', 'S', 'entence', 'Piece', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"--- SentencePiece ライブラリテスト ---\")\n",
    "    \n",
    "# 0. ダミーの訓練データファイルを作成 (1行1文のテキストファイル)\n",
    "dummy_corpus_file = \"dummy_corpus_for_spm.txt\"\n",
    "with open(dummy_corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"This is the first sentence for SentencePiece.\\n\")\n",
    "    f.write(\"SentencePiece can handle raw text directly.\\n\")\n",
    "    f.write(\"こんにちは世界。日本語も扱えます。\\n\") # 日本語の例\n",
    "    f.write(\"New York is a big city.\\n\")\n",
    "    f.write(\"I love natural language processing and subword tokenization.\\n\")\n",
    "\n",
    "# 1. SentencePieceモデルの学習\n",
    "# --input: 訓練データファイル\n",
    "# --model_prefix: 出力されるモデルファイル名のプレフィックス (例: my_sp_model)\n",
    "#                 (my_sp_model.model と my_sp_model.vocab が生成される)\n",
    "# --vocab_size: 最終的な語彙サイズ\n",
    "# --model_type: bpe または unigram (デフォルト), char, word\n",
    "# --character_coverage: 基本文字セットのカバー率 (デフォルト0.9995)\n",
    "# --bos_id=1, --eos_id=2, --unk_id=0, --pad_id=-1 (デフォルトは-1だが、モデルによって調整)\n",
    "    \n",
    "model_prefix = 'my_sp_model'\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f'--input={dummy_corpus_file} --model_prefix={model_prefix} '\n",
    "    f'--vocab_size=300 --model_type=bpe '\n",
    "    f'--bos_id=1 --eos_id=2 --unk_id=0 --pad_id=-1 ' # PyTorchのEmbeddingはpad_id=0を期待することが多いので注意\n",
    "    f'--character_coverage=1.0' # 全ての文字をカバー\n",
    ")\n",
    "print(f\"\\nSentencePiece BPEモデル '{model_prefix}.model' の学習完了。\")\n",
    "\n",
    "# 学習済みモデルのロード\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{model_prefix}.model')\n",
    "print(\"学習済みモデルのロード完了。\")\n",
    "\n",
    "# 2. テキストのエンコード (サブワード分割 + ID化)\n",
    "text1 = \"This is a test for SentencePiece.\"\n",
    "ids1 = sp.encode_as_ids(text1)\n",
    "pieces1 = sp.encode_as_pieces(text1)\n",
    "print(f\"\\nText: '{text1}'\")\n",
    "print(f\"  Pieces: {pieces1}\")\n",
    "print(f\"  IDs: {ids1}\")\n",
    "\n",
    "text2 = \"ニューヨークは大きな都市です。\" # 日本語の例\n",
    "ids2 = sp.encode_as_ids(text2)\n",
    "pieces2 = sp.encode_as_pieces(text2)\n",
    "print(f\"\\nText: '{text2}'\")\n",
    "print(f\"  Pieces: {pieces2}\")\n",
    "print(f\"  IDs: {ids2}\")\n",
    "        \n",
    "# 未知語の扱い (語彙に含まれない文字シーケンス)\n",
    "text_oov = \"NewUnseenWord\" # 語彙にない可能性が高い\n",
    "pieces_oov = sp.encode_as_pieces(text_oov)\n",
    "ids_oov = sp.encode_as_ids(text_oov)\n",
    "print(f\"\\nOOV Text: '{text_oov}'\")\n",
    "print(f\"  Pieces: {pieces_oov}\") # 文字単位か、<unk> になるか (char_coverageによる)\n",
    "print(f\"  IDs: {ids_oov}\")\n",
    "\n",
    "\n",
    "# 3. IDシーケンスのデコード (元のテキストに戻す)\n",
    "decoded_text1 = sp.decode_ids(ids1)\n",
    "decoded_text2 = sp.decode_ids(ids2)\n",
    "print(f\"\\nDecoded IDs {ids1} -> '{decoded_text1}'\")\n",
    "print(f\"Decoded IDs {ids2} -> '{decoded_text2}'\")\n",
    "        \n",
    "# 4. サブワード正則化 (N-best segmentation / sampling)\n",
    "#    model_type=unigram で学習した場合に特に効果的\n",
    "#    sp.sample_encode_as_pieces() や sp.sample_encode_as_ids()\n",
    "print(\"\\nSubword Regularization (Sampling for BPE - 概念):\")\n",
    "for _ in range(3): # 3回サンプリング\n",
    "    # BPEモデルではsample_encodeは決定論的だが、unigramなら確率的になる\n",
    "    sampled_pieces = sp.encode_as_pieces(text1, enable_sampling=True, alpha=0.1, nbest_size=-1)\n",
    "    print(f\"  Sampled pieces for '{text1}': {sampled_pieces}\")\n",
    "\n",
    "import os\n",
    "if os.path.exists(dummy_corpus_file): os.remove(dummy_corpus_file)\n",
    "if os.path.exists(f\"{model_prefix}.model\"): os.remove(f\"{model_prefix}.model\")\n",
    "if os.path.exists(f\"{model_prefix}.vocab\"): os.remove(f\"{model_prefix}.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b330a7",
   "metadata": {},
   "source": [
    "## 6. 各サブワード分割手法の比較と考察\n",
    "\n",
    "| 特徴                       | BPE (Byte Pair Encoding)                                  | WordPiece                                                   | SentencePiece                                                                 |\n",
    "| :------------------------- | :-------------------------------------------------------- | :---------------------------------------------------------- | :---------------------------------------------------------------------------- |\n",
    "| **基本アイデア**             | 最も頻繁な隣接ペアをマージ                                  | マージで言語モデル尤度が最大になるペアをマージ                  | 生のテキストから直接学習。空白も文字として扱う。BPEまたはUnigram LMを選択可能。 |\n",
    "| **プリトークナイゼーション** | 通常必要（単語境界を越えないように）                        | 通常必要                                                      | **不要（言語非依存）**                                                        |\n",
    "| **語彙生成**             | データ駆動（マージ回数で語彙サイズ制御）                   | データ駆動（尤度基準で語彙サイズ制御）                        | データ駆動（指定した語彙サイズになるように学習）                                |\n",
    "| **未知語(OOV)対応**        | 文字レベルまで分解可能                                      | 文字レベルまで分解可能、または特別なサブワード                  | 文字レベルまで分解可能、または`<unk>`トークン                                    |\n",
    "| **可逆性**                 | プリトークナイザとBPEモデルに依存（工夫すれば可逆にも）         | プリトークナイザとモデルに依存                                  | **完全に可逆（Lossless）**                                                    |\n",
    "| **主な利用例**             | 初期のNMT、GPT-2など                                     | BERT、GoogleのNMT (GNMT)など                               | Googleの多くのNLPモデル、多言語モデル、T5など                                   |\n",
    "| **トークン化の仕方**       | ` ` (スペース) で単語を区切り、単語内でBPEを適用。単語末尾に`</w>`など。 | 単語内で分割し、継続サブワードに`##`プレフィックス。          | 空白を`_` (U+2581) に置き換え、文全体をサブワードに分割。                       |\n",
    "| **実装**                   | 比較的シンプル                                            | BPEより複雑（尤度計算が絡む）                                 | C++ライブラリとして提供。Python APIあり。                                       |\n",
    "\n",
    "**考察:**\n",
    "*   **BPE** はシンプルで理解しやすく、多くの場面で効果的なサブワード分割手法です。\n",
    "*   **WordPiece** は言語モデルの尤度を考慮するため、より言語的に自然な分割が得られると期待されますが、アルゴリズムは若干複雑です。\n",
    "*   **SentencePiece** は、プリトークナイゼーションが不要で、言語非依存的に扱えるという大きな利点があります。また、Unigram Language Modelベースの分割やサブワード正則化といった高度な機能も提供しており、現代の多くの最先端モデルで採用されています。可逆性も保証されているため、デトークナイズが容易です。\n",
    "\n",
    "どの手法を選択するかは、タスクの要件、対象言語、利用可能なツール、計算資源などによって異なります。\n",
    "現代の多言語対応やEnd-to-End学習の流れを考えると、SentencePieceのような言語非依存的で自己完結型のツールは非常に強力です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
