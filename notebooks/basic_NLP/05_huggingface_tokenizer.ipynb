{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580f2d5a",
   "metadata": {},
   "source": [
    "# NLP基礎 (補足): Hugging Face Tokenizers体験\n",
    "\n",
    "これまでのノートブックで、サブワード分割の重要性と、BPE、WordPiece、SentencePieceといった代表的なアルゴリズムの基本的な考え方について学びました。\n",
    "実際の研究や開発では、これらのアルゴリズムを効率的に利用するために、Hugging Face `transformers` ライブラリや `tokenizers` ライブラリのような高機能なツールが広く使われています。\n",
    "\n",
    "このノートブックでは、Hugging Face `transformers` ライブラリを使って、いくつかの有名な事前学習済みモデルに付属するTokenizerを実際に動かし、その動作を確認します。\n",
    "これにより、サブワード分割が実際のモデルでどのように機能しているかの具体的なイメージを掴むことを目的とします。\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  Hugging Face `transformers` ライブラリの基本的なTokenizerの使い方。\n",
    "2.  異なる事前学習済みモデル（例: BERT, GPT-2, T5）が採用しているサブワード分割戦略の比較。\n",
    "3.  トークナイズ結果（トークン、ID、アテンションマスクなど）の確認。\n",
    "4.  特殊トークン（`[CLS]`, `[SEP]`, `<|endoftext|>`など）の役割。\n",
    "\n",
    "**前提知識:**\n",
    "*   サブワード分割（BPE, WordPiece, SentencePiece）の基本的な概念の理解（NLP基礎(4)のノートブック）。\n",
    "*   Pythonの基本的な操作。\n",
    "*   (推奨) Hugging Face Transformersライブラリの概要についての知識（なくても進められます）。\n",
    "\n",
    "**準備:**\n",
    "Hugging Face `transformers` ライブラリと、それが依存する `tokenizers` ライブラリが必要です。\n",
    "まだインストールしていない場合は、以下のコマンドでインストールしてください。\n",
    "`pip install transformers tokenizers sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f98c82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.0+cu124\n",
      "Hugging Face Transformers and Tokenizers are expected to be installed.\n"
     ]
    }
   ],
   "source": [
    "import torch # Hugging FaceのTokenizerはPyTorchテンソルも扱える\n",
    "from transformers import AutoTokenizer, BertTokenizer, GPT2Tokenizer, T5Tokenizer\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(\"Hugging Face Transformers and Tokenizers are expected to be installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e409636",
   "metadata": {},
   "source": [
    "## 2. 様々な事前学習済みモデルのTokenizerを試す\n",
    "\n",
    "Hugging Face `transformers` ライブラリの `AutoTokenizer` を使うと、モデル名を指定するだけで適切なTokenizerを自動的にロードできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ce190",
   "metadata": {},
   "source": [
    "### 2.1 BERT Tokenizer (WordPieceベース)\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) は、WordPieceというサブワード分割アルゴリズムを使用しています。\n",
    "WordPieceは、単語をより頻繁に出現する部分文字列に分割し、単語の先頭でないサブワードには通常 `##` というプレフィックスを付けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835444ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BERT Tokenizer (WordPiece) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee73c77f379e4013b4a64d85b2498bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syuuy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\syuuy\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb759bdee22145b7bc916d6c01fd7f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c727972a7e3c44cea904f9800ee5e8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2c95a4ae74fe9bcf063386480a4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'bert-base-uncased' のTokenizerをロードしました。\n",
      "\n",
      "Sample text: 'This is an example of using the BERT tokenizer with subword units, like 'tokenization'.'\n",
      "BERT Tokens: ['this', 'is', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer', 'with', 'sub', '##word', 'units', ',', 'like', \"'\", 'token', '##ization', \"'\", '.']\n",
      "\n",
      "BERT Encoded Input IDs:\n",
      " tensor([[  101,  2023,  2003,  2019,  2742,  1997,  2478,  1996, 14324, 19204,\n",
      "         17629,  2007,  4942, 18351,  3197,  1010,  2066,  1005, 19204,  3989,\n",
      "          1005,  1012,   102]])\n",
      "Corresponding Tokens (manual check):\n",
      "['[CLS]', 'this', 'is', 'an', 'example', 'of', 'using', 'the', 'bert', 'token', '##izer', 'with', 'sub', '##word', 'units', ',', 'like', \"'\", 'token', '##ization', \"'\", '.', '[SEP]']\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Decoded text (from IDs, skipping special tokens): 'this is an example of using the bert tokenizer with subword units, like ' tokenization '.'\n",
      "\n",
      "Text with unknown word: 'A very newwwwwoooord.'\n",
      "BERT Tokens: ['a', 'very', 'new', '##w', '##w', '##w', '##wo', '##oo', '##ord', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"--- BERT Tokenizer (WordPiece) ---\")\n",
    "# 一般的なBERTの事前学習済みモデル名を指定\n",
    "bert_model_name = 'bert-base-uncased' # 小文字化するBERTモデル\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "print(f\"'{bert_model_name}' のTokenizerをロードしました。\")\n",
    "\n",
    "sample_text_bert = \"This is an example of using the BERT tokenizer with subword units, like 'tokenization'.\"\n",
    "\n",
    "# テキストをトークンに分割\n",
    "bert_tokens = bert_tokenizer.tokenize(sample_text_bert)\n",
    "print(f\"\\nSample text: '{sample_text_bert}'\")\n",
    "print(f\"BERT Tokens: {bert_tokens}\")\n",
    "\n",
    "# テキストをトークンIDに変換 (エンコード)\n",
    "# add_special_tokens=True にすると、[CLS]や[SEP]が付加される\n",
    "bert_encoded_input = bert_tokenizer.encode_plus(\n",
    "    sample_text_bert, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, # アテンションマスクも取得\n",
    "    return_tensors='pt' # PyTorchテンソルで返す\n",
    ")\n",
    "bert_input_ids = bert_encoded_input['input_ids']\n",
    "bert_attention_mask = bert_encoded_input['attention_mask']\n",
    "    \n",
    "print(\"\\nBERT Encoded Input IDs:\\n\", bert_input_ids)\n",
    "print(\"Corresponding Tokens (manual check):\")\n",
    "print(bert_tokenizer.convert_ids_to_tokens(bert_input_ids.squeeze().tolist()))\n",
    "print(\"Attention Mask:\\n\", bert_attention_mask) # パディングがないので全て1\n",
    "\n",
    "# IDシーケンスを元のテキストに戻す (デコード)\n",
    "# skip_special_tokens=True にすると、[CLS]や[SEP]が除去される\n",
    "decoded_text_bert = bert_tokenizer.decode(bert_input_ids.squeeze(), skip_special_tokens=True)\n",
    "print(f\"\\nDecoded text (from IDs, skipping special tokens): '{decoded_text_bert}'\")\n",
    "    \n",
    "# 未知語の扱い\n",
    "# BERTの語彙にない単語は [UNK] トークンになる\n",
    "unknown_text_bert = \"A very newwwwwoooord.\"\n",
    "unknown_tokens_bert = bert_tokenizer.tokenize(unknown_text_bert)\n",
    "print(f\"\\nText with unknown word: '{unknown_text_bert}'\")\n",
    "print(f\"BERT Tokens: {unknown_tokens_bert}\") # 'newwwwwoooord' が [UNK] になるはず"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ef29",
   "metadata": {},
   "source": [
    "**BERT Tokenizerのポイント:**\n",
    "*   `bert-base-uncased` は、入力テキストを小文字化してからトークナイズします。\n",
    "*   単語が語彙にない場合や、頻度の低いサブシーケンスは、より細かいサブワード（最終的には文字単位や `[UNK]` トークン）に分割されます。\n",
    "*   `encode_plus` メソッドは、トークンIDだけでなく、アテンションマスクやトークンタイプID（NSP用）など、モデルへの入力に必要な情報をまとめて生成できます。\n",
    "*   特殊トークン `[CLS]` はシーケンスの開始を表し、通常、文全体の表現ベクトルとして利用されます。`[SEP]` は文の区切りを示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab4dce",
   "metadata": {},
   "source": [
    "### 2.2 GPT-2 Tokenizer (BPEベース)\n",
    "\n",
    "GPT-2 (Generative Pre-trained Transformer 2) は、Byte Pair Encoding (BPE) に基づくサブワード分割を使用しています。\n",
    "BPEは、最も頻繁に出現するバイト（文字）ペアを繰り返しマージしていくことで語彙を構築します。\n",
    "GPT-2のTokenizerは、単語の先頭を示すために特別な記号（例: `Ġ`、スペースのメタキャラクタ）を使うことがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdee1812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GPT-2 Tokenizer (BPE) ---\n",
      "'gpt2' のTokenizerをロードしました。\n",
      "GPT-2 Tokenizerに [PAD] トークンを追加しました。\n",
      "\n",
      "Sample text: 'This is an example for the GPT-2 tokenizer, which uses BPE tokenization.'\n",
      "GPT-2 Tokens: ['This', 'Ġis', 'Ġan', 'Ġexample', 'Ġfor', 'Ġthe', 'ĠG', 'PT', '-', '2', 'Ġtoken', 'izer', ',', 'Ġwhich', 'Ġuses', 'ĠB', 'PE', 'Ġtoken', 'ization', '.']\n",
      "\n",
      "GPT-2 Encoded Input IDs (padded/truncated):\n",
      " tensor([[ 1212,   318,   281,  1672,   329,   262,   402, 11571,    12,    17,\n",
      "         11241,  7509,    11,   543,  3544,   347, 11401, 11241,  1634,    13]])\n",
      "Corresponding Tokens:\n",
      "['This', 'Ġis', 'Ġan', 'Ġexample', 'Ġfor', 'Ġthe', 'ĠG', 'PT', '-', '2', 'Ġtoken', 'izer', ',', 'Ġwhich', 'Ġuses', 'ĠB', 'PE', 'Ġtoken', 'ization', '.']\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Decoded text (from IDs, with special tokens if any): 'This is an example for the GPT-2 tokenizer, which uses BPE tokenization.'\n",
      "Decoded text (from IDs, skipping special tokens): 'This is an example for the GPT-2 tokenizer, which uses BPE tokenization.'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- GPT-2 Tokenizer (BPE) ---\")\n",
    "gpt2_model_name = 'gpt2' # 基本的なGPT-2モデル\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "print(f\"'{gpt2_model_name}' のTokenizerをロードしました。\")\n",
    "\n",
    "# GPT-2は特殊なパディングトークンを持たないことがあるので、追加する\n",
    "if gpt2_tokenizer.pad_token is None:\n",
    "    gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"GPT-2 Tokenizerに [PAD] トークンを追加しました。\")\n",
    "\n",
    "\n",
    "sample_text_gpt2 = \"This is an example for the GPT-2 tokenizer, which uses BPE tokenization.\"\n",
    "    \n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(sample_text_gpt2)\n",
    "print(f\"\\nSample text: '{sample_text_gpt2}'\")\n",
    "print(f\"GPT-2 Tokens: {gpt2_tokens}\")\n",
    "# 'Ġ' が単語の開始（スペースを含む）を示していることに注目\n",
    "\n",
    "# エンコード\n",
    "gpt2_encoded_input = gpt2_tokenizer(\n",
    "    sample_text_gpt2,\n",
    "    add_special_tokens=True, # GPT-2は通常、文頭に<|endoftext|>などを自動で追加しない\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    "    padding=True, # バッチ処理を考慮してパディングを有効にする例\n",
    "    truncation=True,\n",
    "    max_length=30 # 例として最大長を設定\n",
    ")\n",
    "gpt2_input_ids = gpt2_encoded_input['input_ids']\n",
    "gpt2_attention_mask = gpt2_encoded_input['attention_mask']\n",
    "\n",
    "print(\"\\nGPT-2 Encoded Input IDs (padded/truncated):\\n\", gpt2_input_ids)\n",
    "print(\"Corresponding Tokens:\")\n",
    "print(gpt2_tokenizer.convert_ids_to_tokens(gpt2_input_ids.squeeze().tolist()))\n",
    "print(\"Attention Mask:\\n\", gpt2_attention_mask)\n",
    "    \n",
    "# デコード\n",
    "# GPT-2の出力は通常、<|endoftext|>のようなEOSトークンで終わる\n",
    "decoded_text_gpt2 = gpt2_tokenizer.decode(gpt2_input_ids.squeeze(), skip_special_tokens=False) # 特殊トークンも表示\n",
    "print(f\"\\nDecoded text (from IDs, with special tokens if any): '{decoded_text_gpt2}'\")\n",
    "    \n",
    "decoded_text_gpt2_skip = gpt2_tokenizer.decode(gpt2_input_ids.squeeze(), skip_special_tokens=True) # 特殊トークンを除去\n",
    "print(f\"Decoded text (from IDs, skipping special tokens): '{decoded_text_gpt2_skip}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce1c10e",
   "metadata": {},
   "source": [
    "**GPT-2 Tokenizerのポイント:**\n",
    "*   GPT-2のBPEは、文字レベルから始まり、頻出するバイトペアをマージしていきます。\n",
    "*   トークン化された結果では、単語の区切りや単語内での分割が、BERTのWordPieceとは異なる形で見られます（例: `Ġ` プレフィックス）。\n",
    "*   GPT-2は主にテキスト生成に使われるため、デコード時には生成されたIDシーケンスを人間が読めるテキストに戻すことが重要になります。\n",
    "*   事前学習済みモデルによっては、パディングトークンが明示的に設定されていない場合があるため、バッチ処理などを行う際には手動で追加設定することがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3708ba91",
   "metadata": {},
   "source": [
    "### 2.3 T5 Tokenizer (SentencePieceベース)\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer) は、SentencePieceというサブワード分割アルゴリズムを使用しています。\n",
    "SentencePieceは、言語非依存であり、生のテキストから直接サブワードモデルを学習できる特徴があります。空白文字も通常の文字と同様に扱い、メタシンボル（例: ` ` (U+2581)）で表現します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c7cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- T5 Tokenizer (SentencePiece) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12737a74a4b047589098a6cf27cf00a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syuuy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\syuuy\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e562c11b62464baaa5f4b6a95ef121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5c0e97d13d467ea82aadf17ed58b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t5-small' のTokenizerをロードしました。\n",
      "\n",
      "Sample text: 'SentencePiece makes subword tokenization language-independent. こんにちは。'\n",
      "T5 Tokens (SentencePiece): ['▁Sen', 't', 'ence', 'P', 'i', 'e', 'ce', '▁makes', '▁sub', 'word', '▁token', 'ization', '▁language', '-', 'in', 'dependent', '.', '▁', 'こんにちは。']\n",
      "\n",
      "T5 Encoded Input IDs (padded/truncated):\n",
      " tensor([[ 4892,    17,  1433,   345,    23,    15,   565,   656,   769,  6051,\n",
      "         14145,  1707,  1612,    18,    77, 17631,     5,     3,     2,     1]])\n",
      "Corresponding Tokens:\n",
      "['▁Sen', 't', 'ence', 'P', 'i', 'e', 'ce', '▁makes', '▁sub', 'word', '▁token', 'ization', '▁language', '-', 'in', 'dependent', '.', '▁', '<unk>', '</s>']\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Decoded text (from IDs, with special tokens): 'SentencePiece makes subword tokenization language-independent. <unk></s>'\n",
      "Decoded text (from IDs, skipping special tokens): 'SentencePiece makes subword tokenization language-independent. '\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- T5 Tokenizer (SentencePiece) ---\")\n",
    "t5_model_name = 't5-small' # T5の小さなバリアント\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "print(f\"'{t5_model_name}' のTokenizerをロードしました。\")\n",
    "\n",
    "sample_text_t5 = \"SentencePiece makes subword tokenization language-independent. こんにちは。\"\n",
    "    \n",
    "t5_tokens = t5_tokenizer.tokenize(sample_text_t5)\n",
    "print(f\"\\nSample text: '{sample_text_t5}'\")\n",
    "print(f\"T5 Tokens (SentencePiece): {t5_tokens}\")\n",
    "# ' ' (U+2581) が単語の開始（スペースを含む）を示していることに注目\n",
    "# 日本語もサブワードに分割される\n",
    "\n",
    "# エンコード (T5はタスクごとにプレフィックスを付けることが多いが、ここでは単純なエンコード)\n",
    "t5_encoded_input = t5_tokenizer(\n",
    "    sample_text_t5,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=30\n",
    ")\n",
    "t5_input_ids = t5_encoded_input['input_ids']\n",
    "t5_attention_mask = t5_encoded_input['attention_mask']\n",
    "\n",
    "print(\"\\nT5 Encoded Input IDs (padded/truncated):\\n\", t5_input_ids)\n",
    "print(\"Corresponding Tokens:\")\n",
    "print(t5_tokenizer.convert_ids_to_tokens(t5_input_ids.squeeze().tolist()))\n",
    "print(\"Attention Mask:\\n\", t5_attention_mask)\n",
    "    \n",
    "# デコード\n",
    "# T5は通常、シーケンスの終わりにEOSトークン (</s>) を付加する\n",
    "decoded_text_t5 = t5_tokenizer.decode(t5_input_ids.squeeze(), skip_special_tokens=False)\n",
    "print(f\"\\nDecoded text (from IDs, with special tokens): '{decoded_text_t5}'\")\n",
    "    \n",
    "decoded_text_t5_skip = t5_tokenizer.decode(t5_input_ids.squeeze(), skip_special_tokens=True)\n",
    "print(f\"Decoded text (from IDs, skipping special tokens): '{decoded_text_t5_skip}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa96e69",
   "metadata": {},
   "source": [
    "**T5 Tokenizer (SentencePiece) のポイント:**\n",
    "*   SentencePieceは、生のテキストから直接学習するため、言語固有の前処理（単語分割など）が不要です。\n",
    "*   空白文字を特別なメタシンボル ` ` (U+2581) として扱うことで、トークナイズ結果から元の文を完全に復元できます（Lossless Tokenization）。\n",
    "*   BPEアルゴリズムまたはUnigram Language Modelアルゴリズムに基づいてサブワード語彙を構築できます（T5はUnigram LMベースが多いです）。\n",
    "*   多言語対応にも優れています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55987c6d",
   "metadata": {},
   "source": [
    "## 3. トークナイザの共通パラメータと出力\n",
    "\n",
    "多くのHugging Face Tokenizerは、テキストをエンコードする際に共通の引数を取ります。\n",
    "\n",
    "*   `text` または `text_pair`: トークナイズする単一のテキストまたはテキストペア。\n",
    "*   `add_special_tokens=True`: `[CLS]`, `[SEP]`, `<bos>`, `<eos>` のようなモデル固有の特殊トークンを自動的に付加するかどうか。\n",
    "*   `padding`:\n",
    "    *   `False` (デフォルト): パディングしない。\n",
    "    *   `True` または `'longest'`: バッチ内で最も長いシーケンスに合わせてパディング。\n",
    "    *   `'max_length'`: `max_length` 引数で指定された長さに合わせてパディング（または切り詰め）。\n",
    "*   `truncation=True`: `max_length` を超える場合にシーケンスを切り詰める。\n",
    "*   `max_length`: パディングまたは切り詰めの最大長。\n",
    "*   `return_tensors`: 返り値の型を指定 (`'pt'` for PyTorch, `'tf'` for TensorFlow, `'np'` for NumPy)。\n",
    "*   `return_attention_mask=True`: アテンションメカニズムでパディングトークンを無視するためのマスクを返す。\n",
    "*   `return_token_type_ids=True`: (主にBERTなど) 2つのシーケンスを入力とする場合に、各トークンがどちらのシーケンスに属するかを示すIDを返す。\n",
    "\n",
    "**主な返り値（辞書形式）:**\n",
    "*   `input_ids`: トークンIDのシーケンス。\n",
    "*   `attention_mask`: アテンションマスク（1が通常のトークン、0がパディングトークン）。\n",
    "*   `token_type_ids`: トークンタイプID（必要な場合）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605181a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenizerの共通パラメータと出力の確認 (BERT例) ---\n",
      "Input Text 1: First sentence.\n",
      "Input Text 2: Second sentence, much longer.\n",
      "\n",
      "Encoded Input IDs:\n",
      " tensor([[ 101, 2034, 6251, 1012,  102, 2117, 6251, 1010, 2172, 2936, 1012,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Decoded Tokens:\n",
      " ['[CLS]', 'first', 'sentence', '.', '[SEP]', 'second', 'sentence', ',', 'much', 'longer', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Token Type IDs:\n",
      " tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Tokenizerの共通パラメータと出力の確認 (BERT例) ---\")\n",
    "bert_tokenizer_example = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_pair_example = (\"First sentence.\", \"Second sentence, much longer.\")\n",
    "\n",
    "encoded_pair = bert_tokenizer_example(\n",
    "    text_pair_example[0], text_pair_example[1], # テキストペアを渡す\n",
    "    add_special_tokens=True,\n",
    "    padding='max_length',       # 最大長に合わせてパディング\n",
    "    truncation=True,            # 最大長を超える場合は切り詰め\n",
    "    max_length=20,              # 例として最大長20\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=True  # NSPタスクなどで使用\n",
    ")\n",
    "\n",
    "print(\"Input Text 1:\", text_pair_example[0])\n",
    "print(\"Input Text 2:\", text_pair_example[1])\n",
    "print(\"\\nEncoded Input IDs:\\n\", encoded_pair['input_ids'])\n",
    "print(\"Decoded Tokens:\\n\", bert_tokenizer_example.convert_ids_to_tokens(encoded_pair['input_ids'].squeeze().tolist()))\n",
    "print(\"Attention Mask:\\n\", encoded_pair['attention_mask'])\n",
    "print(\"Token Type IDs:\\n\", encoded_pair['token_type_ids'])\n",
    "# Token Type IDs: 最初の文 ([CLS]含む) が0、2番目の文 ([SEP]含む) が1 になる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b00be2",
   "metadata": {},
   "source": [
    "## 4. 考察とまとめ\n",
    "\n",
    "このノートブックでは、Hugging Face `transformers` ライブラリを使って、BERT (WordPiece)、GPT-2 (BPE)、T5 (SentencePiece) といった代表的な事前学習済みモデルのTokenizerを簡単に利用し、その動作を確認しました。\n",
    "\n",
    "*   **サブワード分割の多様性:** モデルごとに採用しているサブワード分割アルゴリズムや、特殊トークンの扱い、単語の内部構造の表現方法（例: BERTの`##`、GPT-2の`Ġ`、SentencePieceの` `）が異なることがわかります。\n",
    "*   **ライブラリの利便性:** `AutoTokenizer` を使うことで、モデル名を指定するだけで適切なTokenizerをロードでき、エンコード・デコード処理も統一的なインターフェースで行えます。パディング、切り詰め、特殊トークンの付加といった煩雑な処理も簡単に行えます。\n",
    "*   **モデルへの入力形式:** Tokenizerは、テキストをモデルが直接処理できる数値形式（トークンID、アテンションマスクなど）に変換する重要な役割を担っています。\n",
    "\n",
    "サブワード分割は、現代のNLPモデルの性能と効率を支える基盤技術です。これらのTokenizerがどのように動作するかを理解することは、モデルの挙動を把握し、適切に利用する上で非常に重要です。\n",
    "\n",
    "これで、自然言語処理の基礎的な前処理から表現方法、そしてサブワード分割までの学習セクションは一区切りとなります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
