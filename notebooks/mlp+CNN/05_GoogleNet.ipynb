{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2861144",
   "metadata": {},
   "source": [
    "# GoogLeNet (Inception v1) の実装\n",
    "\n",
    "このノートブックでは、2014年のImageNet LSVRCで優勝したGoogLeNet（別名 Inception v1）のアーキテクチャについて学び、その中核となる**Inceptionモジュール**と**1x1畳み込みによる次元削減**のアイデアを理解します。\n",
    "NumPyで主要な概念を実装・確認した後、PyTorchを使ってCIFAR-10データセット用に調整したGoogLeNet風モデルを実装し、学習と評価を行います。\n",
    "\n",
    "**参考論文:**\n",
    "*   Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... & Rabinovich, A. (2015). Going deeper with convolutions. In *Proceedings of the IEEE conference on computer vision and pattern recognition* (pp. 1-9).\n",
    "\n",
    "**このノートブックで学ぶこと:**\n",
    "1.  GoogLeNet (Inception) の設計思想とアーキテクチャの概要\n",
    "2.  主要なコンポーネントの理解とNumPyによる概念実装:\n",
    "    *   Inceptionモジュールの構造\n",
    "    *   1x1畳み込みによる次元削減（ボトルネック層）\n",
    "    *   Global Average Pooling (GAP)\n",
    "3.  PyTorchを使ったGoogLeNet風モデル（CIFAR-10用）の実装\n",
    "4.  補助分類器 (Auxiliary Classifiers) の概念と実装\n",
    "5.  CIFAR-10データセットでの学習と評価\n",
    "\n",
    "**前提知識:**\n",
    "*   VGGのノートブックで学んだCNNの基礎とPyTorchによる実装経験"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19c08c",
   "metadata": {},
   "source": [
    "## 1. 必要なライブラリのインポート\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16370b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.0+cu124\n",
      "Torchvision Version: 0.20.0+cu124\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # 活性化関数やプーリングなどで使用\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74972ac5",
   "metadata": {},
   "source": [
    "## 2. GoogLeNetの主要な設計思想とNumPyによる概念実装\n",
    "\n",
    "GoogLeNetの核心は、計算資源を効率的に利用しながらネットワークの深さと幅を増やすための**Inceptionモジュール**です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb33aa",
   "metadata": {},
   "source": [
    "### 2.1 Inceptionモジュール と 1x1畳み込みによる次元削減\n",
    "\n",
    "*   **Inceptionモジュールの概念 (論文 Figure 2):**\n",
    "    ![Inception](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.22.39_PM.png)  \n",
    "    従来のCNNでは、各層でどのサイズのカーネル（例: 1x1, 3x3, 5x5）やプーリングを使うかを事前に一つ選択する必要がありました。Inceptionモジュールは、これらの異なる演算を**並列**に行い、その結果をチャネル方向に連結（concatenate）することで、ネットワーク自身に最適な特徴の組み合わせを学習させようとします。\n",
    "\n",
    "    **ナイーブなInceptionモジュール (Figure 2a):**\n",
    "    1.  1x1畳み込み\n",
    "    2.  3x3畳み込み\n",
    "    3.  5x5畳み込み\n",
    "    4.  3x3マックスプーリング\n",
    "    これらの出力を全てチャネル方向に連結します。\n",
    "\n",
    "    **問題点:** 3x3や特に5x5の畳み込みは、入力チャネル数が多い場合、計算コストが非常に高くなります。また、プーリング層も出力チャネル数を維持するため、連結後のチャネル数がどんどん増大してしまいます。\n",
    "\n",
    "*   **次元削減を伴うInceptionモジュール (Figure 2b - GoogLeNetで採用):**\n",
    "    この問題を解決するため、計算コストの高い3x3畳み込みと5x5畳み込みの**前**、およびマックスプーリングの**後**に**1x1畳み込み（ボトルネック層）**を挿入し、入力チャネル数を削減します。\n",
    "\n",
    "    **1x1畳み込みの役割:**\n",
    "    1.  **次元削減:** 出力チャネル数を入力チャネル数より小さく設定することで、後続の畳み込み演算の計算量を大幅に削減できます。\n",
    "    2.  **特徴の線形結合と非線形性の追加:** チャネル間で情報を混ぜ合わせ、ReLUを適用することで非線形性を加えます。これはNetwork in Network [Lin et al., 2013] のアイデアに基づいています。\n",
    "\n",
    "    **次元削減版Inceptionモジュールの構成例:**\n",
    "    1.  ブランチ1: 1x1畳み込み\n",
    "    2.  ブランチ2: 1x1畳み込み（次元削減） → 3x3畳み込み\n",
    "    3.  ブランチ3: 1x1畳み込み（次元削減） → 5x5畳み込み\n",
    "    4.  ブランチ4: 3x3マックスプーリング → 1x1畳み込み（次元削減/特徴変換）\n",
    "    これらの4つのブランチの出力をチャネル方向に連結します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b1a31",
   "metadata": {},
   "source": [
    "#### 2.1.1 NumPyによる1x1畳み込み (次元削減) の概念確認\n",
    "(VGGノートブックの `convolve1x1` を再利用)\n",
    "ここでは、入力チャネル数を減らす例を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80855221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve1x1_simple(input_volume, kernels_out_ch_in_ch, biases_out_ch):\n",
    "    \"\"\"\n",
    "    簡易版1x1畳み込み (入力: C_in x H x W, カーネル: C_out x C_in)\n",
    "    出力: C_out x H x W\n",
    "    \"\"\"\n",
    "    C_in, H, W = input_volume.shape\n",
    "    C_out, _ = kernels_out_ch_in_ch.shape # (C_out, C_in)\n",
    "    \n",
    "    output_volume = np.zeros((C_out, H, W))\n",
    "    for h_idx in range(H):\n",
    "        for w_idx in range(W):\n",
    "            pixel_vector = input_volume[:, h_idx, w_idx] # (C_in,)\n",
    "            # (C_out, C_in) @ (C_in,) -> (C_out,)\n",
    "            transformed_pixel = np.dot(kernels_out_ch_in_ch, pixel_vector) + biases_out_ch\n",
    "            output_volume[:, h_idx, w_idx] = transformed_pixel\n",
    "    return output_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1x1 畳み込み (次元削減) テスト ---\n",
      "入力 (4チャネル x 2x2):\n",
      " (4, 2, 2)\n",
      "\n",
      "1x1畳み込み (次元削減) 後 (2チャネル x 2x2):\n",
      " (2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1x1 畳み込み (次元削減) テスト ---\")\n",
    "# 4チャネル入力画像 (2x2)\n",
    "test_input_bottleneck = np.random.rand(4, 2, 2).astype(np.float32)\n",
    "print(\"入力 (4チャネル x 2x2):\\n\", test_input_bottleneck.shape)\n",
    "\n",
    "# 2出力チャネルに削減する1x1カーネル (2出力 x 4入力)\n",
    "kernels_bottleneck = np.random.rand(2, 4).astype(np.float32) * 0.1\n",
    "biases_bottleneck = np.random.rand(2).astype(np.float32) * 0.01\n",
    "\n",
    "output_bottleneck = convolve1x1_simple(test_input_bottleneck, kernels_bottleneck, biases_bottleneck)\n",
    "print(\"\\n1x1畳み込み (次元削減) 後 (2チャネル x 2x2):\\n\", output_bottleneck.shape)\n",
    "# print(output_bottleneck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e430eae",
   "metadata": {},
   "source": [
    "### 2.2 Global Average Pooling (GAP)\n",
    "\n",
    "*   **概念:**\n",
    "    従来のCNNでは、畳み込み層/プーリング層の後にいくつかの全結合層を配置して最終的な分類を行っていました。全結合層は多くのパラメータを持ち、過学習の原因となりやすいです。\n",
    "    GoogLeNet (や他の現代的なCNN) では、最後の畳み込み層（またはInceptionモジュール）の出力特徴マップに対して、**Global Average Pooling (GAP)** を適用します。\n",
    "    GAPは、各特徴マップの空間的な次元（高さと幅）に対して平均値を計算し、チャネルごとに1つの値を出力します。\n",
    "    例えば、入力が `(C, H, W)` の特徴マップであれば、GAPの出力は `(C, 1, 1)` となり、これをフラット化して `(C)` 次元のベクトルとします。\n",
    "    このベクトルを直接Softmax層に入力するか、オプションで1つだけ全結合層を挟むこともあります。\n",
    "\n",
    "*   **利点:**\n",
    "    *   **パラメータ数の大幅な削減:** 全結合層の代わりに使うことで、モデル全体のパラメータ数を大きく減らせます。\n",
    "    *   **過学習の抑制:** パラメータが少ないため、正則化効果があります。\n",
    "    *   **空間情報への頑健性:** 入力画像の空間的な位置ずれに対して、より頑健になります。\n",
    "    *   **入力画像サイズへの柔軟性:** 理論上は、GAPは任意の入力サイズの特徴マップに適用可能です（ただし、学習時と推論時で分布が変わる可能性には注意）。\n",
    "\n",
    "*   **NumPyによる概念実装:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161d0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_average_pool(feature_maps):\n",
    "    \"\"\"\n",
    "    Global Average Pooling (NumPy実装)\n",
    "    Args:\n",
    "        feature_maps (np.array): 入力特徴マップ (チャネル数, 高さ, 幅)\n",
    "                                  または (バッチサイズ, チャネル数, 高さ, 幅)\n",
    "    Returns:\n",
    "        np.array: プーリング後のベクトル (チャネル数,) または (バッチサイズ, チャネル数)\n",
    "    \"\"\"\n",
    "    if feature_maps.ndim == 3: # (C, H, W)\n",
    "        # HとWの次元 (axis 1と2) に沿って平均を取る\n",
    "        return np.mean(feature_maps, axis=(1, 2))\n",
    "    elif feature_maps.ndim == 4: # (B, C, H, W)\n",
    "        # HとWの次元 (axis 2と3) に沿って平均を取る\n",
    "        return np.mean(feature_maps, axis=(2, 3))\n",
    "    else:\n",
    "        raise ValueError(\"Input must be 3D or 4D tensor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aad5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Global Average Pooling テスト ---\n",
      "GAP入力 (2チャネル x 3x3):\n",
      " [[[1. 2. 3.]\n",
      "  [4. 5. 6.]\n",
      "  [7. 8. 9.]]\n",
      "\n",
      " [[9. 8. 7.]\n",
      "  [6. 5. 4.]\n",
      "  [3. 2. 1.]]]\n",
      "\n",
      "GAP出力 (2チャネル,):\n",
      " [5. 5.]\n",
      "\n",
      "GAP入力 (バッチ, 2x3x4x4):\n",
      " (2, 3, 4, 4)\n",
      "\n",
      "GAP出力 (バッチ, 2x3):\n",
      " (2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Global Average Pooling テスト ---\")\n",
    "# 2チャネル、3x3の特徴マップ\n",
    "test_gap_input = np.array([\n",
    "    [[1,2,3], [4,5,6], [7,8,9]], # Channel 0\n",
    "    [[9,8,7], [6,5,4], [3,2,1]]  # Channel 1\n",
    "], dtype=np.float32)\n",
    "print(\"GAP入力 (2チャネル x 3x3):\\n\", test_gap_input)\n",
    "output_gap = global_average_pool(test_gap_input)\n",
    "print(\"\\nGAP出力 (2チャネル,):\\n\", output_gap)\n",
    "# Ch0_mean = (1+2+3+4+5+6+7+8+9)/9 = 45/9 = 5\n",
    "# Ch1_mean = (9+8+7+6+5+4+3+2+1)/9 = 45/9 = 5\n",
    "# 期待値: [5., 5.]\n",
    "\n",
    "# バッチ処理の場合\n",
    "test_gap_input_batch = np.random.rand(2, 3, 4, 4).astype(np.float32) # (batch=2, ch=3, H=4, W=4)\n",
    "print(\"\\nGAP入力 (バッチ, 2x3x4x4):\\n\", test_gap_input_batch.shape)\n",
    "output_gap_batch = global_average_pool(test_gap_input_batch)\n",
    "print(\"\\nGAP出力 (バッチ, 2x3):\\n\", output_gap_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638de78",
   "metadata": {},
   "source": [
    "## 3. GoogLeNet アーキテクチャの概要\n",
    "\n",
    "GoogLeNetは、複数のInceptionモジュールを積み重ね、途中と最後に補助分類器やGlobal Average Poolingを配置した構造です。\n",
    "論文のTable 1に詳細なアーキテクチャが記載されています。\n",
    "非常に深いネットワーク（22層の重み層）ですが、Inceptionモジュール内の1x1畳み込みによる次元削減のおかげで、パラメータ数はAlexNetよりも大幅に少なくなっています（約500万パラメータ vs AlexNetの約6000万）。\n",
    "\n",
    "**主な構成要素:**\n",
    "1.  初期の畳み込み層とプーリング層（ステム部分）\n",
    "2.  複数のInceptionモジュールのスタック\n",
    "3.  途中に配置される補助分類器（学習時のみ使用）\n",
    "4.  Global Average Pooling層\n",
    "5.  最終的な分類のための全結合層（または直接Softmax）とDropout\n",
    "\n",
    "CIFAR-10用に実装する場合、入力画像サイズが小さいため、最初の畳み込み層のストライドやプーリング層の適用回数、Inceptionモジュール内のチャネル数などを大幅に調整する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b8d78",
   "metadata": {},
   "source": [
    "## 4. PyTorchによるGoogLeNet風モデルの実装とCIFAR-10での学習・評価\n",
    "\n",
    "CIFAR-10データセット用に調整したGoogLeNet風モデル（Inceptionモジュールを含む）を実装します。\n",
    "補助分類器も実装し、学習時に利用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680bd617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 訓練データ数 (GoogLeNet): 50000\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 データセットの準備 (VGGノートブックと同様)\n",
    "transform_cifar_googlenet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset_cifar_gn = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                      download=True, transform=transform_cifar_googlenet)\n",
    "test_dataset_cifar_gn = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                                     download=True, transform=transform_cifar_googlenet)\n",
    "\n",
    "batch_size_cifar_gn = 128 \n",
    "train_loader_cifar_gn = DataLoader(train_dataset_cifar_gn, batch_size=batch_size_cifar_gn, shuffle=True, num_workers=2)\n",
    "test_loader_cifar_gn = DataLoader(test_dataset_cifar_gn, batch_size=batch_size_cifar_gn, shuffle=False, num_workers=2)\n",
    "\n",
    "classes_cifar_gn = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "                    'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"CIFAR-10 訓練データ数 (GoogLeNet): {len(train_dataset_cifar_gn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d034cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inceptionモジュールの定義\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n1x1, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n3x3_reduce, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n5x5_reduce, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=5, padding=2), # 5x5なのでpadding=2\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1), # 出力サイズを維持\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        # 全てのブランチの出力をチャネル方向に連結\n",
    "        return torch.cat([y1,y2,y3,y4], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc8134d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 補助分類器の定義\n",
    "class AuxiliaryClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(AuxiliaryClassifier, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4)) #論文では5x5だがCIFAR-10用に調整\n",
    "        self.conv = nn.Conv2d(in_channels, 128, kernel_size=1) # 論文に倣う\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256) # 論文では1024だがCIFAR用に縮小\n",
    "        self.dropout = nn.Dropout(0.7) # 論文では0.7\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1730950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoogLeNetモデル (CIFAR-10用に大幅に簡略化・調整)\n",
    "class GoogLeNetCIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, aux_logits=True): # 訓練時に補助分類器を使うか\n",
    "        super(GoogLeNetCIFAR, self).__init__()\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        # Stem (初期の畳み込み・プーリング層) - CIFAR-10用に調整\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            # Input: 3x32x32\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1), # 64x32x32\n",
    "            nn.ReLU(True),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # -> 64x16x16 (論文風)\n",
    "            # CIFAR-10では画像が小さいのでプーリングを減らすかカーネルを小さくする\n",
    "        )\n",
    "        \n",
    "        # Inceptionモジュールのスタック\n",
    "        # チャネル数はCIFAR-10用に大幅に削減\n",
    "        self.inception3a = InceptionModule(64,  32, 32, 64,  8, 16, 16) # out: 32+64+16+16 = 128\n",
    "        self.inception3b = InceptionModule(128, 64, 64, 96, 16, 48, 32) # out: 64+96+48+32 = 240\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # -> 240 x 16x16\n",
    "        \n",
    "        self.inception4a = InceptionModule(240, 96,  48, 104,  8, 24,  32) # out: 96+104+24+32=256\n",
    "        if self.aux_logits: # 最初の補助分類器\n",
    "            self.aux1 = AuxiliaryClassifier(256, num_classes) # 入力チャネル数変更\n",
    "            \n",
    "        self.inception4b = InceptionModule(256, 80,  56, 112, 12, 32,  32) # out: 80+112+32+32=256\n",
    "        self.inception4c = InceptionModule(256, 64,  64, 128, 12, 32,  32) # out: 64+128+32+32=256\n",
    "        self.inception4d = InceptionModule(256, 56,  72, 144, 16, 32,  32) # out: 56+144+32+32=264\n",
    "        if self.aux_logits: # 2番目の補助分類器\n",
    "            self.aux2 = AuxiliaryClassifier(264, num_classes) # 入力チャネル数変更\n",
    "\n",
    "        self.inception4e = InceptionModule(264, 128, 80, 160, 16, 64, 64) # out: 128+160+64+64=416\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # -> 416 x 8x8\n",
    "        \n",
    "        self.inception5a = InceptionModule(416, 128, 80, 160, 16, 64, 64) # out: 128+160+64+64=416\n",
    "        self.inception5b = InceptionModule(416, 192, 96, 192, 24, 64, 64) # out: 192+192+64+64=512\n",
    "\n",
    "        # Global Average Pooling と最終分類器\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # 出力を1x1にする\n",
    "        self.dropout = nn.Dropout(0.4) # 論文では0.4\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_layers(x)\n",
    "        \n",
    "        out = self.inception3a(out)\n",
    "        out = self.inception3b(out)\n",
    "        out = self.maxpool3(out)\n",
    "        \n",
    "        out = self.inception4a(out)\n",
    "        if self.training and self.aux_logits and hasattr(self, 'aux1'): # 訓練時のみ補助分類器を使用\n",
    "            out_aux1 = self.aux1(out)\n",
    "        else:\n",
    "            out_aux1 = None\n",
    "            \n",
    "        out = self.inception4b(out)\n",
    "        out = self.inception4c(out)\n",
    "        out = self.inception4d(out)\n",
    "        if self.training and self.aux_logits and hasattr(self, 'aux2'):\n",
    "            out_aux2 = self.aux2(out)\n",
    "        else:\n",
    "            out_aux2 = None\n",
    "            \n",
    "        out = self.inception4e(out)\n",
    "        out = self.maxpool4(out)\n",
    "        \n",
    "        out = self.inception5a(out)\n",
    "        out = self.inception5b(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.dropout(out)\n",
    "        out_final = self.fc(out)\n",
    "        \n",
    "        if self.training and self.aux_logits:\n",
    "            return out_final, out_aux1, out_aux2\n",
    "        return out_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1333118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GoogLeNet-style Model for CIFAR-10:\n",
      "\n",
      "Dummy Output Shapes (final, aux1, aux2): torch.Size([32, 10]) torch.Size([32, 10]) torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "model_googlenet_cifar = GoogLeNetCIFAR(num_classes=10, aux_logits=True).to(device)\n",
    "print(\"\\nGoogLeNet-style Model for CIFAR-10:\\n\")\n",
    "# print(model_googlenet_cifar) # 非常に長いためコメントアウト\n",
    "\n",
    "# ダミー入力でフォワードパスのテスト\n",
    "dummy_input_gn = torch.randn(batch_size_cifar_gn // 4, 3, 32, 32).to(device) # バッチを小さく\n",
    "try:\n",
    "    # 訓練モードでの出力を確認\n",
    "    model_googlenet_cifar.train()\n",
    "    outputs_dummy_gn = model_googlenet_cifar(dummy_input_gn)\n",
    "    if isinstance(outputs_dummy_gn, tuple): # 補助分類器あり\n",
    "        print(\"Dummy Output Shapes (final, aux1, aux2):\", \n",
    "              outputs_dummy_gn[0].shape, outputs_dummy_gn[1].shape, outputs_dummy_gn[2].shape)\n",
    "    else: # 補助分類器なし (推論時など)\n",
    "        print(\"Dummy Output Shape (final):\", outputs_dummy_gn.shape)\n",
    "except Exception as e:\n",
    "    print(\"Error during GoogLeNet dummy forward pass:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4264cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GoogLeNet-style 学習開始 (CIFAR-10データ、40 epochs)...\n",
      "Epoch [1/40] - Dur: 61.0s - TrL: 3.002, TrAcc: 26.87% - TeL: 1.702, TeAcc: 35.68%\n",
      "Epoch [2/40] - Dur: 60.7s - TrL: 2.437, TrAcc: 42.13% - TeL: 1.426, TeAcc: 46.08%\n",
      "Epoch [3/40] - Dur: 56.9s - TrL: 2.048, TrAcc: 53.18% - TeL: 1.182, TeAcc: 56.98%\n",
      "Epoch [4/40] - Dur: 57.0s - TrL: 1.797, TrAcc: 59.94% - TeL: 1.051, TeAcc: 61.88%\n",
      "Epoch [5/40] - Dur: 57.2s - TrL: 1.617, TrAcc: 64.96% - TeL: 0.908, TeAcc: 67.10%\n",
      "Epoch [6/40] - Dur: 57.1s - TrL: 1.464, TrAcc: 68.93% - TeL: 0.854, TeAcc: 69.67%\n",
      "Epoch [7/40] - Dur: 57.3s - TrL: 1.342, TrAcc: 71.67% - TeL: 0.806, TeAcc: 71.09%\n",
      "Epoch [8/40] - Dur: 56.6s - TrL: 1.219, TrAcc: 74.44% - TeL: 0.751, TeAcc: 73.78%\n",
      "Epoch [9/40] - Dur: 57.3s - TrL: 1.133, TrAcc: 76.71% - TeL: 0.725, TeAcc: 74.44%\n",
      "Epoch [10/40] - Dur: 56.9s - TrL: 1.049, TrAcc: 78.41% - TeL: 0.702, TeAcc: 75.52%\n",
      "Epoch [11/40] - Dur: 57.3s - TrL: 0.818, TrAcc: 84.17% - TeL: 0.628, TeAcc: 78.89%\n",
      "Epoch [12/40] - Dur: 57.5s - TrL: 0.767, TrAcc: 85.27% - TeL: 0.621, TeAcc: 79.34%\n",
      "Epoch [13/40] - Dur: 57.3s - TrL: 0.742, TrAcc: 85.92% - TeL: 0.618, TeAcc: 79.55%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     38\u001b[0m optimizer_gn\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 40\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     41\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output_final\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 最終出力で精度計算\u001b[39;00m\n\u001b[0;32m     42\u001b[0m total_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 損失関数とOptimizer\n",
    "criterion_gn = nn.CrossEntropyLoss()\n",
    "# optimizer_gn = optim.SGD(model_googlenet_cifar.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_gn = optim.Adam(model_googlenet_cifar.parameters(), lr=0.001)\n",
    "\n",
    "# 学習率スケジューラ (オプション)\n",
    "scheduler_gn = torch.optim.lr_scheduler.StepLR(optimizer_gn, step_size=10, gamma=0.1) # 論文では8エポック毎に4%減\n",
    "\n",
    "# 学習ループ\n",
    "num_epochs_gn = 40 # GoogLeNetは深いので学習に時間がかかる\n",
    "print(f\"\\nGoogLeNet-style 学習開始 (CIFAR-10データ、{num_epochs_gn} epochs)...\")\n",
    "\n",
    "history_gn = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(num_epochs_gn):\n",
    "    model_googlenet_cifar.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    start_epoch_time = time.time()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader_cifar_gn):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_gn.zero_grad()\n",
    "        \n",
    "        if model_googlenet_cifar.aux_logits: # 訓練時で補助分類器ありの場合\n",
    "            output_final, output_aux1, output_aux2 = model_googlenet_cifar(images)\n",
    "            loss_final = criterion_gn(output_final, labels)\n",
    "            loss_aux1 = criterion_gn(output_aux1, labels)\n",
    "            loss_aux2 = criterion_gn(output_aux2, labels)\n",
    "            # 論文では補助分類器の損失に重み0.3をかける\n",
    "            loss = loss_final + 0.3 * loss_aux1 + 0.3 * loss_aux2\n",
    "        else:\n",
    "            output_final = model_googlenet_cifar(images)\n",
    "            loss = criterion_gn(output_final, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer_gn.step()\n",
    "        \n",
    "        running_train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(output_final.data, 1) # 最終出力で精度計算\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / total_train\n",
    "    epoch_train_acc = 100 * correct_train / total_train\n",
    "    history_gn['train_loss'].append(epoch_train_loss)\n",
    "    history_gn['train_acc'].append(epoch_train_acc)\n",
    "    \n",
    "    model_googlenet_cifar.eval() # 評価モード (補助分類器は使わない)\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for images_test, labels_test in test_loader_cifar_gn:\n",
    "            images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
    "            outputs_test_final = model_googlenet_cifar(images_test) # 推論時は最終出力のみ\n",
    "            loss_test = criterion_gn(outputs_test_final, labels_test)\n",
    "            running_test_loss += loss_test.item() * images_test.size(0)\n",
    "            _, predicted_test = torch.max(outputs_test_final.data, 1)\n",
    "            total_test += labels_test.size(0)\n",
    "            correct_test += (predicted_test == labels_test).sum().item()\n",
    "            \n",
    "    epoch_test_loss = running_test_loss / total_test\n",
    "    epoch_test_acc = 100 * correct_test / total_test\n",
    "    history_gn['test_loss'].append(epoch_test_loss)\n",
    "    history_gn['test_acc'].append(epoch_test_acc)\n",
    "    \n",
    "    end_epoch_time = time.time()\n",
    "    epoch_duration = end_epoch_time - start_epoch_time\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_gn}] - Dur: {epoch_duration:.1f}s - \"\n",
    "          f\"TrL: {epoch_train_loss:.3f}, TrAcc: {epoch_train_acc:.2f}% - \"\n",
    "          f\"TeL: {epoch_test_loss:.3f}, TeAcc: {epoch_test_acc:.2f}%\")\n",
    "    \n",
    "    if scheduler_gn: scheduler_gn.step()\n",
    "\n",
    "print(\"GoogLeNet-style 学習完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習曲線\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs_gn + 1), history_gn['train_loss'], label='Training Loss', marker='.')\n",
    "plt.plot(range(1, num_epochs_gn + 1), history_gn['test_loss'], label='Test Loss', marker='.')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('GoogLeNet Training & Test Loss (CIFAR-10)'); plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs_gn + 1), history_gn['train_acc'], label='Training Accuracy', marker='.')\n",
    "plt.plot(range(1, num_epochs_gn + 1), history_gn['test_acc'], label='Test Accuracy', marker='.')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy (%)'); plt.title('GoogLeNet Training & Test Accuracy (CIFAR-10)'); plt.legend(); plt.ylim(0,100); plt.grid(True)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f\"\\n最終テスト精度 (GoogLeNet-style on CIFAR-10): {history_gn['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2352f",
   "metadata": {},
   "source": [
    "## 5. 考察\n",
    "\n",
    "*   **GoogLeNet (Inception) の設計思想のポイント:**\n",
    "    *   **Inceptionモジュール:** 異なるサイズの畳み込みとプーリングを並列に適用し、結果を連結することで、ネットワークが様々なスケールの特徴を効率的に学習できるようにしました。\n",
    "    *   **1x1畳み込みによる次元削減:** Inceptionモジュール内の計算コストが高い畳み込みの前に1x1畳み込みを配置し、チャネル数を削減することで、計算量を大幅に抑えつつ深いネットワークを実現しました。これは「ボトルネック層」として機能します。\n",
    "    *   **ネットワークの深さと幅の効率的な増加:** Inceptionモジュールを積み重ねることで、計算コストを適切に管理しながらネットワークを深く、かつ各層の「幅」（多様な演算の並列実行）も広げることができました。\n",
    "    *   **補助分類器:** ネットワークの中間層に補助的な分類器を設け、学習時にその損失も考慮することで、勾配がネットワーク全体に行き渡りやすくなり、深いネットワークの学習を助ける効果がありました。また、正則化の効果も期待されます。\n",
    "    *   **Global Average Pooling (GAP):** 従来の全結合層の代わりにGAPを使用することで、パラメータ数を大幅に削減し、過学習を抑制しました。\n",
    "\n",
    "*   **PyTorchによるGoogLeNet風モデルの実装 (CIFAR-10用):**\n",
    "    *   Inceptionモジュールと補助分類器をそれぞれ`nn.Module`として定義し、これらを組み合わせて全体のGoogLeNet風アーキテクチャを構築しました。\n",
    "    *   CIFAR-10の画像サイズ (32x32) に合わせて、元のGoogLeNet (ImageNet用、入力224x224) からプーリング層の数やInceptionモジュール内のチャネル数を大幅に調整・削減しました。そうしないと、特徴マップの空間次元が早々に小さくなりすぎるか、パラメータ数が過大になります。\n",
    "    *   学習時には、補助分類器からの損失も最終的な損失に加算しました（重み0.3）。推論時には補助分類器は使用しません。\n",
    "\n",
    "*   **学習結果と課題:**\n",
    "    *   CIFAR-10用に調整したGoogLeNet風モデルでも、適切な学習を行えば良好な精度が期待できます。補助分類器や1x1ボトルネックの効果を検証するには、それらの有無で比較実験を行うと良いでしょう。\n",
    "    *   元のGoogLeNetは非常に深い（22重み層）ですが、CIFAR-10ではそこまでの深さは必ずしも必要ないか、過学習のリスクを高める可能性があります。モデルのサイズとデータセットのサイズのバランスが重要です。\n",
    "    *   ハイパーパラメータ（学習率、Optimizer、正則化の強さ、補助分類器の損失の重みなど）のチューニングは、性能を最大限に引き出すために依然として重要です。\n",
    "\n",
    "GoogLeNet (Inceptionアーキテクチャ) は、CNNの設計において「どのように計算資源を効率的に使い、性能を向上させるか」という問題に対する一つの強力な回答を示しました。その後の多くのネットワークアーキテクチャ（Inception v2, v3, v4, Inception-ResNetなど）は、このInceptionのアイデアをさらに発展させたものです。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
